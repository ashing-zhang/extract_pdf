Structure-Aware Residual Pyramid Network for Monocular Depth Estimation
Xiaotian Chen, Xuejin Chen∗ and Zheng-Jun Zha
National Engineering Laboratory for Brain-inspired Intelligence Technology and Application
University of Science and Technology of China
ustcxt@mail.ustc.edu.cn, {xjchen99, zhazj}@ustc.edu.cn
9102 luJ 31  ]VC.sc[  1v32060.7091:viXra Abstract
Monocular depth estimation is an essential task
for scene understanding. The underlying structure
of objects and stuff in a complex scene is crit-
ical to recovering accurate and visually-pleasing
depth maps. Global structure conveys scene lay-
outs, while local structure reflects shape details.
Recently developed approaches based on convo-
lutional neural networks (CNNs) significantly im-
prove the performance of depth estimation. How-
ever, few of them take into account multi-scale
structures in complex scenes. In this paper, we pro-
pose a Structure-Aware Residual Pyramid Network
(SARPN) to exploit multi-scale structures for accu- Figure 1: Problems in depth prediction: (a)(b) inaccurate depth val-
rate depth prediction. We propose a Residual Pyra- ues on large planar regions, such as walls. (c)(d) blurry boundaries
mid Decoder (RPD) which expresses global scene and missing details (chair legs). Our approach simultaneously re-
covers large planar structures and object details.
structure in upper levels to represent layouts, and
local structure in lower levels to present shape de-
tails. At each level, we propose Residual Refine-
al., 2016; Fu et al., 2018; Xu et al., 2018b; Hao et al., 2018;
ment Modules (RRM) that predict residual maps
Hu et al., 2019]. To resolve the ambiguity, they typically em-
to progressively add finer structures on the coarser
ploy an encoder-decoder architecture to implicitly fuse fea-
structure predicted at the upper level. In order to
tures that represents object appearance, geometry, semantics,
fully exploit multi-scale image features, an Adap-
spatial relations, etc. The encoder gradually extracts multi-
tive Dense Feature Fusion (ADFF) module, which
scale features, and the decoder employs multi-stage upsam-
adaptively fuses effective features from all scales
pling as well as shortcut connections to restore object details
for inferring structures of each scale, is introduced.
in high-resolution predictions.
Experiment results on the challenging NYU-Depth
Though a great improvement on average pixel-wise met-
v2 dataset demonstrate that our proposed approach
rics has been made, the underlying structure of objects and
achieves state-of-the-art performance in both qual-
stuff is not well preserved by current CNN-based methods.
itative and quantitative evaluation. The code is
The problem becomes especially challenging when the size
available at https://github.com/Xt-Chen/SARPN.
of objects and stuff varies widely in complex scenes. As
Figure 1 shows, it is challenging for existing approaches to
1 Introduction
accurately recover the large-scale geometry (walls) and local
details (boundaries and small parts) at the same time. This in-
Monocular depth estimation, which aims to predict the depth
accurate inference at regions of diverse scales motivates us to
valueofeachpixelfromagivenRGBimage, iscrucialforun-
fully exploit the hierarchical scene structure in depth predic-
derstanding scene geometry, and can be applied to facilitate
tion. Scene structure, depicting the organization and arrange-
other vision tasks, such as semantic segmentation [Park et al.,
ment of multiple interrelated elements in a complex scene,
2017] and hand tracking [Qian et al., 2014]. It is an ill-posed
varieswidelyaccordingtotheelementtype. Theglobalstruc-
problem because of the inherent ambiguity due to perspective
ture represents the spatial arrangement of large-size elements
projection. Recently, CNN-based approaches have achieved
such as walls, floors, and furniture objects. Local structure
significant success in monocular depth estimation [Laina et
describes geometric details of objects and their parts. The
∗Corresponding author natural hierarchy of scene structure provides essential con-

straints between the depth values of pixels in multiple scales. et al., 2018]. Instead of multi-scale network structure, dilated
Although previous CNN-based techniques extract multi-scale convolution is used to extract multi-scale features for depth
image features and gradually fuse them to predict a depth estimation [Hao et al., 2018]. Hu et al. [2019] proposed an
map, the underlying hierarchical structure of the scene has effective multi-scale feature fusion module to produce clear
not been taken into account. object boundaries. Although these methods have achieved re-
In this paper, we introduce a Structure-Aware Residual markable results by fusing multi-scale features, they still face
Pyramid Network (SARPN) to fully exploit scene structures the problem of inaccurate prediction for complex scenes of
in multiple scales for depth prediction. A Residual Pyra- which the structure varies largely in scales, from large room
mid Decoder (RPD) is proposed to predict multi-scale depth layout to fine object details.
maps in a coarse-to-fine manner. Depth maps in upper lev- In order to better restore structure details, a few methods
els in the pyramid represent the global scene structure, while design new loss functions to explicitly constrain scene geom-
depth maps in lower levels capture more local structures of etry. Zheng et al. [2018] proposed an order-sensitive softmax
objects or parts. To convey the global structure and constrain loss to constrain global layouts. Similarly, Fu et al. [2018]
the generation of finer details, we proposed a residual refine- used an ordinary regression loss. With respect to clear bound-
ment module to predict residual depth maps, which progres- aries and details, a loss function is designed by combining
sively add details on the scene structure on a larger scale. depth, surface normal and gradient in a local neighborhood
In order to fuse multi-scale features extracted from the in- of depth maps [Hu et al., 2019].
put image for residual prediction, we propose an Adaptive Due to the strong correlation between many visual tasks,
Dense Feature Fusion (ADFF) module to adaptively select such as depth estimation, semantic segmentation, and nor-
more effective features for each scale. Integrating the residual malestimation, manyapproaches employajoint tasklearning
pyramid decoder and adaptive dense feature fusion module, framework. A multi-scale CNN was designed to simultane-
our method simultaneously preserves the hierarchical scene ously perform semantic segmentation, depth estimation, and
structures and produces accurate depth estimation for both normal estimation [Eigen and Fergus, 2015]. A set of inter-
large-size shapes and fine details of small object parts, as Fig- mediate auxiliary tasks are utilized to guide the final depth
ure 1 shows. Our contributions are summarized as follows: estimation and semantic segmentation [Xu et al., 2018a].
Zhang et al. proposed a novel joint task-recursive learning
• We propose a Structure-Aware Residual Pyramid Net-
method to recursively refine the results of depth estimation
work (SARPN), which takes the underlying scene struc-
and semantic segmentation [Zhang et al., 2018]. A synergy
ture in multiple scales into account for accurate depth
network is proposed to automatically learn information shar-
prediction.
ing strategy between depth estimation and semantic segmen-
• Our Adaptive Dense Feature Fusion (ADFF) module tation [Jiao et al., 2018]. Moreover, based on the observed
adaptively selects features from all scales to predict long-tail distribution of depth values, an attention-driven loss
residual depths at different structure scales. is also designed to improve the accuracy [Jiao et al., 2018].
• The proposed method achieves state-of-the-art perfor-
mance on the challenging NYUD v2 dataset. More im- 3 Methodology
portantly, the visual quality of recovered depth maps is
Our network consists of three main parts: an encoder for
significantly improved.
multi-scale feature extraction, an adaptive dense feature fu-
sion module, and a residual pyramid decoder, as Figure 2
2 Related Work
shows. WefirstintroducethenetworkarchitectureinSec.3.1.
The residual pyramid decoder and adaptive dense feature fu-
In recent years, CNNs have become the most successful tech-
sion module are explained in Sec. 3.2 and 3.3, respectively.
niques for various visual tasks, and were firstly used for
monocular depth estimation [Eigen et al., 2014] in a multiple
3.1 Structure-Aware Residual Pyramid Network
scale scheme. Later on, fully convolutional network (FCN)
was proposed for semantic segmentation [Long et al., 2015] Our approach begins with an encoder which extracts multi-
and has been widely used in many dense prediction tasks, in- scale features {Fi }L from the input image, where Fi
ex i=1 ex
cluding depth estimation. indicates the feature maps extracted at the i-th level. L is
When FCN-based architecture was first adopted for depth the number of layers in our network. Following the state-
estimation, the resolution and accuracy ware largely im- of-the-art approach [Hu et al., 2019], we use SENet [Hu et
proved by using ResNet to extract features and up-projection al., 2018] as the backbone of our encoder. It extracts more
blocks [Laina et al., 2016]. In order to improve the qual- effective features by re-weighting features of different chan-
ity of depth estimation for local details, many strategies have nels. Given an input image with size W × H, the size of
been introduced. Applying conditional random field as post- these feature maps are respectively [W , H], and they carry
2i 2i
processing [Li et al., 2015] or integrating it in CNNs [Xu et both high-level semantic information and low-level detail in-
al., 2017] largely improves the prediction quality for small formation. Then, these multi-scale feature maps are simul-
objects. Later, an attention model is integrated to improve taneously fed to our dense feature fusion module to produce
the estimation performance [Xu et al., 2018b]. Multi-scale a Fused Feature Pyramid (FFP). These feature maps in FFP
architecture becomes a common solution to avoid the loss of are represented by {Fi }L , where Fi indicates the fused
fs i=1 fs
local details caused by spatial pooling and convolutions [Fu feature maps at the i-th level of the pyramid of fused features.

Figure 2: The network architecture. Our Structure-Aware Residual Pyramid Network consists of an encoder which extracts multi-scale visual
features, a Residual Pyramid Decoder (RPD) which progressively infers depth maps in a coarse-to-fine manner, and an Adaptive Dense
Feature Fusion (ADFF) module for dense feature fusion. The residual pyramid effectively adds structure details in each level based on the
scene layout predicted at a coarser level.
In the decoder part, different from the previous methods
that directly predict a depth map by sequentially upsampling
feature maps [Laina et al., 2016; Hu et al., 2019], our resid-
ual pyramid progressively predicts multiple depth maps in a
coarse-to-fine manner. The depth map at the top level with
size W × H is predicted first as the initial scene layout. We
32 32
utilize a 1 × 1 convolution operation to reduce the channel
Figure 3: A Residual Refinement Module (RRM) for the i-th level.
number of the feature maps FL to the same as the channel
ex
number of feature maps FL of fused feature pyramid and
fs
concatenate them together. A residual block is used to pre- fine the prediction and outputs a depth map Di at the i-th
dictadepthmapDL insizeof[W , H ]fromtheconcatenated
scale. This residual architecture induces our network to effec-
2L 2L
feature maps. Then we gradually refine the depth prediction
tively represent the structure details at each scale and hierar-
by our proposed residual pyramid decoder.
chically refine scene structures. Meanwhile, the global scene
layout is well preserved by our residual pyramid decoder.
3.2 Residual Pyramid Decoder
3.3 Adaptive Dense Feature Fusion
Our residual pyramid decoder predicts depth maps of mul-
tiple scales in order to restore the hierarchical scene struc- In general, due to pooling operations and convolution oper-
tures in a coarse-to-fine manner. As shown in Figure 2, the ations with strides in CNNs, a large amount of low-level vi-
depth maps in lower resolutions depicts more global scene sual features are lost. As a result, it is difficult for the de-
layout, while the depth maps in higher resolutions contain coder to recover the lost low-level structure details. However,
more structure details. In each level of the pyramid decoder, both low-level features and high-level features are critical for
we predict a residual map instead of a dense depth map from predicting residual maps in all layers, because the residual
fused image features in FFP. The residual map and the depth maps convey additional details on a global scene structure,
map predicted at the upper level are integrated together to as the residual pyramid illustrates in Figure 2. In order to
produce a refined depth map in the current scale using our provide sufficient information for the prediction of a resid-
Residual Refinement Module (RRM). The components of ual map in each level, we propose an Adaptive Dense Feature
each RRM are shown in Figure 3. The depth map Di+1 pre- Fusion (ADFF) module. This dense fusion module consists
dicted at the upper scale is upsampled to the current scale of L Multi-scale Feature Fusion (MFF) modules to predict L
by bilinear interpolation. A residual depth map Di is gen- fused feature maps, which compose a fused feature pyramid
res
erated by utilizing the fused features Fi . After adding the for residual prediction.
fs
residual map and the upsampled depth map, a residual block, In each layer, the MFF adaptively selects eligible features
which contains three convolutional layers, is employed to re- from all feature scales when predicting the depth map for

Method REL RMS log10 δ < 1.25 δ < 1.252 δ < 1.253
Ladicky et al. [2014] - - - 0.542 0.829 0.941
Li et al. [2015] 0.232 0.821 0.094 0.621 0.886 0.968
Eigen et al. [2014] 0.215 0.907 - 0.611 0.887 0.971
Laina et al. [2016] 0.127 0.573 0.055 0.811 0.953 0.988
Xu et al. [2017] 0.121 0.586 0.052 0.811 0.954 0.987
Xu et al. [2018b] 0.125 0.593 0.057 0.806 0.952 0.986
Hao et al. [2018] 0.127 0.555 0.053 0.841 0.966 0.991
Fu et al. [2018] 0.115 0.509 0.051 0.828 0.965 0.992
Qi et al. [2018] 0.128 0.569 0.057 0.834 0.960 0.990
Jiao et al. [2018] 0.126 0.416 0.050 0.868 0.973 0.993
Hu et al. [2019] 0.115 0.530 0.050 0.866 0.975 0.993
Our Baseline 0.123 0.547 0.052 0.854 0.969 0.992
Our Baseline + RPD 0.115 0.528 0.050 0.871 0.975 0.993
Ours: Baseline + RPD + ADFF 0.111 0.514 0.048 0.878 0.977 0.994
Eigen and Fergus [2015]* 0.158 0.641 - 0.769 0.950 0.988
Xu et al. [2018a]* 0.120 0.582 0.055 0.817 0.954 0.987
Zhang et al. [2018]* 0.144 0.501 - 0.815 0.962 0.992
Jiao et al. [2018]* 0.098 0.329 0.040 0.917 0.983 0.996
Table 1: Comparisons with state-of-the-art depth estimation approaches on NYUD v2 Dataset. Note that joint task learning is employed in
the methods marked by *. The best results on each metric among the single-task approaches are marked in bold type. The results better than
ours are marked in italics.
each individual scale. We follow the detailed implementa-
tion of MFF proposed in [Hu et al., 2019]. The L feature
maps {Fi } are first resized to the resolution of cur-
ex i=1,...,L
rentscaleusingbilinearinterpolationandrefinedwitharesid-
ual refine block. The refined feature maps are concatenated
and fed into a conv-layer to reduce the number of channels.
3.4 Loss Function
In order to train our residual pyramid network for predict-
ing accurate depth maps while preserving scene structures in
Figure 4: Comparison with [Jiao et al., 2018]. The depth maps pre-
various scales, we compute the difference between the pre-
dicted by our method preserve much more accurate depth around
dicted depth map Di and the ground-truth Gi at each scale
object boundaries and keep finer structures, as highlighted in the
and combine the losses of all scales together. For each scale,
boxes.
we follow the definition of the loss function proposed in [Hu
et al., 2019]. It consists of three terms, l considering the
depth
pixel-wise difference between the predicted depth Dl and the 4.1 Experimental Setup
ground truth Gl, l which penalizes errors round edges, The NYU-Depth v2 dataset [Silberman et al., 2012] con-
grad
and l to further improve fine details. Combing all the tains 464 video sequences of indoor scenes captured with Mi-
normal
L scales, our loss function for the entire network is formu- crosoft Kinect. 654 aligned RGB-Depth pairs are provided
lated as for testing depth estimation methods for indoor scenes. All
L images have a resolution of 640 × 480. To training our net-
(cid:88)
L = li + li + li . (1) work, we use the training dataset which contains 50K RGBD
depth grad normal
images, select and then augment in the same way as [Hu et
i=1
al., 2019]. Each image is downsampled to 320 × 240 using
4 Experiments bilinear interpolation, and then center-cropped to 304 × 228.
The predicted depth maps are in a resolution of 152 × 114.
To demonstrate the effectiveness of the proposed approach, For testing, the predicted depth maps are upsampled to match
we evaluate our approach on the challenging NYUD v2 the size of the corresponding ground truth using bilinear in-
dataset [Silberman et al., 2012]. We compare our approach terpolation.
with a couple of state-of-the-art approaches and show the su- We implement the proposed model using PyTorch [Paszke
periority of the proposed method on both quantitative and et al., 2017]. The encoder, SENet, is initialized by a model
qualitative evaluations. pretrained on ImageNet [Deng et al., 2009]. The other lay-

| 0.232   | 0.821   | 0.094   | 0.621   | 0.886   |
|:--------|:--------|:--------|:--------|:--------|
| 0.215   | 0.907   | -       | 0.611   | 0.887   |
| 0.127   | 0.573   | 0.055   | 0.811   | 0.953   |
| 0.121   | 0.586   | 0.052   | 0.811   | 0.954   |
| 0.125   | 0.593   | 0.057   | 0.806   | 0.952   |
| 0.127   | 0.555   | 0.053   | 0.841   | 0.966   |
| 0.115   | 0.509   | 0.051   | 0.828   | 0.965   |
| 0.128   | 0.569   | 0.057   | 0.834   | 0.960   |
| 0.126   | 0.416   | 0.050   | 0.868   | 0.973   |
| 0.115   | 0.530   | 0.050   | 0.866   | 0.975   |
| 0.123   | 0.547   | 0.052   | 0.854   | 0.969   |
| 0.115   | 0.528   | 0.050   | 0.871   | 0.975   |
| 0.111   | 0.514   | 0.048   | 0.878   | 0.977   |
| 0.158   | 0.641   | -       | 0.769   | 0.950   |
| 0.120   | 0.582   | 0.055   | 0.817   | 0.954   |
| 0.144   | 0.501   | -       | 0.815   | 0.962   |
| 0.098   | 0.329   | 0.040   | 0.917   | 0.983   |

Figure 5: Qualitative results on the NYUD2 dataset.
ers in our network are randomly initialized. We use a step Thres Method Prec Recall F1
learning rate decay policy with Adam optimizer, and starting [Laina et al., 2016] 0.489 0.435 0.454
from an initial learning rate of l = 10−4. It is reduced [Xu et al., 2018a] 0.516 0.400 0.436
init
to 10% every 5 epochs. We use β = 0.9, β = 0.999, and 0.25 [Fu et al., 2018] 0.320 0.583 0.402
1 2
weight decay as 10−4. The proposed network was trained for [Hu et al., 2019] 0.644 0.508 0.562
20 epochs with a batch size of 6. Ours 0.645 0.520 0.570
[Laina et al., 2016] 0.536 0.422 0.463
4.2 Performance Comparison [Xu et al., 2018a] 0.600 0.366 0.439
0.5 [Fu et al., 2018] 0.316 0.473 0.412
Quantitative Evaluation
[Hu et al., 2019] 0.668 0.505 0.568
Following previous studies, we adopt four metrics including
Ours 0.663 0.523 0.578
average relative error (REL), root mean squared error (RMS), [Laina et al., 2016] 0.670 0.479 0.548
mean log10 error (log 10), and accuracy with three thresh- [Xu et al., 2018a] 0.794 0.407 0.525
olds, to quantitatively evaluate our depth estimation perfor- 1.0 [Fu et al., 2018] 0.483 0.512 0.485
mance. Table 1 shows the results of our SARPN and recent [Hu et al., 2019] 0.759 0.540 0.623
approaches. Among the approaches of single task learning,
Ours 0.749 0.554 0.630
our approach performs the best on REL, log 10 error, and
accuracy with three thresholds. We are in the third position
Table 2: Accuracy of recovered edge pixels in depth maps under
with respect to RMS. We speculate that the methods [Fu et different thresholds.
al., 2018; Jiao et al., 2018] pay more attention to the absolute
pixel-wise accuracy when designing their networks and loss
functions, ignoring fine structures of target scenes. As a re- bility of our method on restoring clear object boundaries and
sult, these methods achieve higher performance in RMS, but finer details.
performs worse on the REL metric and other metrics. We also analyze the contribution of each component in our
We also compare our method with four approaches that proposed network. We use a simple UNet-like architecture
employ joint task learning [Eigen and Fergus, 2015; Xu et as our baseline, where SENet [Hu et al., 2018] is employed
al., 2018a; Zhang et al., 2018; Jiao et al., 2018]. The results as the backbone of our encoder. The decoder in our baseline
demonstrated that our method outperforms three methods and employs a multi-stage upsampling scheme to recover a depth
achieves comparative performance with [Jiao et al., 2018], map. A variant (baseline+RPD) is implemented by adding
even they use a large number of extra labels for semantic seg- the proposed RPD on the baseline model. As shown in Ta-
mentation during the training process. Moreover, the depth ble 1, the performance is gradually improved by incorporat-
maps produced by [Jiao et al., 2018] present very blurry ob- ing RPD and ADFF. More specifically, after adding the pro-
ject boundaries and miss geometric details. We compare the posed RPD, performance among all the metrics are improved
predicted depth maps in Figure 4 to demonstrate the capa- by a large margin from the baseline, while REL decreases

|   Thres | Method            | Prec   | Recall   | F1    |
|--------:|:------------------|:-------|:---------|:------|
|    0.25 | [Lainaetal.,2016] | 0.489  | 0.435    | 0.454 |
|         | [Xuetal.,2018a]   | 0.516  | 0.400    | 0.436 |
|         | [Fuetal.,2018]    | 0.320  | 0.583    | 0.402 |
|         | [Huetal.,2019]    | 0.644  | 0.508    | 0.562 |
|         | Ours              | 0.645  | 0.520    | 0.570 |
|    0.5  | [Lainaetal.,2016] | 0.536  | 0.422    | 0.463 |
|         | [Xuetal.,2018a]   | 0.600  | 0.366    | 0.439 |
|         | [Fuetal.,2018]    | 0.316  | 0.473    | 0.412 |
|         | [Huetal.,2019]    | 0.668  | 0.505    | 0.568 |
|         | Ours              | 0.663  | 0.523    | 0.578 |
|    1    | [Lainaetal.,2016] | 0.670  | 0.479    | 0.548 |
|         | [Xuetal.,2018a]   | 0.794  | 0.407    | 0.525 |
|         | [Fuetal.,2018]    | 0.483  | 0.512    | 0.485 |
|         | [Huetal.,2019]    | 0.759  | 0.540    | 0.623 |
|         | Ours              | 0.749  | 0.554    | 0.630 |

Figure 6: 3D projection from predicted depth maps. Our method
better preserves the scene structure of various scales, especially the
Figure 7: More results by applying our model on SUN-RGBD
flat shape of large planar regions.
dataset (a)(b) and ScanNet dataset (c)(d).
by 6.5%, RMS decreases by 3.5%, log10 error decreases by
datasets. We test our network, which is trained on the NYUD
3.8%. After adding the ADFF module, the performance is
v2 dataset only, on ScanNet dataset [Dai et al., 2017] and
further improved, while REL decreases by 3.5%, RMS de-
SUN-RGBD dataset [Song et al., 2015], which contain more
creases by 2.7% and log10 error decreases by 4%.
diverse RGBD data. As shown in Figure 7, even the data dis-
In order to prove the effectiveness of our method on pre-
tribution of these two datasets and NYU Depth v2 is greatly
serving object details, we also compute edge accuracy to
different, our method could recover structures in various
measure the quality of recovered edge details, same as [Hu et
scales, including smooth large planar regions and object de-
al., 2019]. Precision, Recall, and F1 score are computed ac-
tails. Moreover, ourmethodalsofillsholesinthegroundtruth
cording to edge pixels in the ground truth map. From Table 2,
depth map automatically while maintains the scene structure.
we can see that our F1 score surpasses all other methods un-
der three different thresholds. This indicates that our method
restores the most structure details. 5 Conclusion
Qualitative Evaluation In this paper, we propose a Structure-Aware Residual Pyra-
We compare a series of depth maps predicted by our method mid Network for accurate monocular depth estimation. A
and other state-of-the-art methods [Laina et al., 2016; Xu et residual pyramid decoder is introduced to predict multi-scale
al., 2017; Fu et al., 2018; Hu et al., 2019] in Figure 5. It depth maps, which takes the underlying hierarchical scene
can be seen that the depth maps predicted by our method are structures into account. The residual pyramid induces our
visually better than other methods. Scene structures are well network to progressively add finer structures at a specific
preserved in different scales, especially for large planar re- scale while preserving the coarser layout predicted at the up-
gions and object details. For example, our method predicts per level. Meanwhile, by using the proposed adaptive dense
accurate geometric details for the bookshelf in the first row, feature fusion module, the image features from all scales are
the chair in the third row, and the sofa in the fifth row. For adaptively fused when predicting the residual depth map for
large planar regions (the upper-left wall in the second row, each scale. Experiment results demonstrate that our method
and the floor of the third), our method also generates better achieves state-of-the-art performance in both quantitative and
results. qualitative evaluation.
To better illustrate the capability of our method on preserv-
ing scene structure of large planar regions, we project the Acknowledgements
predicted depth maps as 3D point clouds and render them
This work was supported by the National Key Research &
in novel views. As Figure 6 shows, our reprojected results
Development Plan of China under Grant 2018YFC0307905,
are the closest to ground truth. In particular, the large wall
the National Natural Science Foundation of China (NSFC)
regions recovered by our method are much more flat, while
under Grants 61632006, 61622211, and 61620106009, the
other methods suffer from severe distortions.
Priority Research Program of Chinese Academy of Sciences
Model Generalization under Grant XDB06040900, as well as the Fundamental
In addition to the NYUD v2 dataset, we further explore Research Funds for the Central Universities under Grant
the generalization ability of our proposed network on other WK3490000003 and WK2100100030.

References [Long et al., 2015] Jonathan Long, Evan Shelhamer, and
Trevor Darrell. Fully convolutional networks for seman-
[Dai et al., 2017] Angela Dai, Angel X Chang, Manolis
tic segmentation. In IEEE Conference on Computer Vision
Savva, Maciej Halber, Thomas Funkhouser, and Matthias
and Pattern Recognition, pages 3431–3440, 2015.
Nießner. ScanNet: Richly-annotated 3D reconstructions
of indoor scenes. In IEEE Conference on Computer Vision [Park et al., 2017] Seong-Jin Park, Ki-Sang Hong, and Se-
and Pattern Recognition, pages 5828–5839, 2017. ungyong Lee. RDFNet: RGB-D multi-level residual fea-
ture fusion for indoor semantic segmentation. In IEEE In-
[Deng et al., 2009] Jia Deng, Wei Dong, Richard Socher, Li-
ternational Conference on Computer Vision, pages 4980–
Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
4989, 2017.
hierarchical image database. 2009.
[Paszke et al., 2017] Adam Paszke, Sam Gross, Soumith
[Eigen and Fergus, 2015] David Eigen and Rob Fergus. Pre-
Chintala, Gregory Chanan, Edward Yang, Zachary De-
dicting depth, surface normals and semantic labels with
Vito, Zeming Lin, Alban Desmaison, Luca Antiga, and
a common multi-scale convolutional architecture. In
Adam Lerer. Automatic differentiation in pytorch. 2017.
IEEEInternationalConferenceonComputerVision, pages
2650–2658, 2015. [Qi et al., 2018] Xiaojuan Qi, Renjie Liao, Zhengzhe Liu,
Raquel Urtasun, and Jiaya Jia. Geonet: Geometric neural
[Eigen et al., 2014] David Eigen, Christian Puhrsch, and
network for joint depth and surface normal estimation. In
Rob Fergus. Depth map prediction from a single image
IEEE Conference on Computer Vision and Pattern Recog-
using a multi-scale deep network. In Advances in Neural
nition, pages 283–291, 2018.
Information Processing Systems, pages 2366–2374, 2014.
[Qian et al., 2014] Chen Qian, Xiao Sun, Yichen Wei, Xi-
[Fu et al., 2018] Huan Fu, Mingming Gong, Chaohui Wang,
aoou Tang, and Jian Sun. Realtime and robust hand track-
Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal
ing from depth. In IEEE Conference on Computer Vision
regression network for monocular depth estimation. In
and Pattern Recognition, pages 1106–1113, 2014.
IEEE Conference on Computer Vision and Pattern Recog-
nition, pages 2002–2011, 2018. [Silberman et al., 2012] Nathan Silberman, Derek Hoiem,
Pushmeet Kohli, and Rob Fergus. Indoor segmentation
[Hao et al., 2018] Zhixiang Hao, Yu Li, Shaodi You, and
and support inference from rgb-d images. In European
Feng Lu. Detail preserving depth estimation from a sin-
Conference on Computer Vision, pages 746–760, 2012.
gle image using attention guided networks. In 3DV, pages
304–313. IEEE, 2018. [Song et al., 2015] Shuran Song, Samuel P Lichtenberg, and
Jianxiong Xiao. Sun rgb-d: A rgb-d scene understanding
[Hu et al., 2018] Jie Hu, Li Shen, and Gang Sun. Squeeze-
benchmark suite. In Proceedings of the IEEE Conference
and-excitation networks. In IEEE Conference on Com-
on Computer Vision and Pattern Recognition, pages 567–
puter Vision and Pattern Recognition, pages 7132–7141,
576, 2015.
2018.
[Xu et al., 2017] Dan Xu, Elisa Ricci, Wanli Ouyang, Xiao-
[Hu et al., 2019] Junjie Hu, Mete Ozay, Yan Zhang, and
gang Wang, and Nicu Sebe. Multi-scale continuous CRFs
Takayuki Okatani. Revisiting single image depth estima-
as sequential deep networks for monocular depth estima-
tion: Toward higher resolution maps with accurate object
tion. In IEEE Conference on Computer Vision and Pattern
boundaries. In IEEE Winter Conference on Applications
Recognition, pages 5354–5362, 2017.
of Computer Vision, 2019.
[Xu et al., 2018a] Dan Xu, Wanli Ouyang, Xiaogang Wang,
[Jiao et al., 2018] Jianbo Jiao, Ying Cao, Yibing Song, and
and Nicu Sebe. Pad-net: Multi-tasks guided prediction-
Rynson Lau. Look deeper into depth: Monocular depth
and-distillation network for simultaneous depth estimation
estimationwithsemanticboosterandattention-drivenloss.
and scene parsing. In IEEE Conference on Computer Vi-
In European Conference on Computer Vision, pages 53–
sion and Pattern Recognition, 2018.
69, 2018.
[Xu et al., 2018b] Dan Xu, Wei Wang, Hao Tang, Hong Liu,
[Ladicky et al., 2014] Lubor Ladicky, Jianbo Shi, and Marc
Nicu Sebe, and Elisa Ricci. Structured attention guided
Pollefeys. Pulling things out of perspective. In IEEE
convolutional neural fields for monocular depth estima-
Conference on Computer Vision and Pattern Recognition,
tion. In IEEE Conference on Computer Vision and Pattern
pages 89–96, 2014.
Recognition, pages 3917–3925, 2018.
[Laina et al., 2016] IroLaina, ChristianRupprecht, Vasileios
[Zhang et al., 2018] Zhenyu Zhang, Zhen Cui, Chunyan Xu,
Belagiannis, Federico Tombari, and Nassir Navab. Deeper
Zequn Jie, Xiang Li, and Jian Yang. Joint task-recursive
depth prediction with fully convolutional residual net-
learning for semantic segmentation and depth estimation.
works. In 3DV, pages 239–248. IEEE, 2016.
In European Conference on Computer Vision, 2018.
[Li et al., 2015] Bo Li, Chunhua Shen, Yuchao Dai, Anton
[Zheng et al., 2018] Kecheng Zheng, Zheng-Jun Zha, Yang
Van Den Hengel, and Mingyi He. Depth and surface nor-
Cao, Xuejin Chen, and Feng Wu. LA-Net: Layout-aware
mal estimation from monocular images using regression
dense network for monocular depth estimation. In ACM
on deep features and hierarchical crfs. In IEEE Confer-
Multimedia Conference on Multimedia Conference, pages
ence on Computer Vision and Pattern Recognition, pages
1381–1388, 2018.
1119–1127, 2015.