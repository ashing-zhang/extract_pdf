Model-based Statistical Depth with
Applications to Functional Data
9102 peS 62  ]EM.tats[  1v21421.9091:viXra Weilong Zhao1, Zishen Xu1, Yun Yang2, Wei Wu1
1
Department of Statistics, Florida State University
2 Department of Statistics, University of Illinois at Urbana-Champaign
Abstract
Statistical depth, a commonly used analytic tool in non-parametric statistics,
has been extensively studied for multivariate and functional observations over the
past few decades. Although various forms of depth were introduced, they are mainly
procedure-based whose definitions are independent of the generative model for obser-
vations. To address this problem, we introduce a generative model-based approach
to define statistical depth for both multivariate and functional data. The proposed
model-based depth framework permits simple computation via Monte Carlo sampling
and improves the depth estimation accuracy. When applied to functional data, the
proposed depth can capture important features such as continuity, smoothness, or
phase variability, depending on the defining criteria. Specifically, we view functional
data as realizations from a second-order stochastic process, and define their depths
through the eigensystem of the covariance operator. These new definitions are given
through a proper metric related to the reproducing kernel Hilbert space of the covari-
ance operator. We propose efficient algorithms to compute the proposed depths and
establish estimation consistency. Through simulations and real data, we demonstrate
that the proposed functional depths reveal important statistical information such as
those captured by the median and quantiles, and detect outliers.
Keywords: model-based, statistical depth, functional data, stochastic process, Gaussian
process, reproducing kernel Hilbert space.
1

1 Introduction
The notion of statistical depth was first introduced (Tukey 1975) as a tool to visualize bi-
variate data sets, and has later been extended to multivariate data over the last few decades.
The depth is a measure of the centrality of a point with respect to certain data cloud, which
helps to set up center-outward ordering rules of ranks. Alternatively, it can be treated as a
multivariate extension of the notion of quantiles for univariate distributions. For instance,
a deepest point in a given data cloud can be viewed as a “multivariate median”. Based
on different criteria on centrality, a large class of depths has been proposed, including the
halfspace depth (Tukey 1975), convex hull peeling depth (Barnett 1976), simplicial depth
(Liu 1990), L -depth (Vardi & Zhang 2000), and projection depth (Zuo et al. 2003). The
1
concept of statistical depth has been widely applied in outlier detection (Donoho & Gasko
1992), multivariate density estimation (Fraiman et al. 1997), non-parametric description of
multivariate distributions (Liu et al. 1999), and depth-based classification and clustering
(Christmann 2002).
In many research areas such as medicine, biology, and engineering, it is natural to
assume the observations being generated from infinite dimensional models, and analyze
them using tools from functional data analysis (FDA). Many efforts have attempted to
extend the notion of depths from finite to infinite dimension in recent years. To name a
few, Fraiman & Muniz (2001) defined the integrated data depth for functional data based
on integrals of univariate depths, and used it to construct an α-trimmed functional mean to
measure the centrality of given data. This method can reduce the effects of outlier bias in
a sample set compared to the regular mean. In addition, Cuesta-Albertos & Nieto-Reyes
(2008) extended the simple random Tukey depth (also called halfspace depth) to functional
data analysis on a separable Hilbert space. A more comprehensive reviews on different
notions of depths for functional data is provided in Section 1.1.
Despite the broad variety and wide usage of statistical depths for both finite and infinite
dimensional observations in exploratory data analysis, existing depth methods suffer from
two apparent drawbacks: 1) They do not make use of any structural information from the
2

generative model when defining or estimating the depths. Utilizing such information may
enhance the power of the depth in tasks such as hypothesis testing, outlier detection, or
classification. 2) For infinite-dimensional observations such as functional data, most depths
are constructed via aggregating point-wise deviations, which fails to capture deviations of
some more important global features such as phase variability and degree of smoothness.
In this paper, we propose a new model-based framework for defining and estimating
statistical depths for both finite and infinite-dimensional data. In particular, we propose
to incorporate information from the data generative model in defining and estimating
the statistical depth. When applied to functional data, our development leads to a new
class of depths that captures global features such as shape and smoothness level. Our new
model-based depth framework overcomes the aforementioned drawbacks and posses several
attractive features:
1. It permits properly utilizing features in the generative model to define a data-
dependent depth. Both computational efficiency and estimation accuracy of the
depth can be benefited from the generative model via Monte Carlo sampling.
2. The depth criterion is flexible, and can be chosen to better capture the underly-
ing generative mechanism or meet specific application purposes. Depending on the
defining criterion, our framework can result in various forms and generalize commonly
used depth functions.
3. The criterion may properly measure the metric distance between observations. This
naturally leads to the notions of centrality and variability in the given data. In
contrast, traditional depth methods are often procedure-based and do not provide
such measurements.
1.1 Related work on functional depth
Band depth (López-Pintado & Romo 2009) is a very commonly used depth for functional
data, which has been successfully used for tasks such as classification. Another important
concept is half-region depth (López-Pintado & Romo 2011), which is closely related to the
3

band depth. It is considered to be applied to high-dimensional data with efficient computa-
tional cost. Based on the graph representation as in band depth, a number of extensions,
modifications and generalizations have emerged. For example, Agostinelli & Romanazzi
(2013) proposed a so-called local band depth to deal with functional data which is con-
sidered to have multiple centers. It measures centrality conditional on a neighborhood of
each point of the space and provide a tool that is sensitive to local features of the data,
while retaining most features of regular depth functions. Set band depth (Whitaker et al.
2013) was proposed for the nonparametric analysis of random sets, and a generalization
of the method of band depth. Balzanella & Elvira (2015) introduced the spatial variabil-
ity among the curves in the definition of band depth, and proposed a method – spatially
weighted band depth to incorporate the spatial information in the curves ordering.
More progress has been made in recent study of functional depth. Chakraborty et al.
(2014) used the spatial distribution to define a so-called spatial depth, since the spa-
tial distribution possesses an invariance property under a linear affine transformation.
Einmahl et al. (2015) proposed to refine the empirical halfspace depth by setting extreme
value to a so-called “tail” to avoid the problem of vanishing value outside the convex hull
of the data, which benefits for inference on extremity. Narisetty & Nair (2016) introduced
a notion called extremal depth, which satisfies the desirable properties of convexity and
“null at the boundary”, for which integrated data depth and band depth lack. These prop-
erties lead to a central region more resistant to outliers. Based on an elastic-metric-based
measure of centrality for functional data, Cleveland et al. (2018) adopted band depth and
modified band depth to estimate the template for functional data with phase variability.
They also showed their performance on outlier detection with new defined boxplots for
time warping functions.
The rest of this article is organized as follows: In Section 2, we first introduce our
model-based framework for statistical depth. We then illustrate several forms of depth and
their relations to commonly used depths. In Section 3, we elaborate on the application
of our framework to functional data as generated from a second-order stochastic process.
In Section 4, we investigate the statistical consistency of our depth estimation procedure.
4

Simulations and real data analysis is provided in Section 5. Section 6 includes a summary
and discusses some future directions. Other computational details and proofs are deferred
to appendices in the supplementary material.
2 Model-Based Statistical Depth
In this section, we introduce our model-based framework for statistical depth, where the
model-based has two meanings: 1) the depth is defined based on a statistical model; and 2)
the depth estimation procedure is two-stage, where we first estimate the model parameter,
and then use a plug-in procedure for estimating the depth. The former view allows the
depth definition itself to be data-dependent and automatically capture features underlying
the data generating process, and the latter may lead to improved estimation accuracy of
the depths due to the estimation efficiency of the model parameter.
0.4   1  
PDF
sample
0.8
0.3
0.6
0.2
0.4
0.1
True
0.2
MC estimate
Sample average
0  0 
−3 −2 −1 0 1 2 3 −1 −0.5 0 0.5 1
(a) i.i.d. sample (b) CDF estiamte
Method 25% quantile median 75% quantile
True -0.67 0 0.67
MC estimate -0.71 0.00 0.65
Sample average -0.76 -0.26 0.78
(c) quantile estimate
Figure 1: Toy example to compare the Monte Carlo method and sample average. (a) 30
i.i.d. sample points from a standard normal distribution. (b) Cumulative distribution
functions of true model (blue), estimated using Monte Carlo method (red), and estimated
using the sample average (cyan) in the range [-1, 1]. (c) Quantile values at 25%, 50%, and
75% of true and two estimate methods.
5

| 0      | 1      |
|:-------|:-------|
| PDF    | PDF    |
| sample | sample |

| True             | None           |
| MC estimate      |                |
| Sample average   |                |
|:-----------------|:---------------|
|                  | True           |
|                  | MC estimate    |
|                  | Sample average |

To illustrate the benefit in estimation accuracy via model-based procedures, we may
compare the Monte Carlo (MC) method with the simple sample average approach. A toy
example is shown in Fig. 1, where we generate 30 i.i.d. sample points from a standard
normal distribution (Fig. 1a). In the MC method, we estimate mean and standard devia-
tion from the sample, and then generate 2000 Monte Carlo sampling points to estimate the
cumulative distribution within [-1,1]. In contrast, the sample average method estimates the
cumulative distribution with the empirical distribution of the 30 points. This comparison
is shown in Fig. 1b. Moreover, we compare the true and estimated quantiles at 25%, 50%,
and 75% in Fig. 1c. It is apparent that the MC method provides more accurate and robust
result.
To begin with, we provide a general definition of depth by considering it as a functional
of the underlying data generating model. Then, we provide a two-stage estimation proce-
dure for the depth via Monte Carlo sampling. In the rest of the paper, we primarily focus on
functional data for illustration, and the development naturally applies to finite-dimensional
data.
2.1 Depths within statistical models
P
Let = : θ Θ be a family of probability measures indexed by a parameter θ over
θ
P { ∈ }
a function (vector) space ([0, 1]) : = f : [0, 1] R : f 2 = 1 f2(x) dx < .
2 2 0
F ⊂ L { → k k ∞}
For example, P can be the measure of a Gaussian Process GP(m, CR) with parameter
θ
R
θ = (m, C) collecting the mean function m : [0, 1] and the covariance function
→
R
C : [0, 1] [0, 1] . Statistical depth should quantify how large a particular observed
× →
P
trajectory f deviates from certain notion of center f under . For example,
obs c θ
∈ F ∈ F
in the case of the Gaussian Process (GP), a natural choice of the center would be its mean
function.
2.1.1 Definitions of Depths
We will now provide the formal definition of a model-based functional depth, as well as the
associated depth contour and central region. All these statistical terms can be considered as
6

infinite-dimensional generalization of the uni-variate survival function/p-value, quantiles,
and highest-probability region.
Our proposed definition can be either norm-based or inner-product based. We refer
to the norm or inner-product as the criterion in the definition. The norm-based depth is
a generalization over various distance-based forms (see the discussion after the following
definition). In contrast, the inner-product depth is motivated with the classical halfspace
depth by Tukey (1975). We at first define the norm-based depth in the following general
form:
Definition 1. (Norm-based statistical depth: general form): The statistical depth
P
D of f in the model relative to the norm and center f is defined
ng obs θ c
∈ F ∈ P k·k ∈ F
as
P
D (f , , , f ) [0, 1],
ng obs θ c
k · k ∈
where D is strictly decreasing with respect to f f , and D 0 when f f
ng obs c ng obs c
k − k → k − k →
.
∞
Norm-based depths are commonly used in statistics literature. For example, the h-
L2
depth (Nieto-Reyes 2011) and spatial depth (Sguera et al. 2014) are based on the norm,
Lp-depth Lp
the is based on the norm (Zuo & Serfling 2000, Long & Huang 2015), and
the Mahalanobis depth is based on the Mahalanobis norm (Liu et al. 1999). The depth
in Definition 1 generalizes these concepts and provides a broader framework for norm-
based methods. In this paper, we study one specific form of this general definition. This
specific form more resembles conventional depths and satisfies more desirable mathematical
properties. The norm in the definition can be considered as a criterion function to compute
the distance between any observation f and the center f and we denote the criterion
obs c
function as ζ(f , f ) in the rest.
obs c
Definition 2. (Norm-based statistical depth: specific form): The statistical depth
7

P
D of f in model relative to norm and center f is defined as
n obs θ c
∈ F ∈ P k · k ∈ F
P P
D (f , , , f ) : = f : f f f f .
n obs θ c θ c obs c
k · k ∈ F k − k ≥ k − k
h i
Remark 1: We point out that this specific form of depth is a representative of all norm-
based depth in the general form as defined in Definition 1. In fact, D (f ) measures the
n obs
degree of extremeness of the observed function f under any normal-based depth
obs
∈ F
D (f ) in the following sense,
ng obs
P P P
D (f) D (f ) = D (f) D (f ) = f f f f = D (f ).
θ ng ng obs θ n n obs θ c obs c n obs
≤ ≤ k − k ≥ k − k
h i h i h i
Remark 2: One proper way to choose the center f is to minimize P( f f a) for
c c
k − k ≥
any given a > 0. Note that
E f f 2
c
P( f f a) k − k .
c
k − k ≥ ≤ a2
L2
When the norm is inner-product induced (e.g. the classical norm), it is easy to
k · k
know that the optimal f should be the expectation Ef. However, f in general can take
c c
different form, dependent on different selection of the norm.
Based on the definitions of the norm-based depth, we can naturally introduce the
notions of depth contour and central region as follows. We adopt the specific form in
Definition 2 to simplify notation (same notion can be directly applied to the general form).
Definition 3. (Depth contour and central region for norm-based depth): For any
P
α [0, 1], the α-th depth contour in the model relative to the norm and center
θ
∈ ∈ P k · k
f is defined as
c
∈ F
P P
C (α, , , f ) : = f : D (f, , , f ) = α .
n θ c n θ c
k · k ∈ F k · k
n o
P
Also, the α-th central region in the model relative to the norm and center
θ
∈ P k · k
8

f is defined as
c
∈ F
P P
R (α, , , f ) : = f : D (f, , , f ) α .
n θ c n θ c
k · k ∈ F k · k ≥
n o
Based on the multivariate halfspace depth, we now define the inner-product-based
depth. In contrast to the general and specific forms in the norm-based case, the inner-
product-based norm is defined only in a specific form as follows.
Definition 4. (Inner-product-based statistical depth): The statistical depth D of
ip
P
f in the model relative to the inner-product , and a subset of is
obs θ
∈ F ∈ P h· ·i G F
defined as
P P
D (f , , , , ) : = inf f : f, g f , g
ip obs θ θ obs
h· ·i G ∈ F h i ≥ h i
g , g =1
∈G || || h i
Remark 3: There are two apparent differences between Definitions 2 and 4: 1) Definition 2
depends on the center f , whereas Definition 4 is independent of it. However, we will point
c
out in Section 2.1.2 that when the distribution function has a center under certain form
of symmetry, this center should be the deepest point under Definition 4. 2) Definition 4
involves an infimum in order to match the half-region depth (López-Pintado & Romo 2011)
for finite-dimensional Euclidean data. Different from the usual half-region depth where
G
as the range of the infimum is taken as the entire function space , the following lemma
F
shows that for infinite-dimensional functional data, is necessary to be a proper (finite-
G
dimensional) subset to avoid depth value degeneracy. A proof is provided in Appendix E.
P
Lemma 1. Let be the probability measure of a zero-mean Gaussian process GP(0, C),
C
where the eigensystem (λ , φ ) of the covariance operator C has infinite number of
j j ∞j=1
{ }
positive eigenvalues λ . If , is an inner-product over such that the P P Gram
j ∞j=1
{ } h· ·i F ×
matrix [ φ , φ ]P of the first P eigenfunctions φ P is positive definite for any P N ,
j k j,k=1 j j=1
h i { } ∈
then
P
D (f, , , , ) = 0
ip C
h· ·i F
almost surely for f GP(0, C).
∈
9

Remark 4: This lemma indicates that special attention is needed for defining an inner-
product-based depth for infinite-dimensional space . Dutta et al. (2011) also observed
F
this anomalous behavior of halfspace depth in infinite-dimensional spaces. As a conse-
quence, the halfspace depth (where = ) is only meaningful for finite-dimensional space.
G F
In contrast, the norm-based depth can be effective for both finite- or infinite-dimensional
space. To have a proper inner-product-based depth, either itself is finite-dimensional,
F
or we use a finite-dimensional subset as shown in Definition 4.
G
Under this model-based framework, we can naturally estimate the proposed statistical
P
depth D(f , , , f ) via the following two-stage procedure: 1. Find an estimate θ of the
obs θ c
·
parameter θ; 2. Compute the estimated depth D(f , P , , f ) by either using a closed-
obs c b
θ ·
from expression of the depth or by a Monte Carlo method for an approximation. For
b
P
example, when is a GP measure and the depth as a functional of parameter θ may not
θ
admit a closed-form expression, we may resort to Monte Carlo method for estimating the
depth. More details of the estimation will be provided in Appendix A in the supplementary
material of the paper.
2.1.2 Mathematical Properties
Zuo & Serfling (2000) introduced a list of favorable mathematical properties to be satisfied
by good multivariate statistical depths. Based on this, Nieto-Reyes et al. (2016) further
explored the extensions of these properties for functional data. Gijbels et al. (2017) dis-
cussed these properties on commonly used methods such as the random Tukey depth, band
depth, and spatial depth. In this part, we discuss these properties on our norm-based and
inner-product based depths.
Before discussing basic properties of these two types depths, we need to clarify the
concept of “halfspace” with the following definition:
Definition 5. A closed halfspace H for g, h is defined in the form
h,g
∈ F
H = f : f h, g 0 .
h,g
∈ F h − i ≥
n o
10

To make the inner-product-based depth satisfy favorable properties, we need the fol-
lowing assumption on the “center” function f .
c
P
Assumption 1: The distribution of a random function f is halfspace symmetric,
θ
∈ F
P
or H-symmetric, about a unique function f . That is, (f H) 1/2 for every closed
c
∈ ≥
P
halfspace H containing f . Moreover, we assume that (f H) < 1/2 for every closed
c
∈
halfspace H that does not contain f .
c
Now we list four basic properties of the norm-based depth (Definition 2) and the inner-
product-based depth (Definition 4), respectively, as following:
Norm-based depth:
P
P-1. (Linear invariance) Let denote the distribution P of a random variable
θ,F θ
R
F . Then for any a 0 and h ,
∈ F ∈ \ { } ∈ F
P P
D(af + h, , , af + h) = D(f , , , f ).
obs θ,aF+h c obs θ,F c
k · k k · k
P P
P-2. (Maximality at center) D(f , , , f ) = sup D(f , , , f ).
c θ c f obs θ c
k · k obs k · k
∈F
P-3. (Monotonicity with respect to the deepest point) Let the deepest function be
P
f . Then for any f and α (0, 1), D(f , , , f ) D(f + α(f
c obs obs θ c c obs
∈ F ∈ F ∈ k · k ≤ −
P
f ), , , f ).
c θ c
k · k
P
P-4. (Vanishing at infinity) D(f , , , f ) 0 as f .
obs θ c obs
k · k → k k → ∞
Inner-product-based depth:
P
P-1’. (Linear invariance) Let denote the distribution P of a random variable
θ,F θ
R
F . Then for any a 0 and h ,
∈ F ∈ \ { } ∈ F
P P
D (af + h, , , , ) = D (f , , , , ).
ip obs θ,aF+h ip obs θ,F
h· ·i G h· ·i G
P P
P-2’. (Maximality at center) D (f , , , , ) = sup D (f , , , , ).
ip c θ f ip obs θ
h· ·i G obs h· ·i G
∈F
11

P-3’. (Monotonicity with respect to the deepest point) Let the deepest function be
P
f . Then for any f and α (0, 1), D (f , , , , ) D (f +
c obs ip obs θ ip c
∈ F ∈ F ∈ h· ·i G ≤
P
α(f f ), , , , ).
obs c θ
− h· ·i G
P
P-4’. (Vanishing at infinity) D (f , , , , ) 0 as f , f .
ip obs θ obs obs
h· ·i G → h i → ∞
We examine these mathematical properties of the three defined depths in Sec. 2.1.1, as
summarized in Lemma 2 below. The detailed proof is given in Appendix G.
Lemma 2. The three depths in Definitions 1, 2, and 4 satisfy the mathematical properties
given below:
1. Norm-based depth in general form (Definition 1): P-2, P-3, P-4.
2. Norm-based depth in specific form (Definition 2): P-1, P-2, P-3, P-4.
3. Inner-product-based depth (Definition 4, given Assumption 1): P-1’, P-2’, P-3’, P-4’.
2.2 Illustration of the Depth Definitions
We have defined two forms of model-based functional depth – norm-based (as in Definitions
1 and 2) and inner-product-based (as in Definition 4). In this section, we provide some
examples, both finite-dimensional and infinite-dimensional, to illustrate these definitions.
We will at first adopt various norms in D , and then demonstrate the inner-product-based
n
definition. Using these depths one can rank functional data based on their amplitude,
continuity, smoothness, or phase variability. Moreover, we will show that some of the
functional depths can also be directly applied to multivariate data.
2.2.1 Norm-based Depth
Lp-
There are various norms on functional variables. One commonly used is the classical
Lp-norm
norm, with p 1. That is, for f in a proper space, its is
≥
1
f = ( f(t) pdt)1/p.
p
k k | |
0
Z
12

L2-norm,
In particular, the Euclidean distance from 0, is most often used in functional
L2
data analysis. Due to the nature of norm, it is a great tool for data visualization and
ranking based on their own amplitude information. Considering functions in a Sobolev
Lp
Space (Hsing & Eubank 2015), we can also use norm on the derivatives functions to
quantify continuity or smoothness feature. We may consider the norm-based depth in the
following two forms:
P P
1. D (f , , , f ) : = f : f f f f
n obs θ c θ c p obs c p
k · k ∈ F k − k ≥ k − k
h i
2. D (f , P , , f ) : = P f : Drf Drf Drf Drf , where Dr
n obs θ c θ c p obs c p
k · k ∈ F k − k ≥ k − k
h i
indicates r-th order differentiation.
Lp
When we adopt the norm, the resulting depth can approxiamte band depth (López-Pintado & Romo
2009) for functional observations from a distribution with mean 0.
Lp
In addition to having variability in amplitude (characterized by norms), functional
observations often exhibit variability in phase. Such variability has been extensively studied
over the past two decades and various methods were proposed to separate phase and am-
plitude, and quantify each variability in the given data (Ramsay & Li 1998, Liu & Müller
2004, Tang & Müller 2008, Cleveland et al. 2018). In particular, phase is represented with
time warping functions – Let Γ be the set of orientation-preserving diffeomorphisms of the
unit interval [0, 1]: Γ = γ : [0, 1] [0, 1] γ(0) = 0, γ(1) = 1, γ˙ > 0 (the dot indicates
{ → | }
derivative operation), and γ is called a warping function. Given two functions u, v, we
denote γ as the optimal warping from u to v. There are various forms to define the “op-
uv
timal” warping, and here we adopt the well-known Fisher-Rao framework (Srivastava et al.
2011) and
γ = arginf (q(u) γ)√γ˙ q(v)
uv γ Γ 2
k ◦ − k
∈
where denotes function composition and q( ) is a transformation on the given function
◦ ·
defined as q(x) = sign(x˙) x˙ . The degree of warpingness from the identity γ (t) = t
id
| |
q
L2
can be properly measured by two distances, namely, the distance and the Fisher-Rao
distance. We may consider the norm criterion based on each of these distances:
P P
1. D (f , , , f ) : = f : γ γ γ γ
n obs θ c θ ff id 2 f f id 2
k · k ∈ F k c − k ≥ k obs c − k
h i
13

P P
2. D (f , , , f ) : = f : d (γ , γ ) d (γ , γ ) , where d (γ , γ ) =
n obs θ c θ FR ff id FR f f id FR uv id
k·k ∈ F c ≥ obs c
h i
cos 1( 1 γ˙ (t) γ˙ (t)dt) = cos 1( 1 γ˙ (t)dt),
− 0 uv id − 0 uv
q q q
R R
Due to the nature of the Fisher-Rao distance, depth based on this criteria in our framework
is sensitive to smoothness in the warping function.
2.2.2 Inner-product-based Depth
For multivariate data, Tukey’s halfspace depth (Tukey 1975) is one of the most popular
depth functions available in literature. Dutta et al. (2011) investigated an extension on
any Banach space, and proposed a specialization on a Hilbert space . If X is a random
H
element in having the distribution F, then the halfspace depth of an observation x
H ∈ H
is defined as
HD(x, F) = inf P h, X x 0 ,
h {h − i ≥ }
∈H
where , stands for the inner product defined on . Note that the inner-product based
h· ·i H
depth in Definition 4 can be rewritten as
P P P
D (f , , , , ) : = inf f : f, g f , g = inf f f , g 0 .
ip obs θ θ obs θ obs
h· ·i G ∈ F h i ≥ h i h − i ≥
g , g =1 g , g =1
∈G || || h i ∈G || || h i
Therefore, the halfspace depth can be treated as one special case in the proposed frame-
work. However, Lemma 1 illustrates that the halfspace depth may collapse to zero for
infinite-dimensional functional data unless the underlying data generating model is intrin-
sically finite-dimensional. As a consequence, the choice of the range of the infimum in
G
the preceding display becomes important.
In general, there is no simple solution to the above minimization process (Tukey 1975,
Rousseeuw & Ruts 1996, Dutta et al. 2011). However, if the functions are samples from
a finite-dimensional stochastic process, an optimal g can be found in closed forms. For
illustration purpose, let us assume that the data generating process is a finite-dimensional
Gaussian process. Then the minimization takes the the following closed-form (see detailed
14

derivation in Appendix F)
D (f ) = 1 Φ( f ),
ip obs obs H
− k k K
where Φ is the c.d.f. of a standard normal random variable and the norm is the
H
k · k K
induced RKHS norm (formal definitions are provided in Section 3). Note that as Φ is a
c.d.f. function, the depth value of f is in the range [0, 1/2], which is consistent to the
obs
notion of halfspace depth in function space (Dutta et al. 2011). It is well known that,
for the halfspace depth, if we have a symmetric distribution in a Hilbert space, then the
maximum depth is 0.5, and the point of symmetry will achieve at the halfspace median.
In this case, it is easy to see that D (f ) = 1/2 f = 0. Therefore, the median (i.e.
ip obs obs
⇔
function with largest depth value) is our center function f = 0.
c
Remark 5: The above result is based on the assumption that the stochastic process is
a Gaussian process. However, the Gaussianity is only used in the step that the c.d.f. Φ
X µ
is independent of g after the standardization (i.e., X g ), and the results can be
−
→ σ g
generalized to any second-order stochastic process.
Simplifications in Multivariate Data: The above inner-product-based depth can also
be applied to multivariate data where the Gaussian process reduces to a multivariate Gaus-
sian distribution, denoted as (µ, Σ). In particular, the corresponding inner-product cri-
N
terion function reduces to a variant of the well-known Tukey’s halfspace depth, or location
depth (Tukey 1975),
D (x) = inf P X : u, X x 0 ,
ip
u Rd, u =1 { h − i ≥ }
∈ k k
where the new halfspace depth incorporates the second moment information Σ through the
(zero-mean) inner product x, y = xTΣ 1y and the norm x 2 = xTΣ 1x induced from
− −
h i k k
the covariance matrix of the multivariate data generating distribution, and becomes the
G
Rd
unit ball of relative to this inner-product. In the special case when X is a random
realization from a zero-mean multivariate normal distribution, or X (0, Σ), then the
∼ N
15

half-space depth D admits a closed form. More concretely, using the singular value
ip
decomposition on the covariance matrix Σ = UΛUT, where Λ is a diagonal matrix with
elements of eigenvalues λ d , we can express X through a finite-dimensional version of
p p=1
{ }
the Karhunen Loève expansion X = d ξ U , where U is the p-th column of U (i.e.
p=1 p p p
the eigenvector corresponding to λ ), aPnd ξ (0, λ ), p = 1, 2, . . ., d are independent
p p p
∼ N
Rd
random variables. Correspondingly, the depth of any x is given as
∈
d ξ2
p
D (x) = 1 Φ( ).
ip v
− u λ
up=1 p
X
u
t
Note that the maximum depth value computed by this way is the same as maximum via
Tukey’s half space depth, which is 1/2.
3 A New Model-Based Depth for Functional Data
In this section, we apply our proposed depth framework to functional data and propose
a new data-dependent functional depth. As we will illustrate, our new model-based func-
tional depths can capture and adapt to global features such as smoothness and shapes in
the underlying data generating processes. Our proposed methods incorporate information
from the reproducing kernel Hilbert space (RKHS) associated with the covariance operator
of the underlying stochastic process.
3.1 Depths induced by reproducing kernels
We will provide a construction of norm-based depth for zero-mean second-order stochastic
processes , where the norm itself is model-dependent and learned from the data. Recall
F
that a stochastic process f(t) : t [0, 1] is a second-order process if E [f2(t)] < for
{ ∈ } ∞
E
all t [0, 1], so that its covariance function [f(s)f(t)] is well-defined. If the process has
∈
a nonzero mean function m, then we can always subtract the mean by choosing the center
f as m.
c
16

3.1.1 Background on covariance kernels
Since f(t) : t [0, 1] is a second-order process, its covariance kernel K [0, 1] [0, 1]
{ ∈ } ∈ × 7→
R , K(s, t) := E [f(s)f(t)] is a well-defined function for all (s, t) [0, 1]2. In addition, K( , )
∈ · ·
is a symmetric, positive semi-definite real-value function, that is,
i) K(s, t) = K(t, s),
1 1
ii) K(s, t) h(s) h(t) ds dt 0 for any h .
≥ ∈ F
0 0
Z Z
According to Mercer’s Theorem (J Mercer 1909), there exists a sequence of orthonormal
eigenfunctions φ (t), φ (t), over [0, 1] and a sequence of corresponding non-negative
1 2
{ · · ·}
eigenvalues λ λ 0 (Riesz & Nagy 1990) satisfying
1 2
≥ ≥ · · · ≥
1
∞
K(s, t)φ (s)ds = λ φ (t), for any p 1, and K(s, t) = λ φ (s)φ (t), (1)
p p p p p p
≥
0
Z p=1
X
which implies 1 1 K2(s, t) ds dt = λ2. The convergence in Equation (1) is absolute
∞
0 0 p=1 p
and uniform onR [0R, 1] [0, 1] (CuckePr & Zhou 2007).
×
By the Karhunen Loève theorem (Ash 1990), a random observation f has the following
representation
∞
f(t) = f φ (t) (2)
p p
p=1
X
E
where f , f , are uncorrelated random variables with mean f = 0, and variance
1 2 p
· · ·
E f2 = λ . Each coefficient f is unique and can be obtained by f = 1 f(s)φ (s)ds.
p p p p 0 p
In particular, if the stochastic process is a GP, f , f , will be indepenRdent Gaussian
1 2
· · ·
random variables.
3.1.2 Reproducing kernel Hilbert space and its induced norm
Any symmetric, positive semi-definite function K on [0, 1] [0, 1] corresponds to a unique
×
H
RKHS with K as its reproducing kernel (Wahba 1990). We denote this RKHS by with
K
17

inner-product
K(s, ), K(t, ) = K(t, ), K(s, ) = K(s, t).
H H
h · · i K h · · i K
H
Moreover, the reproducing property ensures that for any f , f, K(t, ) = f(t).
K H
∈ h · i K
The inner product induces the RKHS norm f = f, f . This leads to an equiv-
H H
k k K h i K
q
H R
alent definition of the RKHS as = f : [0, 1] , f < . Therefore,
K H
{ → k k K ∞}
H
under the representations in Equations (1) and (2), we have f if and only if
K
∈
f2
f 2 = p < .
k kH K p:λ p>0 λ p ∞
For a rPandom trajectory f from a second-order stochastic process with covariance
∈ F
kernel K, it is important to examine if the norm f 2 is finite. If K has only finite
H
k k K
number of positive eigenvalues, this conclusion certainly holds. However, if K has infinite
f2
number of positive eigenvalues, in general p = (a.s.) since by the SLLN,
p:λ p>0 λ p ∞
P
1 n f2 f2
p a.s. p
E( ) = 1
n λ −→ λ
p=1 p p
X
(the case for GP is discussed in (Wahba 1990)).
Consequently, although the RKHS norm contains important global features
H
k · k K
of the underlying data generating process, we cannot use the RKHS norm to define the
depth since the RKHS norm of the observations are infinite almost surely. For example,
for one-dimensional integrated Bownian motions (Vaart & Zanten 2011), it is known that
smoothness level of the sample trajectories is 0.5 smaller than that of its associated RKHS
(for Brownian motion, see (Karatzas & Shreve 2012)), where the corresponding RKHS
norm coincides with the Sobolev norm. In this paper, we aim to combine these global
features reflected in the RKHS norm into the construction of model-based functional depth.
In particular, to solve this divergent issue of the RKHS norm, we propose a modified RKHS
norm in the construction of our norm induced depth for functional data by weakening the
impact of high-frequency signals, which are usually hard to estimate, on the modified norm.
18

3.2 Depth induced by modified RKHS norm
In this section, we propose a modified inner product structure for functions in . This
F
new inner product will induce a modified RKHS norm that is almost surely finite for the
sample trajectories from the second-order stochastic process.
3.2.1 Modified Inner Product and Norm
Suppose f, g are two random realizations over from a second-order stochastic process
F
with covariance kernel K. Recall the eigen-decomposition K(s, t) = λ φ (s)φ (t) for
∞ p=1 p p p
any s, t [0, 1]. Without loss of generality, we assume all eigenvaluesP λ are positive to
p
∈ { }
avoid zero appearing in the denominator.
Recall the Karhunen Loève expansion, f(t) = f φ (t) and g(t) = g φ (t),
∞ p=1 p p ∞ p=1 p p
with f = 1 f(s)φ (s)ds and g = 1 g(s)φ (s)ds. PIn addition, the RKHS inPduced inner-
p 0 p p 0 p
product anRd norm are given in the fRollowing forms, respectively.
f g
∞ p p 1/2
f, g = and f = f, f
H H
h i K λ k k h i K
p=1 p
X
As we discussed earlier in Sec. 3.1, the RKHS norm diverges almost surely. This divergence
motivates us to a modified inner-product as follows:
f g
f, g := ∞ p p a2
mod p
h i λ
p=1 p
X
where a is any real sequence satisfying a2 < . In practice, we may adopt
p ∞p=1 ∞ p=1 p
{ } ∞
commonly used convergent sequence a = 1 P or a = 1 with s > 1/2. Our
p ps }∞p=1 p √p(logp)s }∞p=1
{ {
idea is to assign a decaying weight to each positive eigenvalue, so that the overall sum
converges after the adjustment. This modified inner product yields a squared modified
RKHS norm as
f2
2 ∞ p 2
f = f, f = a .
mod mod p
k k h i λ
p=1 p
X
19

Straightforward calculations yield E ( f 2 ) = E(f p2) a2 = a2 < . As a conse-
∞ ∞
k kmod p=1 λ p p p=1 p ∞
quence, f < almost surely, and the aboPve modified innPer product and norm are
mod
k k ∞
well-defined for the observed trajectories. We can use this modified RKHS norm to define
a model-based norm-induced depth as described in Section 5.1.
P P
Recall of Definition 2 of depth in Sec. 2.1: D(f , , , f ) = f f
obs θ c θ c
k · k k − k ≥
h
f f . In this case, the central function f = 0 is the mean function in our model; the
obs c c
k − k
i P
norm function is the modified RKHS norm = ; is a probability measure defined
mod θ
k·k k·k
by the probability density on f or f 2 . Apparently, with different settings of the
mod mod
k k k k
decaying sequence a , we will have different probability density for f or f 2 .
p ∞p=1 mod mod
{ } k k k k
It is often intractable to derive a closed-from expression on the density. Fortunately, our
model-based depth framework provides a natural way of estimating the depth through
Monte Carlo sampling, where the coefficients ( f in the Karhunen-Loève expansion) can
p
{ }
be simulated with re-sampling techniques such as the Bootstrap.
3.2.2 Depth estimation procedure and algorithm
Suppose we have n zero-mean independent sample functions f , , f on t [0, 1],
1 n
· · · ∈ F ∈
and our goal is to compute the model-based depth of any observed sample f . We
obs
∈ F
propose an estimation algorithm as follows.
Algorithm I. (Input: functional data f , , f , any observation f , a small
1 n obs
{ · · · }
threshold δ > 0, and a sequence a , , a .)
n 1 n
· · ·
1. Compute the sample mean function fˆ (t) = 1 n f (t), and empirical covariance
n i=1 i
kernel Kˆ (s, t) = 1 n [f (s) fˆ (s)][f (t) fˆ (t)P];
n i=1 i − i −
P
2. Eigen-decompose Kˆ = n λˆ φˆ (s)φˆ (t);
p=1 p,n p,n p,n
P
ˆ ˆ
3. Set λ = 0 if λ < δ ;
p,n p,n n
ˆ
4. Set M = arg max λ δ , and C = M n (minimum of M and n);
n m m,n n n n n
{ ≥ } ∧
ˆ 1 ˆ
5. Compute f = f (t)φ (t)dt for all i = 1, , n and p = 1, , C , and compute
i,p 0 i p,n n
· · · · · ·
fˆ = 1 f (t)φˆ R (t)dt;
p 0 obs p,n
R
20

6. For each p 1, , C , re-sample (with replacement) a large number N of coefficients
n
∈ · · ·
gˆ N based on fˆ , , fˆ ;
j,p j=1 1,p n,p
{ } { · · · }
7. Construct g (t) = C n gˆ φˆ (t);
j p=1 j,p p,n
P
fˆ p2 gˆ j2
8. Compute f 2 = C n a2, and g 2 = C n ,p a2;
|| obs ||mˆ od p=1 λˆ p || j ||mˆ od p=1 λˆ p
p,n p,n
P P
9. Estimate the depth of f using g :
obs j
{ }
1 N
D (f ; g N ) = 1 .
n obs j j=1 f 2 g 2
{ } N k obs kmˆod≤k j kmˆod
j=1
X
The first 4 steps aim to estimate the eigen-system of the covariance kernel via given
observations. In particular, the Karhunen Loève expansion (Ash 1990) is used in Step 2
to decompose the covariance kernel, and offer a method to reconstruct samples. Using
a functional principal component analysis (Ramsay 2005), we retain the eigen-functions
which explain meaningful variance in our system by truncating the empirical eigenvalues
in Step 3 (Nicol 2013).
Steps 5-8 are the second part of the algorithm. They estimate the depth value with the
modified RKHS norm, where we need re-sampling techniques and Monte Carlo approx-
imations. This algorithm can be easily adapted to the multivariate data. In such case,
the dimension of the data is already given and the principal component analysis and the
multivariate metric can be directly applied. Step 9 estimates the probability in the depth
definition by resampling from the empirical distribution of the sample basis expansion
ˆ p
coefficients f for each coordinate p = 1, . . ., C .
i,p i=1 n
{ }
In Appendix B in the supplementary material of the paper, we specialize these devel-
opments to finite-dimensional processes (or multivariate data).
4 Asymptotic Consistency
In this section, we will prove the consistency for the new model-based depths in Sec. 3.
We assume the functional data are fully observed over its domain [0, 1]. This assumption
21

is commonly used in asymptotic theory for various depths in functional data such as the
integrated data depth (Fraiman & Muniz 2001), the band depth (López-Pintado & Romo
2009), the half-region depth (López-Pintado & Romo 2011), and the extremal depth (Narisetty & Nair
2016).
As our framework is model-based, there will be a main difference in the proofs between
our framework and the traditional functional depth methods. In particular, since previous
depths are independent of the generative model, usually an LLN suffices to show the
consistency. In contrast, our method is considerably more involved since the depth itself is
data dependent — it depends on the estimated model or parameters from the observations.
Despite this extra difficulty in the theory, our new model-based depth can better utilize
the generative patterns in the data, and therefore yields better (discriminative) power and
efficiency in a variety of applications.
We start by introducing the notation used throughout in our proofs. Recall that
L2([0,
1]) is the function space supporting the observations, which are generated
F ⊆
E
from a second-order stochastic process with covariance function K(s, t) = [(f(s)
−
E E
(f(s)))(f(t) (f(t)))]. Suppose we have n functional replicates f , , f . Note
1 n
− · · · ∈ F
that the empirical approximation of K(s, t) is Kˆ (s, t) = 1 n [(f (s) 1 n f (s))(f (t)
n i=1 i −n p=1 p i −
1 n f (t))]. It is clear that Kˆ is also a symmetric positPive semi-definitPe kernel. By Mer-
n p=1 p
cePr’s theorem, we have
n
∞ ˆ ˆ ˆ ˆ
K(s, t) = λ φ (s)φ (t) and K(s, t) = λ φ (s)φ (t),
p p p p,n p,n p,n
p=1 p=1
X X
ˆ ˆ ˆ
where eigenvalues λ λ and λ λ λ are non-negative, and their
1 2 1,n 2,n n,n
≥ ≥ · · · ≥ ≥ · · · ≥
corresponding eigenfunctions φ and φˆ n are continuous on [0,1]. In this section,
p ∞p=1 p,n p=1
{ } { }
we primarily study the consistency of the proposed depth in the infinite-dimensional case
N
where λ > 0 for any p . Due to space constraint, a counterpart result in the finite-
p
∈
N
dimensional case where λ = 0 for all p > P, where P , is deferred to Appendix D in
p
∈
the supplementary material.
22

4.1 Depth estimation consistency
We study the general case when all eigenvalues λ are positive. For any f , we
p ∞p=1 obs
{ } ∈ F
have shown in Sec. 3.2 that the squared modified norm
f , φ 2
f 2 = ∞ h obs p i a2 (3)
obs mod p
k k λ
p=1 p
X
L2
where , is the classical inner-product and a is a real-valued sequence satisfying
p ∞p=1
h· ·i { }
a2 < . Based on the modified norm, the depth of f is given as follows:
∞ p=1 p obs
∞
P
P P
d (f ) = D (f , , , 0) = f : f f
mod obs n obs mod mod obs mod
k · k k k ≥ k k
h i
= 1 P f : f 2 f 2 = 1 F( f 2 ), (4)
mod obs mod obs mod
− k k ≤ k k − k k
h i
where F(x) denotes the cumulative distribution function of f 2 for the random function
mod
k k
f.
As given in Algorithm I, the sample version of the squared modified norm is given as
C n f , φˆ 2
2 obs p,n 2
f = h i a (5)
k obs kmˆ od λˆ p
p=1 p,n
X
where C = M n (minimum of M and n) and M = arg max λ δ for a given
n n n n m m n
∧ { ≥ }
small threshold δ > 0. In our framework, we adopt the sample version of the depth of
n
f given as
obs
P 2
d (f ) = f : f f = 1 F( f ). (6)
mod,n obs k kmod ≥ k obs kmˆ od − k obs kmˆ od
h i
In this section, we focus on proving d (f ) converges to d (f ) when n is large.
mod,n obs mod obs
Before we proceed to find consistency of the modified norm, we make the following two
assumptions:
23

Assumption 1. β > 1, C, C , C > 0, s.t.
1 2
∃
C p β λ C p β and λ λ Cp (β+1) p N .
1 − p 2 − p p+1 −
≥ ≥ − ≥ ∀ ∈
Assumption 2. There exists a real sequence b and some constant α > 0, such that
p ∞p=1
{ }
b2 < , and a b p α as p goes to .
p p p p −
∞ ≤ ∞
P
For convenience, we abuse the notation “C” to denote any constant coefficient. Followed
by Assumption 1, it is apparent that the multiplicity of each λ is strictly 1. We point
p
out that Assumption 2 can be easily satisfied in commonly used sequences of a . For
p
{ }
example, if we choose a = p (0.5+γ) for γ > 0, then we can choose b = p (0.5+γ/2) (with
p − p −
α = γ/2). Using the sequence b , we can define another type of modified form for any
p
{ }
f . As compared to modified norm in Equation (3), we only change the sequence
obs
∈ F
a to b . That is,
p p
{ } { }
f , φ 2
f 2 = ∞ h obs p i b2. (7)
obs b p
k k λ
p=1 p
X
Our main convergence result is given in Theorem 1 as follows, where the proof is given
in Appendix H.
Theorem 1. Under Assumptions 1 and 2, if the covariance kernel K has infinite number
of positive eigenvalues λ , then the following holds with probability tending to one as
p
{ }
n ,
→ ∞
sup f 2 f 2 C n κ 0, (8)
|k obs kmˆ od − k obs kmod | ≤ − →
f , f 1, f 1
obs obs obs b
∈F || ||≤ || || ≤
L2
where (C, κ) are some positive constants, is the classical norm and , ,
mod mˆ od
k·k k·k k·k
are the norms defined in Equations (3), (5), and (7), respectively. Moreover, for any
b
k · k
f
obs
∈ F
lim d (f ) = d (f ), (9)
mod,n obs mod obs
n
→∞
where the two depths d (f ) and d (f ) are given in Equations (4) and (6), re-
mod,n obs mod obs
spectively.
24

4.2 Monte-Carlo method and sample average
We have proven the convergence of the sample depth to the population depth. In prac-
tical computation such as Algorithm I, the sample depth is obtained using samples. In
the proposed model-based framework, the depth is computed using Monte-Carlo samples.
Alternatively, we can simply use the given sample and the estimate will be the sample
average. In this subsection, we will prove that either of the methods can lead to accurate
estimate asymptotically.
The main result on the Monte-Carlo approximation and sample average can be sum-
marized in the following two theorems, where the detailed proofs are given in Appendix I.
The main result will be based on the following assumption.
Assumption 3. Let f denote an observed sample from the true model. Then f is sub-
b
k k
Gaussian, that is, there exists some constant σ > 0, such that E [exp(t f )] exp(σ2t2/2)
p b
k k ≤
R
for all t .
∈
This assumption essentially controls the tail probability bound for f as a ran-
b
k k
dom variable. In particular, it controls the maximal norm max f as of order
i=1,...,n i b
k k
O (√log n) for an i.i.d. sample f n of size n from the true model, so that we can ap-
p i i=1
{ }
ply Theorem 1 to control the approximation errors f 2 f 2 uniformly over all
k i kmˆ od − k i kmod
(cid:12) (cid:12)
i = 1, 2, . . ., n. (cid:12) (cid:12)
(cid:12) (cid:12)
P
Theorem 2. Let the sample depth d (f ) = f : f f be estimated
mod,n obs mod obs mˆ od
k k ≥ k k
h i
as: 1 n 1 , where f are observed i.i.d. sample from the true model and
n p=1 kf p kmˆod≥kf obs kmˆod { p }
the moPdel paramenters are estimated from this sample. Then under Assumptions 1, 2, and
3, we have
1 n
1 1 F( f 2 ),
n kf p kmˆod≥kf obs kmˆod → − k obs kmod
p=1
X
in probability as n .
→ ∞
For the Monte Carlo approximation, we consider the simpler case where the true model
is a zero mean Gaussian process with covariance function given by K for technical simplic-
25

ity, and the Monte Carlo samples are also from a zero mean Gaussian process, but with
ˆ
the estimated covariance function K.
Theorem 3. Assume the true model is a zero-mean Gaussian process and let the sample
depth d (f ) = P f : f f be estimated as: 1 N 1 ,
mod,n obs k kmod ≥ k obs kmˆ od N p=1 kg p kmˆod≥kf obs kmˆod
h i
where g are an i.i.d. sample from the estimated distribution. ThPen under Assumptions
p
{ }
1 and 2 we have
1 N
1 1 F( f 2 )
N kg p kmˆod≥kf obs kmˆod → − k obs kmod
p=1
X
almost surely as N, n .
→ ∞
5 Simulation and real data analysis
In this section, we illustrate applications of our proposed model-based depths to synthetic
data and real data.
5.1 Simulation Examples
We will at first use several simulations to illustrate the uses of the norm-based and inner-
product-based forms in Section 2.2.1 and Section 2.2.2 for exploratory data analysis of
both multivariate and functional data. In particular, Simulations 1-2 focus on several
commonly used norms (inner-products) for model-based depth developed in Section 2, and
Simulations 3-4 consider the new model-based functional depth introduced in Section 3.
More simulation examples, including multivariate depth, are provided in Appendix C.
Lp
Simulation 1. In this example, we illustrate the induced norms as criteria functions
which are discussed in the first part of Section 2.2.1. We demonstrate our framework by
observations from zero-mean Gaussian Process with Matérn class kernel on [0, 1]. The
generative formula for Matérn kernel is
21 ν √2ν x x √2ν x x
K (x , x ) = − ( | i − j |)νK ( | i − j |), x , x [0, 1]
M i j ν i j
Γ(ν) l l ∈
26

where K is the modified Bessel function of order ν, and the parameter l is the characteristic
ν
length-scale of the process. For instance, if ν = 1 and l = 1, then the Matérn kernel
2
K (s, t) = exp( s t ), and if ν = 3 and l = 1, K (s, t) = (1+√3 s t ) exp( √3 s t ),
1 2
−| − | 2 | − | − | − |
for s, t [0, 1].
∈
3 3 1
0.8
1.5 1.5
0.6
0 0
0.4
-1.5 -1.5
0.2
-3 -3 0
-2 -1 0 1 2 -2 -1 0 1 2
(a) Given Observations (b) ζ(f,0) = f
2
k k
3 1 3 1
0.8 0.8
1.5 1.5
0.6 0.6
0 0
0.4 0.4
-1.5 -1.5
0.2 0.2
-3 0 -3 0
-2 -1 0 1 2 -2 -1 0 1 2
(c) ζ(f,0) = f (d) ζ(f,0) = f
′ 2 ′′ 2
k k k k
Figure 2: Simulation 1: (a) 30 observed functions, where the red one is generated from
GP(0, K ) and 29 blues ones are generated from GP(0, K ). (b) The 30 functions with
1 2
L2
color-labeled depth using norm. Observations assigned with color closer to red are
considered to be deeper than those assigned with color closer to blue. (c),(d) Same as (b)
L2
except for norm on the first and second-order derivative functions, respectively.
For better visualization, we sample only one function from GP(0, K ) on [0, 1], and
1
then mix it with another n = 29 simulated samples from GP(0, K ) on [0, 1]. All these
2
30 functions are shown in Figure 2(a). It is apparent that the one function from K is
1
near the zero-line, but somewhat “noisy”. In contrast, the 29 functions from K have high
2
variability in the amplitude, but are very smooth. We then color-labeled them differently
27

in Panels (b)-(d) using their depth values with respect to different criterion functions,
L2 L2 L2
namely, norm on each function, norm on the first-order derivative function, and
norm on the second-order derivative function. The results clearly illustrate that criteria
properly characterize the desirable features in the data. In Panel (b), we rank the function
L2
with respect to their norm. The one function from K is near the zero-line and has the
1
highest depth value. In contrast, since this function is not smooth, it has the least depth
values with derivative-based norms in Panels (c) and (d).
Simulation 2. In this example, we illustrate the time warping distance in the depth
computation. We study a set of simulated functions f , , f on [ 3, 3]. For i =
1 21
{ · · · } −
1, , 21, we first simulate a set of functions by h (t) = φ e (t 1.5)2/2 + φ e (t+1.5)2/2,
i i,1 − − i,2 −
· · ·
where φ and φ are i.i.d. normal with mean one and variance 1/16. Let the warping
i,1 i,2
function γ (t) = 6(eai(t+3)/6 1) 3 if a = 0 otherwise γ = γ , where a are equally spaced
i eai 1− i i id i
− 6
−
between 1 and 1. The observations are f (t) = h (γ (t)) on [ 3, 3], i = 1, , 21. At the
i i i
− − · · ·
˜
final step, we add some noise to the original f by f (t) = f (t) + ǫ(t), where ǫ(t) is a
11 11 11
Gaussian process with mean 0 and covariance function C(s, t) = 0.01δ . To simplify the
s,t
˜
notation, we abuse f to denote the noise contaminated f .
11 11
All these 21 functions are shown in Figure 3(a), where we use red line to represent
f and blue lines to represent the others. Before computing depth values, we conduct
11
the Fisher-Rao alignment procedure to align the observed functions and obtain the corre-
sponding time warping functions γˆ (t), , γˆ (t) (Srivastava et al. 2011). Let f denote
1 21 c
{ · · · }
the Karcher mean of f , , f (in the sense of SRVF space). Then the optimal time
1 21
{ · · · }
warping function from f to f is γˆ , i = 1, , 21.
i c i
· · ·
We take the criterion function ζ(f , f ) = γˆ γ for the depth computation. The 21
i c i id 2
k − k
color-labeled functions using depth values are shown in Figure 3(b). In general, functions
in the middle along x-axis have large depth values, whereas those at each side have low
values. In particular, because f stays in the middle of the observations, it has the least
11
L2
warping distance from f and largest depth values. As comparison we also use the well-
c
known Fisher-Rao distance function ζ(f , f ) = d (γˆ , γ ) for the depth computation. The
i c FR i id
21 color-labeled functions using depth values are shown in Figure 3(c). As the Fisher-Rao
28

1 1
1.2 1.2 1.2
0.8 0.8
0.9 0.9 0.6 0.9 0.6
0.4 0.4
0.6 0.6 0.6
0.2 0.2
0.3 0.3 0 0.3 0
-3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3
(a) Observed functions (b) ζ(f ,f ) = γˆ γ (c) ζ(f ,f ) = d (γˆ ,γ )
i c i id 2 i c FR i id
k − k
Figure 3: Simulation 2: (a) 21 observed functions, where f is emphasized in red color.
11
L2
(b) The 21 functions with color-labeled depth via the warping distance γˆ γ . (c)
i id 2
k − k
Same as (b) except for the Fisher-Rao distance d (γˆ , γ ).
FR i id
distance is derivative-based, small perturbation on time warping results in large difference.
The small noise on f makes it have smallest depth value in the 21 functions. For other
11
20 smooth functions, their depth values are consistent to those in Panel (b).
Simulation 3. In this illustration, we demonstrate Algorithm I in Section 3 for modified
norm-based depth estimation on a variant of the continuous-time Brownian Bridge on
[0, 1], with different choices of decaying weight sequences a , where the covariance kernel
p
{ }
function is K(s, t) = min(s, t) st for any s, t [0, 1]. According to the notation in
− ∈
Equations (1) and (2), we have λ = 1 , φ (t) = √2sin(πpt), and can simulate f from
p p2π2 p p
independent Laplace distribution with mean 0 and variance λ for p = 1, 2, (note that
p
· · ·
this is different from the normal distribution (0, λ ) in a Brownian bridge).
p
N
More specifically, we sample f (t) by the linear combination f (t) = 1000 f φ (t), i =
i i p=1 i,p p
{ }
1, , n(= 100) to approximate the infinite-dimensional stochastic procPess. We set N =
· · ·
1000 in the Monte Carlo sampling. We have three different settings to choose the weight
coefficients a : a) a 1, b) a = 1/p, and c) a = 1/[√plog(p + 1)], p = 1, , N. In
p p p p
{ } ≡ · · ·
Case a), there is actually no weight terms, and the modified norm is equal to the RKHS
induced norm. In Figure 4(a), we show the 100 functions with color-labeled using its depth
value from this norm. Note that we compute this norm in a finite-dimensional setup and
29

| 0                | 1   | 2                | 3   | 4                |
|:-----------------|:----|:-----------------|:----|:-----------------|
| 1.2              |     | 1                |     | 1                |
| 0.9              |     | 1.2              |     | 1.2              |
| 0.6              |     | 0.8              |     | 0.8              |
| 0.3              |     | 0.9 0.6          |     | 0.9 0.6          |
| -3 -2 -1 0 1 2 3 |     | 0.4              |     | 0.4              |
|                  |     | 0.6              |     | 0.6              |
|                  |     | 0.2              |     | 0.2              |
|                  |     | 0.3 0            |     | 0.3 0            |
|                  |     | -3 -2 -1 0 1 2 3 |     | -3 -2 -1 0 1 2 3 |

it will diverge to when N is large. It is straightforward to find that
∞
N N N f2
1
f′ 2 = 2π2p2 f2 cos(πpt)2dt = π2p2f2 = i,p = f 2 . (10)
i L2 i,p i,p i H
k k · λ k k K
p=1 Z0 p=1 p=1 p
X X X
L2
That is, the RKHS induced norm is the same as norm on the derivative function, a
common measure of smoothness of a function.
In Cases b) and c), the series satisfies the convergent requirement lim N a2 <
N i=1 P
→∞
, and therefore the modified norms are well-defined. In particular, we find Pthat when
∞
L2
a = 1/p, the classical norm
p
N 1 N 1 N 1 1
f 2 = f2 φ2(t)dt = f2 = f2 f 2 . (11)
k i kL2 i,p p i,p π2 i,p · 1/π2p2 · p2 ∝ k i kmod
0
p=1Z p=1 p=1
X X X
L2
That is, the modified norm in Case b) is proportional to the norm. This explains the
result in Figure 4(b) where the 100 functions are color-labeled using its depth value from
this norm. We can see that high-depth functions are near the zero-line and low-depth
functions are near boundary lines. This depth reflects the traditional functional depths
such as band depth and half-region depth (López-Pintado & Romo 2009, 2011). In Case
c), we use another type of weight coefficient and the depth result is shown in Figure 4(c),
which is very similar to the result in Case b). In summary, we have found that 1) the
modified norms can provide different forms of measurement on the center-out rank on the
given functional observation and some of the special forms are consistent to the classical
norms; and 2) the rank may be robust with respect to different choices of norm.
Simulation 4. We consider a finite-dimensional Gaussian process by selecting a sequence
of orthonormal Fourier basis functions up to order P = 10 on [0, 1] such that
1 p = 1

φ (t) =  √2 cos(πpt) p = 2, 4, 6, 8, 10 ,
p 




√2sin(π(p 1)t) p = 3, 5, 7, 9
−






and a set of coefficients a , , a N(0, I ). Then we generate N = 500 functions via
1 P 10
{ · · · } ∼
30

1 1 1
1 1 1
0.8 0.8 0.8
0 0 0
0.6 0.6 0.6
0.4 0.4 0.4
-1 -1 -1
0.2 0.2 0.2
0 0 0
0 0.5 1 0 0.5 1 0 0.5 1
(a) a = 1 (b) a = 1/p (c) a = 1/[√plog(p + 1)]
p p p
Figure 4: Simulated functions with color-labeled depth. (a) Each function is color-labeled
using its depth value, where the sequence a is constant 1. Observations assigned with
p
{ }
color closer to red are considered to be deeper than those assigned with color closer to blue.
(b) and (c), Same as (a) except that the coefficients a = 1/p and a = 1/1/[√p log(p +
p p
1)], p = 1, , N, respectively.
· · ·
linear combination f = P a φ . Panel (a) in Figure 5 shows n = 21 randomly selected
i p=1 i,p p
samples from f (t), t P[0, 1] N .
i i=1
{ ∈ }
From Panel (b) in Figure 5, it is clear that there exists a significant gap in the decreasing
sequence of estimated eigenvalues, and the gap locates just after the order of the dimension
P = 10. This indicates the correct dimension can be easily estimated. From Panel (c),
we can tell that the squared RKHS norm fits χ2(P) well. This is also consistent to the
above theoretical derivation. The estimated depth values are color-labeled in Panel (d).
L2
We note that the RKHS induced norm does not have a conventional type of norm, so
there is no direct visualization to evaluate the depth in this example. However, we point
out that if the data are generated from two random processes, these depth values can help
differentiate the observations, as illustrated in the following.
In particular, we show how the model-based depth can be used for the classification
purpose, and compare the performance between RKHS norm and the modified one. Specif-
ically, we select a sequence of orthonormal Fourier basis functions up to order P on [0, 1]
such that for p = 1, 2, , P,
· · ·
√2sin(π(p + 1)pt) p is odd
φ (t) = ,
p 
 √2 cos(πpt) p is even



31

| 0       | 1   | 2       | 3   | 4       |
|:--------|:----|:--------|:----|:--------|
| 1       |     | 1       |     | 1       |
| 1       |     | 1       |     | 1       |
| 0.8     |     | 0.8     |     | 0.8     |
| 0 0.6   |     | 0 0.6   |     | 0 0.6   |
| 0.4     |     | 0.4     |     | 0.4     |
| -1      |     | -1      |     | -1      |
| 0.2     |     | 0.2     |     | 0.2     |
| 0       |     | 0       |     | 0       |
| 0 0.5 1 |     | 0 0.5 1 |     | 0 0.5 1 |

10 1.5
1.2
5
eulav-λ 0.9
0
0.6
0.3
-5
0
2 4 6 8 10 12 14
-10
Index of λ
0 0.25 0.5 0.75 1
i
(a) 21 functions from the original sample (b) Estimated eigenvalues
10 1
0.8
5
0.6
0
0.4
-5
0.2
-10 0
0 0.25 0.5 0.75 1
(c) Histogram of squared RKHS norm (d) Samples with color-labeled depth
Figure 5: Finite Gaussian Process Illustration: (a) 21 randomly selected samples; (b)
ˆ ˆ
Estimated eigenvalues λ from the covariance K; (c) Histogram of squared RKHS norm
p
aˆ2
f 2 = 10 i,p , where the red line indicates a fit to chi-square distribution χ2(10); (d)
i H p=1 ˆ
k k Kˆ λ p,n
Estimated depth of the 21 samples with color-label, where blue to red indicates the depth
P
value range of [0, 1].
Then we generate 45 functions as f (t) = P a φ (t), i = 1, , 45 and 5 functions
i p=1 i,p p
· · ·
as f (t) = P b φ (t), i = 46, , 50, wPhere independent coefficients a N(0, 1)
i p=1 i,p p i,p
· · · ∼
and b NP(0, 3). Due to the different variance values on the coefficients, the first 45
i,p
∼
functions are in the main cluster and the last 5 functions are outliers. One special case
when only P = 4 low frequency basis functions are used is shown in Figure 6(a). Because
of the smaller coefficient variance, the first 45 functions are in a main cluster. In contrast,
some of the last 5 function have much larger amplitude and are apparently outliers. In
another example, we use P = 100 basis functions shown in Figure 6(b). As compared to
when P = 4, both main clusters functions and the 5 outliers have much higher frequency
32

components.
1
10 0.8
40 eulaV htpeDetaR noitceteD reiltuO
20 0.6
0
0 -20 0.4
-40
0.2
RKHS
Modified RKHS
-10 0
0 0.25 0.5 0.75 1 0 0.25 0.5 0.75 1 0 10 20 30 40 50
(a) 50 functions with P = 4 (b) 50 functions with P = 100 (c) Depths with P = 4
1 1 1
0.8 0.8 0.8
etaR llarevO
eulaV htpeD
0.6 0.6
0.6
0.4 0.4
0.4
0.2 0.2
RKHS RKHS
0.2
RKHS Modified RKHS Modified RKHS
Modified RKHS 0 0
0 4   10  20  30  40  70  100 4   10  20  30  40  70  100
0 10 20 30 40 50 P P
(d) Depths with P = 100 (e) Overall accuracy rate (f) Outlier detection rate
Figure 6: Classification by depth values: (a) 50 function samples for P = 4, where the
blue ones represent 45 functions in the main cluster and the red ones represent 5 outliers.
(b) Same as (a) except P = 100. (c) Depth values of the 50 functions with P = 4 using
the RKHS norm (green circles over blue lines) and modified RKHS norm (yellow squares
over red lines). (d) Same as (c) except P = 100. (e) The classification accuracy for all 50
functions by using the RKHS norm (blue line) and modified RKHS norm (red line), where
P varies on seven different values 4, 10, 20, 30, 40, 70, and 100. (f) Same as (e) except the
accuracy on the 5 outlier functions.
We have shown that the RKHS induced norm can characterize the smoothness level
(L2
in Equation (10) and the modified RKHS norm can characterize the amplitude level
norm) in Equation (11). We at first use these two norms for the case when P = 4 and the
result on depth values are shown in Figure 6(c). Note that for the simulated 5 outliers,
only 3 of them show large amplitude as compared to the main cluster, and therefore only
these three have relatively lower depth values by using either RKHS norm or the modified
norm. In contrast, when P = 100, the difference on amplitude for the main cluster and
the 5 outliers are apparent. This can be easily seen using the modified norm shown in
33

| 0                 | 1   | 2                 | 3     | 4                |
|:------------------|:----|:------------------|:------|:-----------------|
| 10                |     | 40                | eulaV | 1                |
| 0                 |     | 20                | htpeD | 0.8              |
| -10               |     | 0                 |       | 0.6              |
| 0 0.25 0.5 0.75 1 |     | -20               |       | 0.4              |
|                   |     | -40               |       | 0.2              |
|                   |     | 0 0.25 0.5 0.75 1 |       | RKHS             |
|                   |     |                   |       | Modified RKHS    |
|                   |     |                   |       | 0                |
|                   |     |                   |       | 0 10 20 30 40 50 |

| 0                | 1       | 2                    | 3         | 4                    |
|:-----------------|:--------|:---------------------|:----------|:---------------------|
| 1                | etaR    | 1                    | etaR      | 1                    |
| 0.8              | llarevO | 0.8                  | noitceteD | 0.8                  |
| eulaV            |         | 0.6                  | reiltuO   | 0.6                  |
| 0.6              |         | 0.4                  |           | 0.4                  |
| htpeD            |         | 0.2                  |           | 0.2                  |
| 0.4              |         | RKHS                 |           | RKHS                 |
| 0.2              |         | Modified RKHS        |           | Modified RKHS        |
| RKHS             |         | 0                    |           | 0                    |
| Modified RKHS    |         | 4 10 20 30 40 70 100 |           | 4 10 20 30 40 70 100 |
| 0                |         | P                    |           | P                    |
| 0 10 20 30 40 50 |         |                      |           |                      |

Figure 6(d). As all high frequency basis functions can have large un-smooth level, the
RKHS norm is not able to clearly differentiate 5 outliers from the main cluster. This is
also shown in Figure 6(d).
To measure the classification performance, we set a threshold of 0.1 on the depth value
for all functions. This is done for the number of basis components P being 4, 10, 20,
30, 40, 70, or 100, which varies from highly smooth to highly nonsmooth observations.
The classification result on all 50 functions is shown in Figure 6(e). In particular, we also
show the detection on the 5 outliers in Figure 6(f). When P is small, both norms produce
reasonable classification accuracy around 95% (a couple of errors in the outliers). When
P gets larger, the modified RKHS can capture larger amplitude in the outliers and reach
100% classification accuracy. In contrast, all 50 functions have similar smoothing level
which makes the RKHS norm not able to detect the outliers.
5.2 Real Data Illustration
In this subsection, we apply our proposed method to detect outliers on a real dataset.
The dataset is taken from the SCOP database (Murzin et al. 1995). We take the subset of
proteins with sample size 23 from PDZ domain using PISCES server (Wang & Dunbrack Jr
2003). The data have been pre-processed as described in (Wu et al. 2013), and we get
normalized data where the three componenets are properly rotated and aligned. This
given data are shown in Figure 7(a) as 3-dimensional curves and the three coponents are
shown in Figure 7(b).
L2
This given data has been applied with two different norms; one is the classical norm
L2
on 3-dimensional functions and the other is the norm on the first derivative functions.
The depth values computed by these two different norms are shown in Fig 7(c) and (d),
respectively. We note that the depth results are vey close to each for the two norms – both
methods indicate that the 8th and 12th protein sequences are outliers in our dataset by
using a detection threshold α = 0.05. The two outliers are shown in Figure 7(e) and (f)
as 3-dimensional curves and for 3 coordinate components, respectively. It is apparent that
the depth values successfully detect the outliers in the given data.
34

1
20
xyzeulaV htpeD 0
-20
0.75
0 0.2 0.4 0.6 0.8 1 eulaV htpeDxyzeulaV htpeD
0 0.5
-20
0 0.2 0.4 0.6 0.8 1
0.25
0
-20
0
0 8 16 23
0 0.2 0.4 0.6 0.8 1
Index of f
(a) 3-D data (b) Component-wise data (c) Depth with ζ(f) = f
2
k k
1 20
0
-20
0.75
eulaV htpeDeulaV htpeD 0 0.2 0.4 0.6 0.8 1
20
0.5 0
-20
0 0.2 0.4 0.6 0.8 1
0.25
20
0
0 -20
0 8 16 23
0 0.2 0.4 0.6 0.8 1
Index of f
(d) Depth with ζ(f) = f (e) Outliers in 3-D (f) Outlier components
′ 2
k k
Figure 7: Real data example: (a) 3-dimensional data of 23 PDZ domain observations. (b)
The 3-coordinate components of given observations. (c) Depth values computed using the
L2 L2
classical norm. (d) Depth values computed using the norm on the first derivative
functions. (e) Detected outliers (black dash lines) in 3-dimension by using depth values.
(f) 3-coordinate components of the detected outliers (black dashed lines).
1 1 1 1
0.8
0.75 0.75 0.75
eulaV htpeD
0.6
0.5 0.5 0.5
0.4
0.25 0.25 0.25
0.2
0 0 0 0
0 8 16 24 0 8 16 23 0 8 16 23 0 8 16 23
Index of f Index of f Index of f Index of f
(a) RKHS norm (b) Modified RKHS (c) Band depth (d) Modified band depth
Figure 8: Comparison: (a) Depth values using RKHS induced norm. (b) Depth values
using modified RKHS norm with a = 1/p. (c) Depth values computed using band depth
p
method. (d) Depth values computed using modified band depth method.
For comparison, the depth values obtained by the RKHS norm in our framework are
shown in Figure 8(a), where a lot of functions have low depth values and the two outliers
cannot be clearly identified. Figure 8(b) shows the depth values computed by modified
35

| 0   | 1   | 2                   | 3     | 4          |
|:----|:----|:--------------------|:------|:-----------|
|     | xyz | 20                  | eulaV | 1          |
|     |     | 0                   | htpeD | 0.75       |
|     |     | -20                 |       | 0.5        |
|     |     | 0 0.2 0.4 0.6 0.8 1 |       | 0.25       |
|     |     | 0                   |       | 0          |
|     |     | -20                 |       | 0 8 16 23  |
|     |     | 0 0.2 0.4 0.6 0.8 1 |       | Index of f |
|     |     | 0                   |       |            |
|     |     | -20                 |       |            |
|     |     | 0 0.2 0.4 0.6 0.8 1 |       |            |

| 0          | 1   | 2   | 3   | 4                   |
|:-----------|:----|:----|:----|:--------------------|
| 1          |     |     | xyz | 20                  |
| 0.75       |     |     |     | 0                   |
| eulaV      |     |     |     | -20                 |
| 0.5        |     |     |     | 0 0.2 0.4 0.6 0.8 1 |
| htpeD      |     |     |     | 20                  |
| 0.25       |     |     |     | 0                   |
| 0          |     |     |     | -20                 |
| 0 8 16 23  |     |     |     | 0 0.2 0.4 0.6 0.8 1 |
| Index of f |     |     |     | 20                  |
|            |     |     |     | 0                   |
|            |     |     |     | -20                 |
|            |     |     |     | 0 0.2 0.4 0.6 0.8 1 |

| 0          | 1     | 2          | 3     | 4          | 5     | 6          |
|:-----------|:------|:-----------|:------|:-----------|:------|:-----------|
| 1          | eulaV | 1          | eulaV | 1          | eulaV | 1          |
| 0.75       | htpeD | 0.75       | htpeD | 0.75       | htpeD | 0.8        |
| eulaV      |       | 0.5        |       | 0.5        |       | 0.6        |
| 0.5 htpeD  |       | 0.25       |       | 0.25       |       | 0.4        |
| 0.25       |       | 0          |       | 0          |       | 0.2        |
| 0          |       | 0 8 16 23  |       | 0 8 16 23  |       | 0          |
| 0 8 16 24  |       | Index of f |       | Index of f |       | 0 8 16 23  |
| Index of f |       |            |       |            |       | Index of f |

norm with a = 1/p in our model based depth framework. It is clearly to see that the 8th
p
L2
and 12th functions have lowest depth values, though not as close to 0 as the two norms
in Figure 7.
In addition, we compare our approach to the well-known depth methods – band depth
and its modified version (López-Pintado & Romo 2009). The performance of band depth
shown in Figure 8(c) is very poor, and it implies that there are great difficulties in applying
the band depth due to large variation in the three coordinates. On the other hand, the
result in Figure 8(d) shows that depth values obtained by modified band depth have a
clear large gap between the two outliers and main portion of the data, consistent to the
result in Figure 7, whereas the depth values are distributed in a very narrow range.
6 Summary and Future works
In this article, we have proposed a new framework to define model-based statistical depth
for functional as well as multivariate observations. Our definitions have two forms: norm-
based and inner-product-based. Depending on the selection (of norms), the norm-based
depth can have various center-outward ranks. For the inner-product depth, it is mainly the
generalization of the multivariate halfspace depth. We then focus on using norms which
are naturally defined with the generative model. That is, we use induced RKHS norm
from the finite-dimensional covariance kernel in a second-ordered stochastic process. For
an infinite-dimensional kernel, we have introduced a modified version to avoid the infinity
value on the induced norm. For practical use, we propose efficient algorithms to compute
the proposed depths. Through simulations and real data, we demonstrate the proposed
depths reveal important statistical properties of given observations, such as median and
quantiles. Furthermore, we establish the consistency theory on the estimators.
Statistical depth is an extensively-studied area. However, all previous methods are
either procedure-based or properties-based. To the best of our knowledge, this is the first
model-based investigation. This paper introduced the basic framework, but the model is
limited to covariance-based method. Due to the nature of covariance kernel, our framework
36

have a tendency to deal with second-order stochastic process like Gaussian family well.
We plan to work in a space where higher order statistics can also be important in the
future. In addition, we have discussed four important properties for the proposed depths.
As Gijbels et al. (2017) provided an elaboration on more desirable properties (such as
receptivity and continuity) of statistical depths for functional data, our future work is to
investigate whether our proposed framework would meet those properties. Moveover, since
we obtain median and quantiles by the proposed depths, we can also extend our method
to construct boxplot visualization. Last but not least, we are seeking broader applications
of the new framework in real world problems such as clustering, classification, and outlier
detection.
37

Supplement to Model-based Statistical
Depth with Applications to Functional Data
A Algorithms for general model-based depth
Suppose we have n zero-mean independent sample functions f , , f on t [0, 1],
1 n
· · · ∈ F ∈
and our goal is to compute the model-based depth of any observed sample f . We
obs
∈ F
first describe the norm-based depth estimation algorithm as follows:
Algorithm II. (Input: observations f , , f , any observation f , a threshold
1 n obs
{ · · · }
ǫ > 0, the center function f , and the selected norm , which means ζ(f, f ) = f f
c c c
k · k k − k
for any observation f.)
1. Compute the sample mean function f¯ (t) = 1 n f (t), and empirical covariance
n i=1 i
kernel Kˆ (s, t) = 1 n [f (s) f¯ (s)][f (t) f¯ (t)P];
n i=1 i − i −
P
2. Eigen-decompose Kˆ = n λˆ φˆ (s)φˆ (t);
p=1 p,n p,n p,n
P
ˆ ˆ
3. Choose a number P if λ is the first eigenvalue such that λ < ǫ for sufficiently
P+1 P+1
small ǫ; then Kˆ (s, t) = P λˆ φˆ (s)φˆ (t);
p=1 p,n p,n p,n
P
ˆ 1 ˆ
4. Compute f = f (t)φ (t)dt for all i = 1, , n and p = 1, , P, and compute
i,p 0 i p,n
· · · · · ·
fˆ = 1 f (t)φˆ R (t)dt;
p 0 obs p,n
R
5. Re-sample (with replacement) a large number N of coefficients gˆ N based on
j,p j=1
{ }
the coefficients of fˆ , and construct g (t) = P gˆ φˆ (t);
i,p j p=1 j,p p
{ }
P
6. Estimate the sample depth of f w.r.t. g :
obs j
{ }
1 N 1 N
D (f ; g N ) = 1 = 1 ,
n obs { j }j=1 N (ζ(f obs,f c) ζ(g j,f c)) N ( f obs f c g j f c )
≤ k − k≤k − k
j=1 j=1
X X
where 1 is the indicator function.
( )
·
38

Steps 1-4 are the first part in the algorithm. They aim to estimate the eigen-system of
the covariance kernel via given observations. In particular, the Karhunen Loève expansion
(Ash 1990) is used in Step 2 to decompose the covariance kernel, and offer a method to
reconstruct samples (the background on the Karhunen Loève expansion will be provided
in Section 3). Using a functional principal component analysis (Ramsay 2005), we retain
the eigen-functions which explain meaningful variance in our system. Steps 5-6 are the
second part of the algorithm. They estimate the depth value with the given norm, where
we need re-sampling techniques and Monte Carlo approximations. This algorithm can be
easily adapted to the multivariate data. In such case, the dimension of the data is already
given and the principal component analysis and the multivariate metric can be directly
applied.
Rd
In general, computing a halfspace depth in is a very challenging task. So far,
exact computations can be given only when d = 2 (Rousseeuw & Ruts 1996) and d = 3
(Rousseeuw & Struyf 1998). There are approximation algorithms when d 4 (Zuo 2018).
≥
However, if the data distribution is a multivariate normal, our framework will result in an
optimal solution similar to that obtained for the Gaussian process. For infinite dimensional
GP, Lemma 1 shows that the inner-produt-based depth can only be feasible for finite-
dimensional space. Fortunately, when the random samples are from a finite-dimensional
zero-mean Gaussian process, the depth has simple closed-form (see detail in Appendix A).
We adopt this special case and modify the above algorithm for halfspace depth as follows,
where Steps 4-6 are simplified as follows:
ˆ 1 ˆ
4. Compute f = f (t)φ (t)dt for p = 1, , P;
p 0 obs p,n
· · ·
R
fˆ p2
5. Compute the induced RKHS norm f 2 = P ;
obs H p=1 ˆ
k k Kˆ λ p,n
P
6. Compute the depth as D (f ) = 1 Φ( f ), where Φ(x) denotes the cumula-
ip obs obs H
− || || Kˆ
tive distribution function of a standard normal random variable.
39

B Applications of norm-based depth in finite-dimensional
Process
Finite-dimensional process is a commonly used stochastic process in practical applications.
In particular, this include any finite-dimensional Gaussian Process (GP) and multivari-
ate Gaussian distribution as special cases. In this appendix, we simplify our model in
Section 3.2 into a zero-mean finite-dimensional process, which means that K has a finite
number P of positive eigenvalues, and P will be referred as the dimension of this process.
That is, K(s, t) = P λ φ (s)φ (t). For convenience we denote this kernel as K = K .
j=1 j j j P
One important benePfit in this process is that the associated RKHS norm is always finite and
can be directly used in our construction of norm-induced depth as described in Section 5.1.
B.1 RKHS norm induced depth for finite-dimensional process
L2([0,
Suppose we have functional observation f 1]) from a zero-mean stochastic pro-
∈
cess with covariance kernel K (s, t) = P λ φ (s)φ (t) on [0, 1] [0, 1]. Then f(t) =
P p=1 p p p
×
P f φ (t), where f , , f are uncorPrelated and E(f ) = 0, E(f2) = λ , j = 1, , P.
p=1 p p 1 P p p p
· · · · · ·
PIn particular, when the process is a Gaussian process, f , , f are independent. In this
1 P
· · ·
case, X = f / λ P are i.i.d. samples from a standard normal distribution, and the
p p p p=1
{ }
q
squared induced norm f 2 = P f2/λ = P X2 follows a χ2 distribution with P
H p=1 p p p=1 p
k k K
degrees of freedom, denoted as χ2P(P). P
P P
The computation of depth still depends on Definition 2: D(f , , , f ) = ζ(f, f )
obs θ c θ c
k·k ≥
h
ζ(f , f ) . The central function f = 0 is the mean function in our model; the criterion
obs c c
i P
function ζ(f, g) = f g ; is a probability measure. We can now rewrite the
H θ
k − k K
definition of depth in the following form:
P P
D(f , , , 0) = f : f f
obs P H P H obs H
k · k K k k K ≥ k k K
h i
= 1 P f : f 2 f 2 = 1 F( f 2 ), (12)
P H obs H obs H
− k k K ≤ k k K − k k K
h i
where F( ) denotes the cumulative distribution function (c.d.f.) of f 2 . In the case of
H
· k k K
40

Gaussian process, this is a c.d.f. of χ2(P). Moreover, for any α [0, 1], the α-th depth
∈
contour is rewritten as
C(α, P , , 0) = f : F( f 2 ) = 1 α ,
P H obs H
k · k K ∈ F k k K −
n o
and central region for this model is
R(α, P , , 0) = f : F( f 2 ) 1 α .
P H obs H
k · k K ∈ F k k K ≤ −
n o
Based on the above derivation, it is easy to see that the depth contours defined via
induced RKHS norm on a Gaussian process are P-dimensional ellipsoids, and the center
RP.
of all ellipsoids is the origin in For illustrative purpose, we let P = 2 and (f , f )
1 2
∼
(0, Σ), with Σ = diag(λ , λ ). For any random samples f(t) = 2 f φ (t), we could
1 2 j=1 p p
N
use a point (f , f ) R2 to represent random function f(t), becauPse the coefficients set
1 2
∈
for each f(t) is unique with respect to the eigen-functions basis. In Figure 9, if we have
any (f , f ) locating on the same ellipsoid, their corresponding random observations will
1 2
have the same depth defined by the induced RKHS norm. In particular, when Σ = I ,
2
the depth contours are concentric circles. Moreover, any random observations f(t), whose
coefficients (f , f ) locates inside of α-th contour, will have a larger depth than α.
1 2
4
2
2
f2f2 0 0
0.7 0.7
0.3 0.3
-2
0.1 0.1
0.05 -2 0.05
0.01 0.01
-4
-4 -2 0 2 4 -4 -2 0 2 4
f f
1 1
(a) Σ = I (b) Σ = diag(1,0.25)
2
Figure 9: Illustration depth contours in the 2-D case where x-axis and y-axis represents the
values of f and f , respectively. (a) Depth contours at different levels for zero mean Gaus-
1 2
sian Process with (f , f ) N(0, I ). (b) Same as (a) except that (f , f ) diag(1, 0.25).
1 2 2 1 2
∼ ∼
41

|    | 0.7    | None   |
|    | 0.3    |        |
|    | 0.1    |        |
|    | 0.05   |        |
|    | 0.01   |        |
|:---|:-------|:-------|
|    |        | 0.7    |
|    |        | 0.3    |
|    |        | 0.1    |
|    |        | 0.05   |
|    |        | 0.01   |

|      | None   |
|:-----|:-------|
| 0.7  |        |
| 0.3  |        |
| 0.1  |        |
| 0.05 |        |
| 0.01 |        |
|      | 0.7    |
|      | 0.3    |
|      | 0.1    |
|      | 0.05   |
|      | 0.01   |

B.2 Depth estimation procedure and algorithm
Similar to the infinite-dimensional case, we can derive algorithm to compute depth on
a finite-dimensional stochastic process. Suppose we have n independent random sample
functions f , , f on t [0, 1], and is a zero-mean P dimensional stochastic
1 n
{ · · · } ⊆ F ∈ F
Process. The following algorithm is to compute the depth based on of any observed
F
sample f . In practice when P is unknown, we can set a small threshold ǫ to identify
obs
∈ F
it.
Algorithm III. (Input: functional data f , , f , any observation f , and a
1 n obs
{ · · · }
threshold ǫ > 0.)
1. Compute the sample mean function f¯ (t) = 1 n f (t), and empirical covariance
n i=1 i
kernel Kˆ (s, t) = 1 n [f (s) f¯ (s)][f (t) f¯ (t)P];
n i=1 i − i −
P
2. Eigen-decompose Kˆ = n λˆ φˆ (s)φˆ (t);
p=1 p,n p,n p,n
P
ˆ ˆ
3. Choose a number P if λ is the first eigenvalue such that λ < ǫ for sufficiently
P+1 P+1
small ǫ; then Kˆ (s, t) = P λˆ φˆ (s)φˆ (t);
p=1 p,n p,n p,n
P
ˆ 1 ˆ
4. Compute f = f (t)φ (t)dt for all i = 1, , n and p = 1, , P, and compute
i,p 0 i p,n
· · · · · ·
fˆ = 1 f (t)φˆ R (t)dt;
p 0 obs p,n
R
5. For each p 1, , P, re-sample (with replacement) a large number N of coefficients
∈ · · ·
gˆ N based on fˆ , , fˆ ;
j,p j=1 1,p n,p
{ } { · · · }
6. Construct g (t) = P gˆ φˆ (t);
j p=1 j,p p,n
P
fˆ p2 gˆ j2
7. Compute f 2 = P , and g 2 = P ,p ;
obs H p=1 ˆ j H p=1 ˆ
|| || Kˆ λ p,n || || Kˆ λ p,n
P P
8. Estimate the depth of f using g :
obs j
{ }
1 N
D(f ; g N ) = 1 .
obs j j=1 f 2 g 2
{ } N obs H j H
k k Kˆ≤k k Kˆ
j=1
X
This algorithm is very similar to Algorithm II. The first 3 steps are to estimate the
eigen-system of the covariance kernel via our observations. As there are only finite number
42

P of positive eigenvalues, we can set small threshold to estimate P. Steps 4-8 are to
estimate the modified RKHS norm by resampling based on the eigen-decomposition on the
covariance.
An important special case is when the process is a Gaussian process. In this case,
we have pointed out the squared norm f 2 has a Chi-square distribution. Therefore
H
k k Kˆ
resampling will not be needed and the estimation of depth will be more robust and efficient.
Steps 4-8 can be simplified and modified to the following 3 steps:
ˆ 1 ˆ
4. Compute f = f (t)φ (t)dt for all i = 1, , n and p = 1, , P;
p 0 obs p,n
· · · · · ·
R
fˆ p2
5. Compute the induced RKHS norm f 2 = P ;
obs H p=1 ˆ
k k Kˆ λ p,n
P
6. Compute the depth as D = 1 F( f 2 ), where F(x) denotes the cumulative
obs H
− || || Kˆ
distribution function of χ2(P).
C More simulation examples
Simulation 5. In this example, we illustrate the inner-product criterion in depth com-
putation. We first select a sequence of orthonormal Fourier basis functions up to order
P = 10 on [0, 1] such that
1 p = 1

φ (t) =  √2cos(πpt) p = 2, 4, 6, 8, 10
p 




√2sin(π(p 1)t) p = 3, 5, 7, 9
−






Next we random generate N = 500 coefficient vectors (a , , a ) N following a
i,1 i,10 i=1
{ · · · }
multivariate normal distribution (0, diag(1, ((P 1)/P)2, , (1/P)2)). Then we gener-
N − · · ·
ate N functions via linear combination f = P a φ . We apply Algorithm I for inner-
i p=1 i,p p
product depth discussed in the above sectionPon this simulated data. We display these 500
functions in Figure 10(a), where the five deepest curves are represented in bold red. We
see that these 5 red ones stay in the middle of the sample, which illustrate the effectiveness
of the depth measurement. As a comparison, we also show the result obtained by modified
43

half-region depth (López-Pintado & Romo 2011) and the result is shown in Figure 10(b).
Visually, the five deepest functions displayed in Panel (a) seem to be more centralized near
x-axis, and our method provide better center-outward rank than the modified half-region
depth.
(a) Inner-product-based Depth (b) Modified Half-region Depth
Figure 10: Simulation 3: (a) All 500 simulated functions, where the red curves have the
five deepest values obtained by the proposed method. (b) Same as (a) except that the
depth values are obtained by the modified half-region depth.
Simulation 6. In this example, we illustrate the norm-based depth on a multivariate
data set and compare the Monte Carlo estimate with sample average (as indicated in
R2
the beginning of this section). We at first generate n = 50 random samples in from
multivariate normal distribution (µ, Σ), where
N
0 1 1/3
µ = and Σ = .
   
0 1/3 1/4
   
   
R2,
We choose a Mahalanobis distance as the criterion function, that is, for any x
∈
ζ(x, µ) = (x µ)TΣ 1(x µ). Therefore, it is straightforward to derive the closed form
−
− −
q
of the depth function D(x) = 1 F(ζ(x, µ)2), where F( ) denotes the cumulative distribu-
− ·
tion function of chi-square distribution with 2 degrees of freedom.
We compute the depth value for each of these 50 points by Monte-Carlo-based Algo-
rithm I, and then compare the result to the algorithm integrated with sample average of
these points. We display these 50 points with color label of their depth values in Figure
11(a). Note that the depth value using the Mahalanobis distance criterion ranges from 0
44

to 1 and the distirbution of these depth values approximately follow elliptic contours for
a two dimensional normal distribution. Since we obtain the closed-form depth values, we
can use them to compare the performance of Monte Carlo and sample average method. In
Algorithm I, we generate 5000 re-sampling points in step 5. The results in Figure 11 (b)(c)
show that the depth values computed by Algorithm I are very close to the theoretical ones,
whereas the sample average method does not have the same level of accuracy.
1 1 1
)elpmaS( eulav htpeD
1 0.8 )CM( eulav htpeD 0.8 0.8
0.6 0.6 0.6
2x 0
0.4 0.4 0.4
0.2 0.2 0.2
-1
0 0
0
-2 0 2 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1
x1 Theoretical Depth value Theoretical Depth value
(a) Given observations (b) Depth value comparison 1. (c) Depth value comparison 2.
Figure 11: Simulation 4: (a) 50 points from multivariate normal density with color-labeled
depth by Algorithm I. (b) Depth value comparison: closed-form depth function (x-axis)
vs. Monte-Carlo-based Algorithm I (y-axis) (c) Same as (b) except for sample average
method in y-axis.
D Depth estimation consistency in finite-dimensional
data
N
In this case, P such that λ > 0 and λ = 0, p > P under the notation
P p
∃ ∈ ∀
setup in Section 4 of the main paper. Then K(s, t) = P λ φ (s)φ (t) and Kˆ (s, t) =
p=1 p p p
n λˆ φˆ (s)φˆ (t). Therefore, for any f , we hPave the squared RKHS induced
p=1 p,n p,n p,n obs
∈ F
Pnorm
P f , φ 2
f 2 = h obs p i < , (13)
obs H
k k K λ ∞
p=1 p
X
where , indicates the inner product operation in RKHS with K as reproducing kernel.
h· ·i
45

| 0      | 1                       |
|:-------|:------------------------|
| 1      | 1                       |
| 1      | 0.8 )CM(                |
| 0.8    | 0.6 eulav               |
| 0.6    | 0.4                     |
| 0 2x   | htpeD                   |
| 0.4    | 0.2                     |
| -1 0.2 | 0                       |
| 0      | 0 0.2 0.4 0.6 0.8 1     |
| -2 0 2 | Theoretical Depth value |
| x1     |                         |

| 0   | 1   |
|:----|:----|
|     |     |

Based on the RKHS norm, the depth of f is given as follows:
obs
d(f ) = D (f , P , , 0) = P f : f f = 1 F( f 2 ), (14)
obs n obs H H obs H obs H
k · k K k k K ≥ k k K − k k K
h i
where F(x) denotes the cumulative distribution function of f 2 for all f .
H
k k K ∈ F
As given in Algorithm II in Appendix G, the sample version of the squared modified
norm is given as
n f , φˆ 2
2 obs p,n
f = h i . (15)
k obs kH Kˆ λˆ
p=1 p,n
X
Similar to Case I, we adopt the sample version of the depth of f
obs
d (f ) = P f : f f = 1 F( f 2 ). (16)
n obs H obs H obs H
k k K ≥ k k Kˆ − k k Kˆ
h i
We focus on proving d (f ) converges to d(f ) when n is large. This is shown in Theorem
n obs obs
4 as follows. In this case, neither a convergent weight series a nor Assumption 1 is needed
p
{ }
in the proof of consistency.
N
Theorem 4. If the covariance kernel K has only P( ) positive eigenvalues λ , then
p
∈ { }
we have
sup f 2 f 2 a.s. 0.
obs H obs H
|k k Kˆ − k k K| −→
f , f 1
obs obs
∈F || ||≤
Moreover, for any f
obs
∈ F
lim d (f ) = d(f ).
n obs obs
n
→∞
Proof: As convergence almost surely implies convergence in distribution, it is apparent
a.s.
that we only need to prove the first convergence f 2 f 2 .
obs H obs H
k k Kˆ −→ k k K
Based on the work done by Dauxois et al. (1982) and Bosq (2012), when n is large, we
ˆ a.s. ˆ a.s.
have λ λ > 0 for p 1, , P , while λ 0 for p P + 1, , n (Tran
p,n p p,n
−→ ∈ { · · · } −→ ∈ { · · · }
2008).
We denote K˘ (s, t) = P λˆ φˆ (s)φˆ (t), and we will get K˘ Kˆ a.s. 0 as n .
p=1 p,n p,n p,n
k − k −→ → ∞
Besides, we have Kˆ KP a.s. 0(Dauxois et al. 1982), hence K˘ K a.s. 0.
k − k −→ k − k −→
46

If we denote K 1(s, t) = P φ p(s)φ p(t) and K˘ 1(s, t) = P φˆ p,n(s)φˆ p,n(s) ,
− p=1 λ − p=1 λˆ
p p,n
P P
K˘ 1 K 1 = K˘ 1(K˘ K)K 1
− − − −
k − k k − k
K˘ 1 K˘ K K 1
− −
≤ k kk − kk k
1 1
˘ a.s.
= K K 0.
λˆ λ k − k −→
p,n p
In Algorithm 2, the estimated depth is written as
P f , φˆ 2
f 2 = h obs p,n i
k obs kH K˘ λˆ
p=1 p,n
X
P 1 1 φˆ (s)φˆ (s)
p,n p,n
= f (s)f (t) dsdt
obs obs ˆ
0 0 λ
p=1Z Z p,n
X
1 1
= f (s)f (t)K˘ 1(s, t)dsdt
obs obs −
0 0
Z Z
Therefore,
1 1 1 1
f 2 f 2 = f (s)f (t)K˘ 1(s, t)dsdt f (s)f (t)K 1(s, t)dsdt
obs H obs H obs obs − obs obs −
| k k K˘ − k k K | | − |
0 0 0 0
Z Z Z Z
= f , (K˘ 1 K 1)f
obs − − obs
| h − i |
2 ˘ 1 1 a.s.
f K K 0.
obs − −
≤ k k k − k −→
E Proof of Lemma 1
To better streamline the proof, we first prove the claimed result when the inner-product is
induced from the reproducing kernel Hilbert space (RKHS) associated with the covariance
function of the GP (which satisfies the Gram matrix condition of the lemma), and then
extend the proof to a general inner product. Specifically, we show:
1. (basic form) If we take the induced RKHS (reproducing-kernel Hilbert space) inner-
47

product , using the covariance function C, then
h· ·i
P
D (f , , , , ) = 0
ip obs C
h· ·i F
almost surely for f GP(0, C)
obs
∈
2. (general form) The above result will in fact hold for any inner-product on that
F
satisfies the condition in the lemma.
Proof: (Part 1) Based on the result in Sec 3.1.2, assume the covariance function C( , )
· ·
in a Gaussian process GP(0, C) has infinite number of positive eigenvalues. Then the
covariance can be represented as C(s, t) = λ φ (s)φ (t). For any f GP(0, C), let
∞ p=1 p p p obs
∈
f = 1 f (s)φ (s)ds. We have f (t) P= f φ (t). Hence, the induced RKHS
obs,p 0 obs p obs ∞ p=1 obs,p p
norm R P
f2
∞ obs,p
f bs = = (a.s.)
o H
k k C λ ∞
p=1 p
X
For any integer P > 0 and function f , we let fP represent the finite cutoff of f at
∈ F
the P-th order. That is, fP(t) = P f φ (t). Let denote the finite-dimensional space
p=1 p p P
G
expanded by φ (t) P . Using thPe result in Appendix A, the inner-product depth
p p=1
{ }
D (fP , P , , , ) = 1 Φ( fP ). (17)
ip obs C h· ·i GP − k obs kH C
f2
Note that fP = P obs,p (a.s.) as P . Then 1 Φ( fP ) 1
H H
k obs k C p=1 λ p → ∞ → ∞ − k obs k C → −
Φ( ) = 1 1 = 0 (a.sP.) Finally, we have
∞ −
D (f , P , , , ) inf D (fP , P , , , ) 0. (a.s.)
ip obs C ip obs C P
h· ·i F ≤ P h· ·i G →
(Part 2) We see that the proof of Part 1 mainly relies on the result in Appendix A
(Equation (17)), where we use the induced RKHS inner-product. Let f be a realization
from the Gaussian process GP(0, C). Here we just need to show that using the new inner-
product, such equation will still hold. Again, we consider the finite cut-off of fP at the
obs
P-th order, and will show that Equation (17) remains valid with the new inner product
48

, . Therefore, we suppress the superscript P in proving this equation in the rest of this
h· ·i
part. Under this notation, we can write
P
H
f(t) = f φ (t) ,
p p K
∈
p=1
X
P
H
g(t) = g φ (t) ,
p p K
∈
p=1
X
P
H
f (t) = f φ (t) = 0 ,
obs obs,p p K
6 ∈
p=1
X
E
where f are independent normal random variables with f = 0 and V arf = λ , p =
p p p p
1, , P.
· · ·
For this new inner-product , on , let r = φ , φ for 1 i, j P. We also denote
ij i j
h· ·i F h i ≤ ≤
P P P P P P
X = f f , g = (f f )g r = f g r + f g r .
obs i obs,i j ij obs,i j ij i j ij
h − i − −
i=1 j=1 i=1 j=1 i=1 j=1
X X X X X X
It is straightforward to know that X is normally distributed with E X = P P f g r :
i=1 j=1 obs,i j ij
−
= µ and V arX = P ( P g r )2λ : = σ2. Now we can compute the PprobaPbility
g i=1 j=1 j ij i g
P P
X µ µ X µ µ
P P P g g P g g
f f , g 0 = X 0 = − = 1 − .
θ obs θ θ θ
h − i ≥ ≥ σ ≥ −σ − σ ≤ −σ
g g g g
h i h i h i h i
P µ
With the normal assumption, f f , g 0 = 1 Φ( g ) where Φ is the c.d.f.
θ h − obs imod ≥ − −σ g
h X µ i
of a standard normal random variable g (it does not depend on g). To minimize the
−
σ
g
probability with respect to g, we need to maximize µ /σ , or µ2/σ2.
g g g g
−
By the Cauchy inequality, we have
µ2 ( P P f g r )2 ( P f obs,i√λ P g r )2
g i=1 j=1 obs,i j ij i=1 √λ i j=1 j ij
= = i
σ g2 P P i=1P( P j=1 g jr ij)2λ i P P i=1( P j=1 gP jr ij)2λ i
PP (f oPbs,i)2 P (√λ P g Pr )2 P f2
i=1 √λ i=1 i j=1 j ij obs,i
i =
≤ P P (P P g r P)2λ λ
i=1 j=1 j ij i i i
X
P P
The equality holds if and only if there exists c > 0 such that cf obs,i = √λ P g r , i =
√λ i j=1 j ij
i
P
49

1, , P. That is,
· · ·
P f
obs,i
g r = c , i = 1, 2, . . ., P.
j ij
λ
j=1 i
X
Under the condition on the inner-product in the lemma, this set of linear equations always
µ
admits a unique solution. By plugging-in this solution, the maximum of g is obtained
−σ
g
at
P f P g r P f cf obs,i P f2
i=1 obs,i j=1 j ij i=1 obs,i λ obs,i
− = i = = f .
− PP i=1( P j=1Pg jr ij)2λ i P P i=1(cf o λbs,i)2λ i v u uXi=1 λ i k obs kH C
i
t
q q
P P P
Finally, the depth of f is still given in the following form:
obs
P
D (f , , , , ) = 1 Φ( f ).
ip obs θ obs H
h· ·i F − k k C
F Optimal solution of depth with inner-product-based
criterion
L2([0,
We assume that f ( 1])) is random realizations from one zero-mean Gaus-
∈ F ⊂
sian process with covariance kernel K in a finite Karhunen Loève expansion K(s, t) =
P λ φ (s)φ (t), s, t [0, 1]. As discussed in Sec. 3.1.2, the realizations from the Gaus-
p=1 p p p
∈
sPian process form an RKHS H . Let
K
P
H
f(t) = f φ (t) ,
p p K
∈
p=1
X
P
H
g(t) = g φ (t) ,
p p K
∈
p=1
X
P
H
f (t) = f φ (t) = 0 ,
obs obs,p p K
6 ∈
p=1
X
E
where f are independent normal random variables with f = 0 and V arf = λ , p =
p p p p
1, , P.
· · ·
50

Using the inner-product, we denote
P (f f )g P f g P f g
p obs,p p obs,p p p p
X = f f , g = − = + .
obs H
h − i K λ − λ λ
p=1 p p=1 p p=1 p
X X X
It is straightforward to know that X is normally distributed with E X = P f obs,pg p : =
− p=1 λ p
µ and V arX = P g p2 : = σ2. Now we can compute the probability P
g p=1 λ g
p
P
X µ µ X µ µ
P P P g g P g g
f f , g 0 = X 0 = − = 1 − .
θ obs H θ θ θ
h − i K ≥ ≥ σ ≥ −σ − σ ≤ −σ
g g g g
h i h i h i h i
P µ
With the normal assumption, f f , g 0 = 1 Φ( g ) where Φ is the c.d.f.
θ obs H
h − i K ≥ − −σ g
h X µ i
of a standard normal random variable g (it does not depend on g). To minimize the
−
σ
g
probability with respect to g, we need to maximize µ /σ , or µ2/σ2.
g g g g
−
Let
f 1
obs,p
a = , b = .
p p
λ
λ
p
p
q
Then use the Cauchy inequality,
µ2 ( a g )2 ( a pb g )2 (a p)2 (b g )2 P a
g = p p p = p b p p p p b p p p p = ( p )2.
σ2 b2g2 P b2g2 ≤ P Pb2g2 b
g P p p p p p p p p p p=1 p
X
P P P
a
The equality holds if and only if there exists c > 0 such that c p = b g , p = 1, , P.
p p
b p · · ·
That is,
a f
p obs,p
g = c = c λ = cf .
p p obs,p
b2 · λ ·
p p
With the constraint g = 1,
H
|| || K
P g2 P c2f2 P f2
1 = g, g = p = obs,p = c2 obs,p = c2 f 2 .
H obs H
h i K λ λ λ k k K
p=1 k p=1 p p=1 p
X X X
Therefore, c2 = 1 and g = f obs,p . We have found the optimal solution
kf obs k2 H K p kf obs kH K
P f f (t)
P obs,p obs
g (t) = arginf f f , g 0 = φ (t) = .
∗ g ∈F, ||g ||H K=1 θ h h − obs iH K ≥ i p X=1 kf obs kH K p kf obs kH K
51

With this optimal g ,
∗
P f obs,pg p P f obs,p f obs,p
µ
g∗ = − p=1 λ p = p=1 λ p · kf obs kH K = f .
obs H
−σ g∗ − P P g p2 P f o2 k k K
P bs,p 1
r p=1 λ p s p=1 kf obs k2 H · λ p
K
P
P
Finally, the depth of f is given in the following form:
obs
P
D (f ) := D (f , , , , ) = 1 Φ( f ).
ip obs ip obs θ obs H
h· ·i F − k k K
G Proof of Lemma 2
Proof: 1) norm-based depth in general form:
P
P-2: By definition, the general depth D (f , , , f ) is strictly decreasing with
n obs θ c
• k · k
P
respect to f f . As f f f f = 0, we have D (f , , , f )
obs c obs c c c n obs θ c
k − k k − k ≥ k − k k·k ≥
P
D (f , , , f ).
n c θ c
k · k
P-3: For any α (0, 1), f + α(f f ) f = α f f f f . By
c obs c c obs c obs c
• ∈ k − − k k − k ≤ k − k
P P
Definition 1, D (f + α(f f ), , s, f ) D (f , , , f ).
n c obs c θ c n obs θ c
− ≥ k · k
P-4: Obvious.
•
2) norm-based depth in specific form:
P P
P-1: D (af + h, , , af + h) = f : af + h (af + h) af +
n obs θ,aF+h c θ c obs
• k · k k − k ≥ k
P h P
h (af + h) = f : f f f f = D (f , , , f )
c θ c obs c n obs θ,F c
− k k − k ≥ k − k k · k
i h i
P P P
P-2: D (f , , , f ) = f : f f f f f : f f 0 =
n obs θ c θ c obs c θ c
• k · k k − k ≥ k − k ≤ k − k ≥
P h i h i
D (f , , , f )
n c θ c
k · k
P P
P-3: For any α (0, 1), D (f + α(f f ), , s, f ) = f : f f f +
n c obs c θ c θ c c
• ∈ − k − k ≥ k
P h P
α(f f ) f = f : f f α (f f ) f : f f
obs c c θ c obs c θ c
− − k k − k ≥ k − k ≥ k − k ≥
i P h i h
f f = D (f , , , f ).
obs c n obs θ c
k − k k · k
i
52

P P P
P-4: D (f , , , f ) = f : f f f f f : f f
n obs θ c θ c obs c θ obs
• k · k k − k ≥ k − k ≤ k k ≥ k k −
h i h
2 f 0 (as f ).
c obs
k k → k k → ∞
i
3) inner-product-based depth:
P-1’:
•
P
D (af + h, , , , )
ip obs θ,aF+h
h· ·i G
P
= inf f : af + h, g af + h, g
θ obs
∈ F h i ≥ h i
g , g =1
∈G || || h i
P
= inf f : a f, g a f , g
θ obs
∈ F h i ≥ h i
g , g =1
∈G || || h i
P
= D (f , , , , )
ip obs θ,F
h· ·i G
P-2’: It is straightforward to prove this property followed by Assumption 1. For any
•
g , it is easy to verify that the set f : f f , g 0 is a closed halfspace
c
∈ G ∈ F h − i ≥
n P o P
that contains f . By Assumption 1, D (f , , , , ) = inf f :
c ip c θ,F g , g =1 θ
h· ·i G ∈ F
∈G || ||
P h
f f , g 0 1/2. Assume h(= f ) satisfies that D (h, , , , ) >
c c ip θ,F
h − i ≥ ≥ 6 ∈ F h· ·i G
i P P
1/2. Then for any g , f : f h, g 0 > 1/2. Hence, is also
θ θ
∈ G ∈ F h − i ≥
h i
H-symmetirc about h, contradicting to Assumption 1 that f is unique. Therefore,
c
P P
D (f , , , , ) = sup D (f , , , , ).
ip c θ f ip obs θ
h· ·i G obs h· ·i G
∈F
P-3’: For any f (= f ) , we need to prove that for any α (0, 1),
obs c
• 6 ∈ F ∈
P P
inf f : f f , g 0 inf f : f (f + α(f f )), g 0 .
θ obs θ c obs c
∈ F h − i ≥ ≤ ∈ F h − − i ≥
g , g =1 g , g =1
∈G || || h i ∈G || || h i
In fact, note that f f : f f , g 0 f f : f (f + α(f f )), g
c obs c c obs c
∈ ∈ F h − i ≥ ⇔ ∈ ∈ F h − − i ≥
n o n
0 . By Assumption 1, we only need to consider g such that the halfspace does not
o
53

contain f . Therefore,
c
P
inf f : f f , g 0
θ obs
∈ F h − i ≥
g , g =1
∈G || || h i
P
= inf f : f f , g 0
θ obs
∈ F h − i ≥
g , g =1, f f ,g <0
c obs
∈G || || h − i h i
P
inf f : f f , g (1 α) f f , g
θ obs c obs
≤ ∈ F h − i ≥ − h − i
g , g =1, f f ,g <0
c obs
∈G || || h − i h i
P
= inf f : f (f + α(f f )), g 0
θ c obs c
∈ F h − − i ≥
g , g =1, f f ,g <0
c obs
∈G || || h − i h i
P
inf f : f (f + α(f f )), g 0 .
θ c obs c
≤ ∈ F h − − i ≥
g , g =1
∈G || || h i
P-4’:
•
P P
D (f , , , , ) = inf f : f f , g f f , g
ip obs θ,F θ c obs c
h· ·i G ∈ F h − i ≥ h − i
g , g =1
∈G || || h i
P P
= inf f : f, g f , g f : f, f f , f
θ obs θ obs obs obs
∈ F h i ≥ h i ≤ ∈ F h i ≥ h i
g , g =1
∈G || || h i h i
P
f : f, f f , f f , f (Cauchy inequality)
θ obs obs obs obs
≤ ∈ F h ih i ≥ h i
h q i
P
= f : f f 0 (as f )
θ obs obs
∈ F k k ≥ k k → k k → ∞
h i
H Proof of Theorem 1
Proof: Throughout the proof, we use letter C to denote some constant whose meaning may
change from line to line. According to Lemma 14 in Tran (2008), we have E Kˆ K 2
k − k ≤
∞
C n 1. Therefore, by Markov’s inequality
−
log n √n
P ( Kˆ K ) ( )2E ( Kˆ K 2 ) C(log n) 2 0.
−
k − k∞ ≥ √n ≤ log n k − k ≤ →
∞
ˆ logn P
Let denote the event K K . Then, ( ) 1 as n .
A {k − k∞ ≤ √n } A → → ∞
ˆ ˆ
Recall that in Algorithm I, we set λ = 0 if λ is less than a threshold δ satisfying
p,n p,n n
β
√n logn
δ 0 and δ C( ) . Then for a sufficiently large n, we have δ 2 .
−2β+1
n n n
→ ≥ logn ≥ √n
Consequently, the M = arg max λ δ as defined in Equation (5) satisfies M
n m m n n
{ ≥ } → ∞
54

as n . In addition, using Assumption 1, we have
→ ∞
√n
β
C M β λ δ C( ) , C (M + 1) β λ < δ ,
1 n− M n −2β+1 2 n − M n+1 n
≥ n ≥ ≥ log n ≤
implying C δ 1/β M C δ 1/β C( √n )2β1 +1. In addition, under event , we have,
1′ n− n 2′ n−
≤ ≤ ≤ logn A
by Weyl’s theorem and the definition of M , that
n
log n δ λ λ
ˆ ˆ n M p
arg max λ λ K K n ,
p,n p
| − | ≤ k − k∞ ≤ √n ≤ 2 ≤ 2 ≤ 2
1 p M
n
≤ ≤
where in the last step we have used the fact that λ is a nonincreasing sequence. Conse-
p
ˆ λ
quently, λ p holds for each p = 1, , M .
p,n n
≥ 2 · · ·
By Proposition 16 in Tran (2008) and our Assumption 1 on the eigenvalues, we obtain
that for each p = 1, . . ., M ,
n
C
ˆ ˆ
φ φ K K
p,n p
k − k ≤ min λ λ , λ λ k − k∞
p 1 p p p+1
{ − − − }
C log n log n
Kˆ K Cpβ+1 CMβ+1 .
≤ p (β+1)k − k∞ ≤ √n ≤ n √n
−
By combining this with the bound on M , we obtain
n
β+1 1
ˆ β+1 log n n4β+2−2 −β β
φ φ C δ n− β C log n = Cn 4β+2(log n) 2β+1 0.
p,n p
k − k ≤ √n ≤ (log n)(β+1)/(2β+1) →
By the Cauchy-Schwarz inequality, for any p 1, , M ,
n
∈ { · · · }
f , φˆ 2 f , φ 2 = f , φˆ + φ f , φˆ φ
obs p,n obs p obs p,n p obs p,n p
|h i − h i | |h ih − i|
f 2( φˆ + φ ) φˆ φ
obs p,n p p,n p
≤ k k k k k k k − k
= 2 f 2 φˆ φ .
obs p,n p
k k k − k
Combing the last two displays, we obtain
f , φˆ 2 f , φ 2 C f 2 pβ+1log n C f 2 n −β (log n) β ,
4β+2 2β+1
obs p,n obs p obs obs
|h i − h i | ≤ k k √n ≤ k k
55

and for each p = 1, 2, . . ., M ,
n
f , φˆ 2 f , φ 2 log n log n
h obs p,n i − h obs p i C f 2 p2β+1 C f 2 M2β+1 C f 2.
obs obs n obs
λ ≤ k k √n ≤ k k √n ≤ k k
(cid:12) p (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
Now we are ready to prove Equation (8). By Assumption 2, it is easy to verify that
the series f obs,φ p 2 a2 is uniformly convergent for any f 1 (as for N sufficiently
∞ p=1 h λ p i p k obs kb ≤
large, P f obs,φ p 2 a2 N 2α f obs,φ p 2 b2 N 2α f ). Therefore, according to
p ≥N h λ p i p ≤ − p ≥N h λ p i p ≤ − k obs kb
AssumPption 2, for each N 1P, we have f obs,φ p 2 a2 < N 2α and a2 <
≥ ∞ p=N+1 h λ p i p − ∞ p=N+1 p
N 2α b2 C N 2α. According to the errPor bounds on λˆ and f , φˆ ,Pwe have that
− p p − p,n obs p,n
≤ h i
underPevent ,
n
A
f , φˆ 2 f , φ 2 log n
h obs p,n i h obs p i < C f 2 δ + N2β+1 , p = 1, . . ., N,
| λˆ − λ | k obs k n √n
p,n p (cid:18) (cid:19)
M n n f , φˆ 2 f , φ 2 M n n
∧ h obs p,n i − h obs p i a2 C f 2 ∧ a2 C f 2 N 2α.
p obs p obs −
λ ≤ k k ≤ k k
(cid:12)p=N+1 p (cid:12) p=N+1
(cid:12) X (cid:12) X
(cid:12) (cid:12)
(cid:12) (cid:12)
Therefore, we obtain
M n n f , φˆ 2 M n n f , φˆ 2
∧ h obs p,n i a2 2 ∧ h obs p,n i a2
λˆ p ≤ λ p
p=N+1 p,n p=N+1 p
X X
M n n f , φˆ 2 f , φ 2 M n n f , φ 2
2 ∧ h obs p,n i − h obs p i a2 + ∧ h obs p i a2 C N 2α,
−
p p
≤ λ λ ≤
(cid:12)p=N+1 p (cid:12) p=N+1 p
(cid:12) X (cid:12) X
(cid:12) (cid:12)
(cid:12) (cid:12)
ˆ
where the first inequality is due to λ λ /2 for all p M .
p,n p n
≥ ≤
Putting pieces together, we can conclude that
M n n f , φˆ 2 f , φ 2
f 2 f 2 = ∧ h obs p,n i a2 ∞ h obs p i a2
k obs kmˆ od − k obs kmod λˆ p − λ p
(cid:12) (cid:12) (cid:12) p=1 p,n p=1 p (cid:12)
(cid:12) X X (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12) N hf obs, φˆ i2 hf obs, φ i2 (cid:12) (cid:12) hf obs, φ i2 M n ∧n hf obs, φˆ i2
p,n p a2 + ∞ p a2 + p,n a2
≤ λˆ − λ p λ p λˆ p
(cid:12) p=1(cid:18) p,n p (cid:19) (cid:12) p=N+1 p p=N+1 p,n
(cid:12) X (cid:12) X X
(cid:12) log n (cid:12)
< C(cid:12) N 2α + δ + N2β+1 . (cid:12)
− n
√n
(cid:18) (cid:19)
56

(2α+2β+1)
By choosing N = logn − , we have f 2 f 2 n κ for κ = 2α/(2α+
√n k obs kmˆ od−k obs kmod ≤ −
(cid:18) (cid:19)
(cid:12) (cid:12)
2β + 1) > 0 under event . (cid:12) (cid:12)
A (cid:12) (cid:12)
I Proof of Theorem 2 and Theorem 3
Proof of Theorem 2: According to the proof of theorem 1, there exists some event
A
whose probability tending to one as n , such that under this event
→ ∞
g g C n κ g
mˆ od mod − b
k k − k k ≤ k k
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
for all g such that g (note that g is always dominated by g according to
b mod b
k k ≤ ∞ k k k k
Assumption 2). Under this condition, we have the following inclusion relationships
g f Cn κ ( g + f ) g f
p mod obs mod − p b obs b p mˆ od obs mˆod
k k ≥ k k − k k k k ⊂ k k ≥ k k
n o n o
g f + Cn κ ( g + f ) .
p mod obs mod − p b obs b
⊂ k k ≥ k k k k k k
n o
According to Assumption 3 and a standard tail probability bound for the max of sub-
Gaussian random variables, we have P (max g Cσ√log n) 1 n 1 for some
p=1,...,n p b −
k k ≤ ≥ −
constant C > 0. Let to denote this event. Then, under event , we have
B A ∩ B
n 1 n 1 n
1
U = n 1 1 V = 1 ,
n − g f +ε g f n g f ε
k p kmod ≥k obs kmod n ≤ n k p kmˆod≥k obs kmˆod ≤ n k p kmod ≥k obs kmod − n
p=1 p=1 p=1
X X X
where ε = Cn κ√log n. By Markov inequality, we have
n −
log n C
P U 1 F(( f + ε )2) 1 ,
n − − k obs kmod n | ≤ √n ≥ − log2 n
(cid:18)(cid:12) (cid:19) (cid:19)
(cid:16)
(cid:12)
log n C
(cid:12)
P (cid:12)V 1 F(( f ε )2 ) 1 .
n − − k obs kmod − n | ≤ √n ≥ − log2 n
(cid:18)(cid:12) (cid:19) (cid:19)
(cid:16)
(cid:12)
(cid:12)
(cid:12)
Let denote the intersection of the two events inside above probabilities, and = .
C E A∩B∩C
57

P
Then ( ) 1 as n , and under this event , we have
n
E → → ∞ E
1 n
1 F(( f + ε )2) 1 1 F(( f ε )2).
obs mod n g f obs mod n
− k k ≤ n p kmˆod≥k obs kmˆod ≤ − k k −
k
p=1
X
This implies the claimed result by using the fact that F is a continuous function and ε 0
n
→
as n .
→ ∞
Proof of Theorem 3: By the Markov inequality, given the data D, the conditional
probability
1 N log N
P 1 1 F ( f ) D 1 C/(log N)2,
N kg p kmˆod≥kf obs kmˆod − − n k obs kmˆ od ≤ √N ≥ −
(cid:18)(cid:12) p=1 (cid:12) (cid:12) (cid:19)
(cid:12) X (cid:16) (cid:17) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12)
P
where the randomness in is due to the Monte Carlo sampling, and for any t > 0,
M
n
F (t) = P a2Z2 t2 D ,
n p p
≤
(cid:18)p=1 (cid:12) (cid:19)
X (cid:12)
(cid:12)
(cid:12)
only dependent on M (defined in Equation (5)), is the probability that a weighted sum of
n
squares of the first M standard normal random variables Z are less than or equal
n p ∞p=1
{ }
to t. By taking expectation with respect to D on both side, we can further obtain
1 N log N
P 1 1 F ( f ) 1 C/(log N)2,
N kg p kmˆod≥kf obs kmˆod − − n k obs kmˆ od ≤ √N ≥ −
(cid:18)(cid:12) p=1 (cid:12) (cid:19)
(cid:12) X (cid:16) (cid:17) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
P
where now the randomness in is due to both the randomness in data D and the random-
ness in the Monte Carlo sampling. In addition, function F in the desired limit is
F(t) = P ∞ a2Z2 t2 .
p p
≤
(cid:18) p=1 (cid:19)
X
According to Theorem 1, we have f f C n κ with probability tending
obs mˆ od obs mod −
k k −k k ≤
(cid:12) (cid:12)
to one as n . Therefore, due(cid:12)to the continuity of F(cid:12) in t, it remains to show that for
→ ∞ (cid:12) (cid:12)
58

R
each t ,
∈
F (t) F(t) in probability as n .
n
→ → ∞
In fact, according to Assumption 2 and the fact that M as n , we have
n
→ ∞ → ∞
E ∞ a2Z2 = ∞ a2 M 2α ∞ b 0
p p p n− p
≤ →
(cid:20) p=M +1 (cid:21) p=M +1 p=M +1
Xn Xn Xn
as n . This implies the convergence in probability of M n a2Z2 to a2Z2 as
∞
p=1 p p p=1 p p
→ ∞
n . Then the desired convergence of F to F is a coPnsequence of Pthe fact that
n
→ ∞
convergences in probability imply convergences in distribution.
59

References
Agostinelli, C. & Romanazzi, M. (2013), Ordering curves by data depth, in ‘Statistical
Models for Data Analysis’, Springer, pp. 1–8.
Ash, R. B. (1990), ‘Information theory. corrected reprint of the 1965 original’.
Balzanella, A. & Elvira, R. (2015), A depth function for geostatistical functional data, in
‘Advances in Statistical Models for Data Analysis’, Springer, pp. 9–16.
Barnett, V. (1976), ‘The ordering of multivariate data’, Journal of the Royal Statistical
Society. Series A (General) pp. 318–355.
Bosq, D. (2012), Linear processes in function spaces: theory and applications, Vol. 149,
Springer Science & Business Media.
Chakraborty, A., Chaudhuri, P. et al. (2014), ‘The spatial distribution in infinite dimen-
sional spaces and related quantiles and depths’, The Annals of Statistics 42(3), 1203–
1231.
Christmann, A. (2002), Classification based on the support vector machine and on regres-
sion depth, in ‘Statistical Data Analysis Based on the L1-Norm and Related Methods’,
Springer, pp. 341–352.
Cleveland, J., Zhao, W. & Wu, W. (2018), ‘Robust template estimation for functional data
with phase variability using band depth’, Computational Statistics & Data Analysis .
Cucker, F. & Zhou, D. X. (2007), Learning theory: an approximation theory viewpoint,
Vol. 24, Cambridge University Press.
Cuesta-Albertos, J. A. & Nieto-Reyes, A. (2008), ‘The random tukey depth’, Computa-
tional Statistics & Data Analysis 52(11), 4979–4988.
Dauxois, J., Pousse, A. & Romain, Y. (1982), ‘Asymptotic theory for the principal com-
ponent analysis of a vector random function: some applications to statistical inference’,
Journal of multivariate analysis 12(1), 136–154.
60

Donoho, D. L. & Gasko, M. (1992), ‘Breakdown properties of location estimates based on
halfspace depth and projected outlyingness’, The Annals of Statistics pp. 1803–1827.
Dutta, S., Ghosh, A. K., Chaudhuri, P. et al. (2011), ‘Some intriguing properties of
tukeyâĂŹs half-space depth’, Bernoulli 17(4), 1420–1434.
Einmahl, J. H., Li, J., Liu, R. Y. et al. (2015), ‘Bridging centrality and extremity: Re-
fining empirical data depth using extreme value statistics’, The Annals of Statistics
43(6), 2738–2765.
Fraiman, R., Liu, R. Y. & Meloche, J. (1997), ‘Multivariate density estimation by probing
depth’, Lecture Notes-Monograph Series pp. 415–430.
Fraiman, R. & Muniz, G. (2001), ‘Trimmed means for functional data’, Test 10(2), 419–
440.
Gijbels, I., Nagy, S. et al. (2017), ‘On a general definition of depth for functional data’,
Statistical Science 32(4), 630–639.
Hsing, T. & Eubank, R. (2015), Theoretical foundations of functional data analysis, with
an introduction to linear operators, John Wiley & Sons.
J Mercer, B. (1909), ‘Xvi. functions of positive and negative type, and their connection
the theory of integral equations’, Phil. Trans. R. Soc. Lond. A 209(441-458), 415–446.
Karatzas, I. & Shreve, S. (2012), Brownian motion and stochastic calculus, Vol. 113,
Springer Science & Business Media.
Liu, R. Y. (1990), ‘On a notion of data depth based on random simplices’, The Annals of
Statistics pp. 405–414.
Liu, R. Y., Parelius, J. M., Singh, K. et al. (1999), ‘Multivariate analysis by data depth:
descriptive statistics, graphics and inference,(with discussion and a rejoinder by liu and
singh)’, The annals of statistics 27(3), 783–858.
61

Liu, X. & Müller, H.-G. (2004), ‘Functional convex averaging and synchronization for time-
warped random curves’, Journal of the American Statistical Association 99(467), 687–
699.
Long, J. P. & Huang, J. Z. (2015), ‘A study of functional depths’, arXiv preprint
arXiv:1506.01332 .
López-Pintado, S. & Romo, J. (2009), ‘On the concept of depth for functional data’, Journal
of the American Statistical Association 104(486), 718–734.
López-Pintado, S. & Romo, J. (2011), ‘A half-region depth for functional data’, Computa-
tional Statistics & Data Analysis 55(4), 1679–1695.
Murzin, A. G., Brenner, S. E., Hubbard, T. & Chothia, C. (1995), ‘Scop: a structural
classification of proteins database for the investigation of sequences and structures’,
Journal of molecular biology 247(4), 536–540.
Narisetty, N. N. & Nair, V. N. (2016), ‘Extremal depth for functional data and applica-
tions’, Journal of the American Statistical Association 111(516), 1705–1714.
Nicol, F. (2013), Functional principal component analysis of aircraft trajectories, PhD
thesis, ENAC.
Nieto-Reyes, A. (2011), On the properties of functional depth, in ‘Recent advances in
functional data analysis and related topics’, Springer, pp. 239–244.
Nieto-Reyes, A., Battey, H. et al. (2016), ‘A topologically valid definition of depth for
functional data’, Statistical Science 31(1), 61–79.
Ramsay, J. (2005), ‘Functional data analysis’, Encyclopedia of Statistics in Behavioral
Science .
Ramsay, J. O. & Li, X. (1998), ‘Curve registration’, Journal of the Royal Statistical Society:
Series B (Statistical Methodology) 60(2), 351–363.
62

Riesz, F. & Nagy, B. S. (1990), ‘Functional analysis, frederick ungar, new york, 1955’,
English Translation .
Rousseeuw, P. J. & Ruts, I. (1996), ‘Algorithm as 307: Bivariate location depth’, Journal
of the Royal Statistical Society. Series C (Applied Statistics) 45(4), 516–526.
Rousseeuw, P. J. & Struyf, A. (1998), ‘Computing location depth and regression depth in
higher dimensions’, Statistics and Computing 8(3), 193–203.
Sguera, C., Galeano, P. & Lillo, R. (2014), ‘Spatial depth-based classification for functional
data’, Test 23(4), 725–750.
Srivastava, A., Wu, W., Kurtek, S., Klassen, E. & Marron, J. (2011), ‘Registration of
functional data using fisher-rao metric’, arXiv preprint arXiv:1103.3817 .
Tang, R. & Müller, H.-G. (2008), ‘Pairwise curve synchronization for functional data’,
Biometrika 95(4), 875–889.
Tran, N. M. (2008), ‘An introduction to theoretical properties of functional principal com-
ponent analysis’, Department of Mathematics and Statistics, The University of Mel-
bourne, Victoria, Australia .
Tukey, J. W. (1975), Mathematics and the picturing of data, in ‘Proceedings of the Inter-
national Congress of Mathematicians, Vancouver, 1975’, Vol. 2, pp. 523–531.
Vaart, A. v. d. & Zanten, H. v. (2011), ‘Information rates of nonparametric gaussian process
methods’, Journal of Machine Learning Research 12(Jun), 2095–2119.
Vardi, Y. & Zhang, C.-H. (2000), ‘The multivariate l1-median and associated data depth’,
Proceedings of the National Academy of Sciences 97(4), 1423–1426.
Wahba, G. (1990), Spline models for observational data, SIAM.
Wang, G. & Dunbrack Jr, R. L. (2003), ‘Pisces: a protein sequence culling server’, Bioin-
formatics 19(12), 1589–1591.
63

Whitaker, R. T., Mirzargar, M. & Kirby, R. M. (2013), ‘Contour boxplots: A method for
characterizing uncertainty in feature sets from simulation ensembles’, IEEE Transactions
on Visualization and Computer Graphics 19(12), 2713–2722.
Wu, W., Srivastava, A., Laborde, J. & Zhang, J. (2013), An efficient multiple protein
structure comparison method and its application to structure clustering and outlier
detection, in ‘2013 IEEE International Conference on Bioinformatics and Biomedicine’,
IEEE, pp. 69–73.
Zuo, Y. (2018), ‘A new approach for the computation of halfspace depth in high dimen-
sions’, Communications in Statistics-Simulation and Computation pp. 1–22.
Zuo, Y. & Serfling, R. (2000), ‘General notions of statistical depth function’, Annals of
statistics pp. 461–482.
Zuo, Y. et al. (2003), ‘Projection-based depth functions and associated medians’, The
Annals of Statistics 31(5), 1460–1490.
64