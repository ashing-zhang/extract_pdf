SCALING AND BIAS CODES FOR MODELING SPEAKER-ADAPTIVE
DNN-BASED SPEECH SYNTHESIS SYSTEMS
Hieu-Thi Luong1, Junichi Yamagishi1,2
1National Institute of Informatics, Tokyo, Japan 2The University of Edinburgh, Edinburgh, UK
ABSTRACT synthesis models and adapting them to unseen speakers are
8102 tcO 1  ]SA.ssee[  2v23611.7081:viXra
still challenging problems, and resulting models are far from
Most neural-network based speaker-adaptive acoustic mod-
perfect, especially when less than ideal datasets are used [6].
els for speech synthesis can be categorized into either layer-
Most adaptation methods for neural network models can
based or input-code approaches. Although both approaches
be described as either (a) fine-tuning a set of or all of pa-
have their own pros and cons, most existing works on speaker
rameters of speaker-independent network so it explains un-
adaptation focus on improving one or the other. In this pa-
seen speaker’s data better or (b) factorizing a neural network
per, after we first systematically overview the common prin-
into speaker-specific and common parts and estimating the
ciples of neural-network based speaker-adaptive models, we
speaker-specific components for the unseen speaker’s data.
show that these approaches can be represented in a unified
The speaker-specific components may be composed by in-
framework and can be generalized further. More specifically,
put codes (e.g. one-hot vector) [7], embedding vectors ob-
we introduce the use of scaling and bias codes as general-
tained externally (e.g. i-vector) [8], or latent variables (e.g.
ized means for speaker-adaptive transformation. By utilizing
variational auto-encoder) [3, 9, 10]. Of course any of those
these codes, we can create a more efficient factorized speaker-
speaker-specific components may be jointly optimized with
adaptive model and capture advantages of both approaches
the common parts (e.g. [7, 10, 11]). Although there are a lot
while reducing their disadvantages. The experiments show
of variants on multi-speaker modeling and adaptation, most
that the proposed method can improve the performance of
approaches for augmenting the speaker-specific components
speaker adaptation compared with speaker adaptation based
into a neural network are equivalent to adapting a bias term
on the conventional input code.
of each hidden layer and this bias term is typically constant
Index Terms— speech synthesis, speaker adaptation,
across all frames of all utterances. Although Wu et al. [12]
neural network, factorization, speaker code
and Nachmachi et al. [13] proposed frame-dependent com-
ponents, these components are still bias adaptation and their
1. INTRODUCTION underlying frameworks and concepts have mathematical sim-
ilarities.
Recent speaker-dependent speech synthesis systems can gen- In this paper we first systematically overview the com-
erate high-quality reading speech indistinguishable from nat- monconceptsofneural-networkbasedspeaker-adaptivemod-
ural human speech when their training data is recorded in els and show that these approaches can be represented in a
a quality-controlled condition and have sufficient amount unified framework. Further, we introduce a scaling code as
of data [1]. The speech synthesis community is currently an extended speaker-adaptive transformation. As its name in-
trying to solve more challenging problems. A good ex- dicates, this code introduces an additional scaling operation
ample is multi-speaker speech synthesis and its adaptation as an approximation to adaptation of weight matrices unlike
[2, 3, 4, 5]. Here multi-speaker synthesis means generat- the conventional deep neural network (DNN) adaptation ap-
ing synthetic speech of multiple known speakers included proaches. Section 2 details relevant work. Section 3 describes
in a training dataset using a common model, and adaptation our factorized speaker adaptation based on scaling and bias
means adapting the speaker-independent common model to codes. Section 4 explains our experiments and shows both
unseen speakers and generating their speech. This speaker- objective and subjective results. We conclude our work and
adaptive speech synthesis systems are expected to opens describe the future direction for this method in Section 5.
possibilities for a wide range of new applications for speech
synthesis such as a customizable, user-specific voice interface
2. RELATED WORK
and voice preservation for people with medical conditions in-
volving voice losses. However, training the multi-speaker
Constrained Maximum Likelihood Linear Regression (CM-
LLR) [14, 15], also known as feature-space MLLR (fMLLR),
This work was partially supported by MEXT KAKENHI Grants
(16H06302,17H04687,18H04120,and18H04112). is a widely used speaker adaptation technique for hidden

Markov model (HMM)-based speech processing systems in of vectors. In LHUC, since we apply the transformation after
which a speaker-dependent affine transformation is applied the activation function of the current layer, we may write the
to source acoustic features to explain target data better. In the LHUC operation at the next hidden layer as follows:
case of automatic speech recognition (ASR), the transforma-
h = f(W h + c ) (5)
tion acts as a method of normalization, whereas in the case l+1 l+1 l l+1
(cid:16) (cid:16) (cid:17) (cid:17)
of speech synthesis, the transformation purpose is to diverge (k)
= f W · DiagA ◦ h + c (6)
l+1 l l l+1
the acoustic output to each target speaker [16]. The fMLLR
(cid:16) (cid:17)
A(k)
method can be described using the following equation: = f W h + c (7)
l+1 l l l+1
x = A(k)x + b(k) (1) (cid:16) (k) (cid:17)
= f W h + c (8)
l+1 l l+1
where x is the source acoustic features, x represents approxi-
From these equations, we see that a speaker-specific weight
mated acoustic features of the target speaker k, A(k) is a full
(k) (k)
linear matrix and b(k) is the bias vector. A(k) and b(k) are matrix W is factorized as W l+1A .
l+1 l
For the speaker-dependent input-code approach, a vector
transformation parameters specific to each speaker.
representing the speaker identity is fed into one or many lay-
A feedforward layer of a standard neural network can be
ers of a neural network. This vector can be as simple as an
defined by the following equation:
one-hot vector [7, 19] or an embedding vector obtained from
h = f(W h + c ) (2)
l l l−1 l outside systems like speaker verification [6, 23] or speaker
recognition [24]. Although there are many variations, each
where h is the output of the l-th hidden layer. To simplify
l
may be viewed as a bias adaptation of a hidden layer and the
our equation, let us assume all hidden layers have the same
number of hidden units m, that is, h ,h ∈ Rm×1 and the speaker-dependent input approach can be written as:
l l−1
l-th hidden layer has a weight matrix W ∈ Rm×m and a bias
l h = f(W h + c + Wbs(k)) (9)
vector c ∈ Rm×1. f(.) is an element-wise non-linear activa- l l l−1 l l
l
(k)
tion function (such as sigmoid or tanh) that deterministically = f(W h + c ) (10)
l l−1 l
Rm×1
squashes each dimension of an input vector to a limited
where s(k) ∈ Rq×1 is the auxiliary input vector specific to
range.
speaker k and has an arbitrary size q; Wb ∈ Rm×q is a new
Next we explain the existing DNN-based speaker adapta- l
weight matrix added to the layer to handle the new input. The
tion methods, that is, speaker-dependent layers and speaker-
input code approach provides the flexibility of using an out-
dependent input code using similar notations to the above fM-
side system to constrain the model. It is also convenient to
LLR. For the speaker-dependent layers [17, 18] approach, the
present each speaker (or speaking style) as one single vector
weight matrices and bias vectors of specific layers are fine-
since it may be used for controlling characteristics of syn-
tuned using adaptation data, therefore we can rewrite Equa-
thetic speech [7, 3, 25]. As the number of speaker-dependent
tion 2 as:
(k) (k) parameters q is typically small, this method shows preferable
h = f(W h + c ) (3)
l l l−1 l
results when the amount of adaptation data is limited. How-
(k) (k)
where W and c are now specific to a target speaker k ever, it does not seem to improve the adaptation performance
l l
and h also represents an adapted hidden layer . The method when the adaptation data is plentiful [19].
l
(k)
has the advantage of modeling both a full matrix W and
l
(k)
the bias vector c , which usually yield favorable result when 3. FACTORIZED SPEAKER TRANSFORMATION
l
the adaptation data is sufficient [8, 18]. However when the BASED ON SCALING AND BIAS CODES
amount of adaptation data is limited, the result is unstable as
number of parameters estimated is very large [19]. This is 3.1. Scaling and bias codes
also the reason that this method typically involves reducing
The above approaches are obviously complementary. Our
the number of parameters estimated [20, 21, 18] in order to
proposal, illustrated in Figure 1, is therefore the design of
retain the adaptation performance.
a new speaker transformation by combining the above two
Learning Hidden Unit Contribution (LHUC) [22] is an
types of approaches and further factorizing its essential com-
adaptation method that transforms outputs of the activation
ponents on the basic of “scaling” and “bias” codes. The main
function using a speaker-dependent diagonal transformation
idea is to explicitly transform both the weight matrix and the
matrix, whichsignificantlyreducesthenumberofparameters:
bias vector as:
(k)
h l = DiagA l ◦ f(W lh l−1 + c l) (4) h = f(A(k) W h + c + b(k) ) (11)
l l l l−1 l l
whereA(k) ∈ Rm×m isadiagonalmatrixforspeakerk, Diag A(k) = diag(WAsA,(k)) (12)
l l l
is an operation to extract diagonal elements of a m × m ma-
b(k) = Wbsb,(k) (13)
trixasam × 1vector, and◦isanelement-wisemultiplication l l

f(.) f(.)
f(.)
W WA U
f(.) sA,(k)
W WA Wb
sA,(k) sb,(k) W Wb V WA Wb
sb,(k) sA,(k) sb,(k)
(a) Multilevel (b) Bottleneck
Fig.2. Extendedstrategiesutilizingthescalingandbiascodes
speaker-embeded table
to integrate speaker transformations into neural network
Fig. 1. Proposed factorized speaker transformation based on
scaling and bias codes. Gray boxes indicate layers with non-
linear activation function, and the white box indicates a layer
with linear function.
where A(k) ∈ Rm×m is a diagonal matrix for the scaling
l
operation at the l-th layer. The matrix is further factorized
into a speaker-independent projection matrix WA ∈ Rm×p
l
and a scaling code vector sA,(k) ∈ Rp×1. diag is an operation
to change a m × 1 vector into a diagonal m × m matrix. The
(k)
speaker-specific bias term b is also factorized in the same (a) Nonlinear case (b) Linear case
l
way using Wb ∈ Rm×q and sb,(k) ∈ Rq×1. As described Fig. 3. Different injection points of proposed factorized
l
previously, sb,(k) is basically equivalent to the conventional speaker transformation. It may be applied to intermediate
speaker code, but we call it as bias code here to better outline hidden layers with non-linear activation functions or used at
its property. These codes may have arbitrary lengths, but, p a specific layer where all remaining operations are linear. Re-
and q are usually chosen to be much smaller than m to reduce lationships between speaker transforms and acoustic features
the number of free parameters further. are non-linear for the former case but linear for the latter case.
Factorizingmodelsexplicitlyandusinglower-dimensional
the work of Xue et al. [30], a weight matrix is decomposed
subspaces is a powerful concept used in various models (e.g.
into three linearly connected matrices using singular value de-
Heteroscedastic Linear Discriminant Analysis (HLDA) [26],
composition (SVD). Therefore, instead of multiplying a scal-
subspace Gaussian mixture model [27]). The proposed fac-
ing matrix to a weight matrix, we may first decompose the
torization is somewhat similar to Factorize Hidden Layer
weight matrix into the three linearly connected matrices and
(FHL) introduced by Samrakoon and Sim [20], but we focus
use the proposed scaling matrix to approximate one of the de-
on performing the scaling and bias adaptation simultaneously
composed matrices further as follows:
using lower dimensional vectors. A concept similar to scaling
and bias codes was also investigated for ASR in [28, 29], but
(k) (k)
h = f(W h + c + b + h ) (14)
instead of mapping the scaling and bias transformation from l l l−1 l l l−1
W(k) A(k)
a common vector we use separated vectors as scaling and bias = U V (15)
l l l l
codes to give ourselves more degrees of freedom to design a
A(k) = diag(WAsA,(k)) (16)
speaker-adaptive architecture. If necessary, we may directly l l
adapt A(k) and b(k) when the amount of adaptation data is b(k) = Wbsb,(k) (17)
l l l l
sufficient.
where U ∈ Rm×n, V ∈ Rn×m and A(k) ∈ Rn×n with
l l l
n (cid:28) m1. Note that residual connections are also added here.
3.2. Extensions of the proposed method
When we use this model for time-series speech data, the in-
In this paper, we investigate two more strategies as extensions put varies at each time and the residual part becomes a time-
(k) (k)
of the proposed method. The first strategy is to separately use variantbiastermash = f(W h +c +b +h )
l,t l l−1,t l l l−1,t
the scaling and bias codes at different layers and to explicitly where h is output of the l-th hidden unit at time t. The bot-
l,t
perform either scaling or bias operations only as illustrated by tleneck method can be summarized as Figure 2-b.
Figure 2-a. This is a special case of the proposed method.
1ItisalsopossibletotheoreticallyincludeSVDbottleneckspeakeradap-
The second strategy is to combine the proposed method
tation with low-rank approximation [31]. To do this, a constrain W ≈
l
with other type of matrix decomposition. For example, in U V needstobeadded.
l l

| W WA   |
|:-------|
| W      |

| 0    | 1      | 2   |
|:-----|:-------|:----|
| f(.) | sA,(k) |     |

| 0   | 1      | 2      |
|:----|:-------|:-------|
|     | sA,(k) | sb,(k) |

| 0   | 1   | 2      |
|:----|:----|:-------|
|     |     | sb,(k) |

| 0   | 1      | 2      |
|:----|:-------|:-------|
|     | sA,(k) | sb,(k) |

| None   |    |    | None   |
|:-------|:---|:---|:-------|
|        |    |    |        |
|        |    |    |        |
|        |    |    |        |

| 0   | 1   | 2   |
|:----|:----|:----|
|     |     |     |

| 0   | 1   | 2   |
|:----|:----|:----|
|     |     |     |

Table 1. Divisions of English and Japanese speech corpora used in our experiments.
Train(Speech&Text) Valid(Speech&Text) Test(Text) Speakers
Set
Eachspeaker Total Eachspeaker Total Eachspeaker Total Male Female Total
en.base ∼370 26785 5 360 - - 31 41 72
en.target.10 10 80
en.target.40 40 500
5 40 15 120 4 4 8
en.target.160 160 1280
en.target.320 320 2560
jp.base ∼148 34713 3 705 - - 51 184 235
jp.target.10 10 200
jp.target.50 50 1000 3 60 10 200 10 10 20
jp.target.100 100 2000
22
Table 2. Different strategies evaluated in this paper. The pa-
MCD
rameter’s size was purposely chosen so that all models used 6.6 F0RMSE
1
the same number of parameters. 20
Size 6.4 1 2
Notation Strategy Scaling Bias Bottleneck
18 )zH(ESMR0F
bias biascode - 64 - )Bd(DCM
scale scalingcode 64 - - 6.2 8 64
32
affine bias+scaling 32 32 - 4 16 128 16
level multilevel 32 32 -
2
6.0
bottle bottleneck 64 32 512
4 14
8 16
We also investigate to which layers we should inject the 5.8 32 64
128
proposed transformation and what kinds of activation func- 12
100 101 102
tions should be used after the speaker transformation. More size
Fig. 4. Objective evaluation of changing size of scaling code
specifically, we investigate whether the proposed transforma-
in nonlinear setup.
tion should be used at intermediate hidden layers with non-
as follows. In the multi-speaker task, we used en.base and
linear activation functions as shown in Figure 3-a or at a spe-
one of en.target.{10, 40, 160, or 320} for training a multi-
cific layer where all remaining operations are linear as shown
speaker neural network common to all speakers per strategy.
in Figure 3-b. By analyzing this, we can understand whether
In the adaptation task, we used en.base for training a multi-
the relationship between the proposed speaker transformation
speaker neural network per strategy and adapted it to each
functions and generated acoustic features should be repre-
target speaker included in en.target.*. In both the tasks, the
sented in a non-linear way like the former case, or in a linear
evaluation was performed using target speakers included in
one like the latter case.2
en.target.*. This increased the number of models needed to
be constructed but reduced the mismatch between the multi-
4. EXPERIMENTS speaker and adaptation tasks so we could directly compare
them.
4.1. Experimental condition For the DNN-based acoustic model, we used a con-
ventional multi-task learning neural network similar to our
We use two speech corpora to evaluate our proposal: an En-
previous works [7, 34]. The neural network maps linguis-
glish corpus containing 80 speakers, which is a subset of
tic features (depending on languages) to several acoustic
the VCTK [32, 33], and an in-house Japanese speech corpus
features including 60-dimensional mel-cepstral coefficients,
with over 250 speakers. The English corpus was used to
25-dimensional band-limited aperiodicities, interpolated log-
objectively evaluate various aspects of our proposal while the
arithm fundamental frequencies, and their dynamic counter-
Japanese corpus is used to reproduce the results and eval-
part. A voiced/unvoiced binary flag is also included. The
uate subjectively with native Japanese listeners. We split
neural network model has five feedforward layers each with
each corpora into the base and target sets as shown in Table
1024 neurons, followed by a linear layer to map to the desired
1 and conducted two tasks (multi-speaker and adaptation)
dimensional output. All layers have the sigmoid activation
function unless stated otherwise. We experimented with
2For the combination of the linear case with the strategy in Figure 2-a,
five strategies utilizing either scaling code, bias code, or
which has operations at two different layers, we first used speaker transfor-
mationbasedonthebiascodeatahiddenlayerwiththenon-linearactivation both as shown in Table 2. Further, to investigate the im-
functions and further used speaker transformation based on the scaling code
pacts of different waveform generation methods, we used
at the next linear layer. This is technically a mix of linear and non-linear
both a speaker-independent Wavenet vocoder [35, 36] and
speaker transformations, but we included this in ”the linear setup” in our ex-
periments. the WORLD vocoder [37] for speech waveform generation .

| Set           | Train(Speech&Text)   | None   | Valid(Speech&Text)   | None   | Test(Text)   | None   | Speakers   | None   | None   |
|:--------------|:---------------------|:-------|:---------------------|:-------|:-------------|:-------|:-----------|:-------|:-------|
|               | Eachspeaker          | Total  | Eachspeaker          | Total  | Eachspeaker  | Total  | Male       | Female | Total  |
| en.base       | ∼370                 | 26785  | 5                    | 360    | -            | -      | 31         | 41     | 72     |
| en.target.10  | 10                   | 80     | 5                    | 40     | 15           | 120    | 4          | 4      | 8      |
| en.target.40  | 40                   | 500    |                      |        |              |        |            |        |        |
| en.target.160 | 160                  | 1280   |                      |        |              |        |            |        |        |
| en.target.320 | 320                  | 2560   |                      |        |              |        |            |        |        |
| jp.base       | ∼148                 | 34713  | 3                    | 705    | -            | -      | 51         | 184    | 235    |
| jp.target.10  | 10                   | 200    | 3                    | 60     | 10           | 200    | 10         | 10     | 20     |
| jp.target.50  | 50                   | 1000   |                      |        |              |        |            |        |        |
| jp.target.100 | 100                  | 2000   |                      |        |              |        |            |        |        |

|    | 1     | MC      | MC   | D      |
|    | 1 2   | F0      | F0   | RMSE   |
|    | 8     | 64      |      |        |
|    | 4     | 32      |      |        |
|    | 2     | 16      |      |        |
|    | 4     | 16      |      |        |
|    | 8     | 32 64   |      |        |
|:---|:------|:--------|:-----|:-------|
|    |       |         |      | 128    |
|    |       |         |      | 128    |

nonlinear linear nonlinear linear
Fig. 5. Objective evaluations results of different strategies in
Fig. 6. Objective evaluation results of different strategies in
the multi-speaker task using the English corpus. adaptation task using English corpus. Here biasm shows ref-
erence results in the multi-speaker task using the bias code
However, our Wavenet model is still under development and
in the nonlinear setup. All other results are for adaptation of
we experienced the collapse of generated speech problems,
unseen speakers using data included in en.target.*.
which is described in [38].
speaker neural networks were trained using en.base and one
4.2. Objective evaluation
of en.target.{10, 40, 160, or 320} and synthetic speech was
generated using text of the test set of target speakers ) using
We first evaluated the scaling code by itself in a nonlinear
the bias code in the nonlinear setup. All other results are
setup since, at the time of writing, using scaling code for
adaptation results for the unseen speaker task. The amounts
multi-speaker speech synthesis has not been investigated. We
of adaptation data vary from 10 to 320.
changed the size of scaling codes from 1 to 128 to see how
they impact the objective performance of the multi-speaker From this figure, we see that adaptation to the unseen
task in a similar way to experiments that we did on bias codes speakers is more difficult than multi-speaker modeling.
previously [7]. The multi-speaker models were trained using Moreover, while the results of multi-speaker modeling are
en.base and en.target.320 together. The objective evaluation improved significantly when we increase the amount of data,
results, including mel-cepstral distortion (MCD) in dB and the adaptation results for the unseen speakers show marginal
F root mean square error (F RMSE) in Hz, are illustrated improvements when more data is available. This suggests
0 0
in Figure 4. We can see that both the distortions decrease that the proposed adaptation transformation needs to be gen-
when we increase the size of the scaling code. eralized better. Another important pattern that we can see
Next we evaluated multiple strategies described in Table from the figure is that in terms of F RMSE, all strategies in
0
the linear setup outperform their nonlinear counterparts.
2 for the multi-speaker task in either nonlinear or linear se-
tups. Again the multi-speaker models were trained using the
4.3. Subjective evaluations
en.base and en.target.320 data together. Figure 5 shows ob-
jective evaluation results of the strategies. If we look at the
Next we reproduced several selected strategies using the
non-linear setups, we see that there are no obvious differences
Japanese dataset. We doubled the size of speaker codes
between these strategies. However, at least we can determine
shown in Table 2 and chose strategies that showed reasonable
that the proposed scaling code can be used by itself without
improvements in the objective evaluation using the English
decreasing the performance. If we look at the linear setups,
dataset. The objective evaluation results using the Japanese
we can clearly see that the using the bias code by itself is a
corpus are shown in Figure 7, from which we can see the
poor strategy for multi-speaker modeling. It resulted in much
same trend as the result using the English one3.
worse MCD even though its F RMSE is comparable to other
0
We used the Japanese systems and conducted a subjec-
systems. In [39], Wang found out that the model structures re-
tive listening test to see how participants perceived these dif-
quired for mel-cepstrum and fundamental frequency are dif-
ferences. The listening test contained two sets of questions.
ferent. Our results also support this finding.
In the first part, participants were asked to judge the natural-
Figure 6 shows objective evaluation results of the strate-
ness of the presented speech sample using a five-point scale
gies in the adaptation task using different amounts of data.
The first block indicated biasm corresponds to reference
3Speech samples using the English corpus can be found at http://
results in the multi-speaker task (i.e., systems where multi- www.hieuthi.com/papers/slt2018

| None   | None   |      | None   | None   |       | None   | None   |       | None   | None   |      | None   |      | None   |       | None   | None   |      | None   | None   |       | None   | None   |      | None   |      | None   | None   |
|:-------|:-------|:-----|:-------|:-------|:------|:-------|:-------|:------|:-------|:-------|:-----|:-------|:-----|:-------|:------|:-------|:-------|:-----|:-------|:-------|:------|:-------|:-------|:-----|:-------|:-----|:-------|:-------|
|        |        |      |        |        |       |        | no     | nline |        | ar     |      |        |      |        |       |        |        |      |        | l      | inear |        |        |      |        |      |        |        |
|        |        | Obj  |        |        | ctiv  |        |        | eva   |        |        | ati  |        | sre  |        | sults |        |        | ofd  |        |        | ffer  |        |        | tst  |        | ateg |        |        |
|        |        | i-sp |        |        | ake   |        |        | tas   |        |        | usin |        | the  |        | Eng   |        |        | ish  |        |        | orpu  |        |        | .    |        | men  |        |        |
|        |        | r, o |        |        | rW    |        |        | ven   |        |        | m    |        | eli  |        | sstil |        |        | und  |        |        | rd    |        |        | elo  |        | rob  |        |        |
|        |        | erie |        |        | ced   |        |        | he c  |        |        | lla  |        | e of |        | gen   |        |        | rate |        |        | sp    |        |        | ech  |        | non  |        |        |
|        |        | de   |        |        | crib  |        |        | din   |        |        | 38]  |        | ing  |        | code  |        |        | by   |        |        | self  |        |        | n a  |        | cod  |        |        |
|        |        | jec  |        |        | ive   |        |        | valu  |        |        | tio  |        | wr   |        | iting |        |        | usi  |        |        | g s   |        |        | lin  |        | ate  |        |        |
|        |        | ev   |        |        | uat   |        |        | d th  |        |        | sca  |        | esis |        | has   |        |        | otb  |        |        | eni   |        |        | est  |        | se   |        |        |
|        |        | nce  |        |        | at t  |        |        | e ti  |        |        | e    |        | cod  |        | es f  |        |        | om   |        |        | to    |        |        | 28 t |        | i-sp |        |        |
|        |        | eak  |        |        | rsp   |        |        | ech   |        |        | ynt  |        | erfo |        | rma   |        |        | ce   |        |        | f th  |        |        | mu   |        | ias  |        |        |
|        |        | th   |        |        | size  |        |        | of s  |        |        | alin |        | rim  |        | ents  |        |        | hat  |        |        | edi   |        |        | on   |        | ed   |        |        |
|        |        | pac  |        |        | the   |        |        | bje   |        |        | ive  |        | peak |        | erm   |        |        | odel |        |        | we    |        |        | tra  |        | valu |        |        |
|        |        | sim  |        |        | lar   |        |        | ayt   |        |        | ex   |        | oget |        | her.  |        |        | he   |        |        | bje   |        |        | ive  |        | d    |        |        |
|        |        | sly  |        |        | ]. T  |        |        | hem   |        |        | lti- |        | tral |        | dist  |        |        | tion |        |        | (M    |        |        | D)   |        | llus |        |        |
|        |        | and  |        |        | en.t  |        |        | rget  |        |        | 20   |        | (F   |        | RM    |        |        | E) i |        |        | Hz    |        |        | are  |        | dec  |        |        |
|        |        | inc  |        |        | din   |        |        | me    |        |        | cep  |        | 0    |        | both  |        |        | the  |        |        | isto  |        |        | ion  |        | in   |        |        |
|        |        | me   |        |        | n sq  |        |        | are   |        |        | rro  |        | hat  |        | esca  |        |        | ng   |        |        | ode.  |        |        | ribe |        | ine  |        |        |
|        |        | e 4  |        |        | W     |        |        | can   |        |        | see  |        | fth  |        | stra  |        |        | egie |        |        | des   |        |        | r or |        | usin |        |        |
|        |        | ein  |        |        | reas  |        |        | the   |        |        | ize  |        | iple |        | eith  |        |        | r no |        |        | line  |        |        | ned  |        | how  |        |        |
|        |        | t w  |        |        | eva   |        |        | uate  |        |        | mu   |        | k in |        | mode  |        |        | sw   |        |        | etr   |        |        | e 5  |        | ok   |        |        |
|        |        | e m  |        |        | lti-s |        |        | eak   |        |        | r ta |        | er   |        | toge  |        |        | er.  |        |        | Figu  |        |        | we l |        | ffer |        |        |
|        |        | gain |        |        | the   |        |        | ulti  |        |        | spe  |        | ata  |        | stra  |        |        | egie |        |        | . If  |        |        | us   |        | ete  |        |        |
|        |        | and  |        |        | en.t  |        |        | rget  |        |        | 20   |        | the  |        | erea  |        |        | eno  |        |        | obv   |        |        | can  |        | f w  |        |        |
|        |        | eval |        |        | atio  |        |        | res   |        |        | lts  |        | atth |        | ver,  |        |        | tlea |        |        | tw    |        |        | itse |        |      |        |        |
|        |        | ars  |        |        | tup   |        |        | we    |        |        | eet  |        | owe  |        | can   |        |        | e us |        |        | d b   |        |        |      |        |      |        |        |
|        |        | the  |        |        | est   |        |        | ateg  |        |        | s.   |        | ode  |        |       |        |        |      |        |        |       |        |        |      |        |      |        |        |
|        |        | pro  |        |        | ose   |        |        | sca   |        |        | ng   |        |      |        |       |        |        |      |        |        |       |        |        |      |        |      |        |        |

| None   | None   |    |    | None   |    |     | None   |    |    |    | None   |    |    | None   |    |    |    | None   |     |     |     | None   |    |    |    | None   |    |     | None   |    |    | None   |     | None   |
|:-------|:-------|:---|:---|:-------|:---|:----|:-------|:---|:---|:---|:-------|:---|:---|:-------|:---|:---|:---|:-------|:----|:----|:----|:-------|:---|:---|:---|:-------|:---|:----|:-------|:---|:---|:-------|:----|:-------|
|        |        |    |    |        |    |     |        |    | no | nl | in     | ea | r  |        |    |    |    |        |     |     |     |        |    |    |    | l      | in | ear |        |    |    |        |     |        |
|        |        | b  | j  |        | c  | ti  |        | e  | i  | ev |        | al | ua |        | o  | n  | h  |        | su  | lts | o   |        | H  | d  | if |        | er | e   |        | m  | s  |        | eg  |        |
|        |        | n  | t  |        | as | k   |        | s  | t  | n  |        | E  | n  |        | li | s  | s  |        | or  | pu  | s.  |        | s  | e  | r  |        | b  | ia  |        | h  | e  |        | ow  |        |
|        |        | s  | u  |        | ts | i   |        | s  | e  | he |        | m  | u  |        | i  | -  | t  |        | ak  | er  | ta  |        | u  | k  | u  |        | in | g   |        | a  | d  |        | as  |        |
|        |        | nl | i  |        | e  | ar  |        | u  | s  | tu |        | p. | A  |        | l  | o  | n  |        | rr  | es  | ul  |        | a  | e  | ar |        | f  | o   |        | .* | .  |        | ati |        |
|        |        | pe | a  |        | ke | rs  |        | e  | t  | i  |        | g  | da |        | a  | i  | r  |        | ud  | ed  | i   |        | k  | s  | n. |        | ar | g   |        | b  | a  |        | an  |        |
|        |        | e  | u  |        | ra | l   |        | t  | 4  | w  |        | r  | ks |        | w  | e  | r  |        | ra  | in  | ed  |        | l  | s  | in |        | g  | en  |        | c  | s  |        | ec  |        |
|        |        | ge | t  |        | {  | 10  |        | s  | e  | 0, |        | 1  | 60 |        | o  | t  | e  |        | 0   | } a | n   |        | s  | r  | y  |        | th | et  |        | a  | k  |        | )   |        |
|        |        | d  | u  |        | i  | ng  |        | a  | t  | xt |        | of | t  |        | e  | n  | e  |        | se  | t   | of  |        | s  | e  | g  |        | t  | sp  |        | r  | h  |        | ul  |        |
|        |        | c  | o  |        | e  | i   |        | g  | u  | h  |        | n  | o  |        | i  | n  | s  |        | se  | tu  | p   |        | i  | pt | A  |        | as | ot  |        | T  | o  |        | m   |        |
|        |        | n  | o  |        | es | ul  |        | r  | e  | fo |        | t  | h  |        | u  | m  | e  |        | n   | sp  | ea  |        | a  | ti | r  |        | o  | k.  |        | o  | h  |        | u   |        |
|        |        | ti | t  |        | n  | da  |        | c  | t  | v  |        | ry | f  |        | s  | e  | d  |        | to  | 3   | 20  |        | t  | p  | a  |        | pe | n   |        | r  | s  |        | od  |        |
|        |        | i  | s  |        | is | fi  |        | s  | a  | re |        | if | w  |        | l  | t  | m  |        | ha  | t   | ad  |        | i  | e  | -  |        | k  | a   |        | m  | s  |        | lin |        |
|        |        | r, | s  |        | wh | m   |        | d  | u  | h  |        | r  | fi |        | l  | ts | h  |        | an  | u   | m   |        | h  | e  | e  |        | e  | er  |        | o  | r  |        | of  |        |
|        |        | a  | ti |        | gn | il  |        | s  | he | nt |        | y  | es |        | e  | n  | s  |        | m   | nc  | lti |        | i  | l  | t  |        | er | a   |        | o  | s  |        | ma  |        |
|        |        | m  | e  |        | n  | ifi |        | e  | a  | lt |        | f  | w  |        | h  | n  | a  |        | ei  | en  | re  |        |    | t  | ak |        | e  | s   |        | d  | e  |        | ug  |        |
|        |        | r  | o  |        | ts | r   |        | u  | A  | n  |        | m  | or |        | e  | m  | b  |        | se  | s   | s   |        |    | t  | ab |        | n  | .   |        | t  | t  |        | be  |        |
|        |        | b  | e  |        | o  | se  |        | .  | t  | d  |        | p  | o  |        | o  | r  | s  |        | i   | fo  | av  |        |    | M  | io |        | n  | ne  |        | te |    |        | ca  |        |
|        |        | fi | g  |        | e  | r.  |        |    | ut | n  |        | th | ta |        | i  | t  | i  |        | ns  | nt  | r   |        |    | e  | er |        | E, | th  |        | e  |    |        | eg  |        |
|        |        | s  | e  |        | ur | e   |        |    | v  | h  |        | t  | e  |        | e  | r  |    |        | rta | F   | p   |        |    | z  | ar |        | co | a   |        | p  |    |        | rts |        |
|        |        | j  | r  |        | tu | p   |        |    | c  | p  |        | rf | in |        | n  | u  |    |        | of  | o   | 0   |        |    | a  | st |        | at | u   |        | d  |    |        | in  |        |
|        |        | T  | d  |        | ti | ve  |        |    | a  | a  |        | u  | o  |        | e  | t  |    |        | rn  | ec  | nli |        |    | o  | e  |        | o  | eg  |        |    |    |        | r   |        |
|        |        | m  | e  |        | p  | ro  |        |    | t  | e  |        | W  | at |        | o  |    |    |        | sel | th  | te  |        |    |    | t  |        | h  | f   |        |    |    |        | so  |        |
|        |        |    |    |        | ta | se  |        |    |    | n  |        | c  | se |        | e  |    |    |        | d   | gie | e   |        |    |    | n  |        | u  | ow  |        |    |    |        | E   |        |
|        |        |    |    |        | bl | e   |        |    |    | h  |        | o  | e  |        | c  |    |    |        | te  | al  | s   |        |    |    |    |        |    | si  |        |    |    |        |     |        |
|        |        |    |    |        | ts | i   |        |    |    |    |        |    | h  |        |    |    |    |        | ev  |     | ua  |        |    |    |    |        |    |     |        |    |    |        |     |        |
|        |        |    |    |        |    |     |        |    |    |    |        |    | b  |        |    |    |    |        |     |     |     |        |    |    |    |        |    |     |        |    |    |        |     |        |

natural
vocoded
nonlinear linear
WORLD vocoder Wavenet vocoder
nonlinear linear nonlinear linear
natural
vocoded
Fig. 7. Objective evaluation results of selected strategies in Fig. 8. Subjective evaluation results of selected strategies
adaptation task using Japanese corpus. Like the English test, in adaptation task using Japanese corpus. Top figure shows
biasm shows reference results in the multi-speaker task us- mean opinion scores on naturalness. Bottom figure shows
ing the bias code in the nonlinear setup. All other results are speaker similarity scores. Recorded speech and vocoded
adaptation results. speech using correct acoustic features were also evaluated at
the same time.
ranged from 1 (very unnatural) to 5 (very natural). In the sec-
ond part, participants were asked to compare a speech sample
neck variant (in the linear setting) have better results than the
of a system with recorded speech of the same speaker and
adaptation method using the bias code in the nonlinear setting
judge if they are the same speaker or not using a four-point
(which is our previous work) for both WORLD and Wavenet
scale ranged from 1 (different, sure) to 4 (same, sure). This
vocoders. This would be because of improved F0 adaptation,
evaluation methodology is similar to our previous study [34].
as we can see objectively in Figure 7. Regarding the quantity
In addition to synthetic speech generated from the proposed
of the adaptation data, more data seems to slightly improve
speech synthesis systems using the above selected strategies,
speaker similarity of synthetic speech in general but does not
we also evaluated recorded speech, WOLRD vocoded speech,
improve the perception of quality. In some cases, it makes the
and Wavenet vocoded speech for comparison. A large-scale
quality of synthetic speech slightly worse.
listening test was done with 289 subjects. The statistical anal-
ysis was conducted using pairwise t-tests with a 95% confi-
5. CONCLUSIONS
dence margin and Holm-Bonferroni compensation for multi-
ple comparisons. In this paper, we have explained several major existing adap-
Subjective evaluation results are presented in Figure 8. tation frameworks for DNN speech synthesis and showed
In the quality test, we can first see that participants judged one generalized speaker-adaptive transformation. Further, we
all systems using our speaker-independent Wavenet vocoder have factorized the proposed transformation on the basic of
samples to be worse than counterparts using the WORLD scaling and bias codes and investigated its variants such as
vocoder. This is inconsistent with other publication results bottleneck.
and indicates that our Wavenet is not properly trained. For From objective and subjective experiments, we showed
the future works, we could further fine-tune a part of the that the proposed method, specifically the ones using both
speaker-independent Wavenet model to stabilize the neural- the scaling and bias codes in the linear setting, can reduce
net vocoder [40, 41]. However, unlike the quality test, the acoustic errors and improve subjective speaker similarity in
subjects judged synthetic speech using the Wavenet vocoder the adaptation of unseen speakers . Moreover, our results
to be closer to the target speakers in the speaker similarity test clearly indicate that there are still large gaps between vocoded
although there are still large gaps between vocoded speech speech and synthetic speech in terms of speaker similarity and
and synthetic speech. this clearly indicates that there is room for improving multi-
We can also see that a reference multi-speaker system speaker modeling and speaker adaptation.
marked as biasm using 100 utterances has the highest simi- Our future work includes comparing our method with
larity score among the other systems, and this is consistent other adaptation methods such as LHUC and SVD bottleneck
with the objective evaluation results. Regarding the adapta- speaker adaptation with low-rank approximation. Another
tion to the unseen speakers, we could see that the proposed interesting experiment we would like to see is the use of
method using both the scaling and bias codes and its bottle- i-vector or d-vector [24] as a scaling code.

| None   | None   |      |     |     | None   |      |     |     | None   |       |      | None   |     |     |      | None   |     |    |      | None   |
|:-------|:-------|:-----|:----|:----|:-------|:-----|:----|:----|:-------|:------|:-----|:-------|:----|:----|:-----|:-------|:----|:---|:-----|:-------|
|        |        |      |     | no  | nlin   | ear  |     |     |        |       |      |        | lin | ear |      |        |     |    |      |        |
|        |        | Ob   | je  | cti |        | ev   | al  | uat |        | n re  | sult |        | f s | el  | ect  |        | st  | ra | teg  |        |
|        |        | on   | ta  | sk  |        | ing  | J   | ap  |        | ese   | corp |        | . L | ik  | et   |        | E   | ng | lis  |        |
|        |        | ho   | ws  | re  |        | ren  | ce  | r   |        | ults  | in t |        | m   | ult | i-s  |        | ak  | er | tas  |        |
|        |        | bia  | sc  | od  |        | int  | he  | n   |        | line  | ars  |        | p.  | A   | ll   |        | er  | re | sul  |        |
|        |        | on   | re  | su  |        | ry   | un  | na  |        | ral)  | to5  |        | ery | n   | atu  |        | l). | In | th   |        |
|        |        | fro  | m   | 1(  |        | nts  | w   | er  |        | ske   | dto  |        | m   | pa  | re   |        | pe  | ec | hs   |        |
|        |        | t,p  | ar  | tic |        | rec  | o   | rde |        | spe   | ech  |        | th  | e   | sa   |        | s   | pe | ake  |        |
|        |        | ste  | m   | wi  |        | the  | s   | am  |        | spea  | ker  |        | n   | ot  | usi  |        | a   | f  | our  |        |
|        |        | th   | ey  | ar  |        | 1    | (d  | iff |        | ent,  | sure |        | to  | 4 ( | sa   |        | , s | u  | re). |        |
|        |        | nge  | d   | fr  |        | olo  | g   | yi  |        | imi   | lart |        | ur  | pr  | ev   |        | us  | st | udy  |        |
|        |        | on   | m   | eth |        | the  | ti  | c s |        | ech   | gen  |        | ate | d   | fro  |        | th  | e  | pro  |        |
|        |        | ion  | t   | o s |        | yst  | e   | ms  |        | ing   | the  |        | ov  | es  | el   |        | ed  | s  | trat |        |
|        |        | syn  | th  | esi |        | rec  | or  | de  |        | pee   | ch,  |        | OL  | R   | Dv   |        | od  | e  | dsp  |        |
|        |        | eva  | lu  | at  |        | ode  | d   | sp  |        | ch f  | or c |        | pa  | ri  | son  |        | A   | la | rge  |        |
|        |        | ven  | et  | v   |        | don  | e   | wi  |        | 289   | sub  |        | cts | .T  | he   |        | ati | st | ical |        |
|        |        | gte  | st  | w   |        | d    | us  | ing |        | airw  | ise  |        | es  | ts  | wi   |        | a   | 95 | %    |        |
|        |        | s c  | on  | du  |        | Ho   | l   | m-  |        | nfe   | rron |        | om  | p   | ens  |        | on  | f  | or   |        |
|        |        | ar   | gi  | na  |        | alu  | ati | on  |        | esul  | ts a |        | pr  | ese | nt   |        | in  | nt | Fig  |        |
|        |        | par  | is  | on  |        | t, w | e   | ca  |        | firs  | t se |        | hat | p   | art  |        | pa  | et | s j  |        |
|        |        | ject | iv  | e   |        | ou   | r s | pe  |        | er-i  | nde  |        | nde | nt  | W    |        | en  | io | vo   |        |
|        |        | ual  | it  | y t |        | orse | t   | ha  |        | cou   | nter |        | rts | u   | sin  |        | the | ai | WO   |        |
|        |        | ems  | u   | si  |        | inc  | o   | nsi |        | nt    | with |        | he  | r   | pu   |        | cat | pa | n r  |        |
|        |        | to   | b   | e   |        | ou   | r   | W   |        | enet  | is   |        | t p | ro  | per  |        | tr  | th | ned  |        |
|        |        | . T  | h   | is  |        | w    | e   | co  |        | d fu  | rthe |        | fin | e-t | un   |        | a   | ty | rt   |        |
|        |        | icat | es  | t   |        | ent  | W   | av  |        | et    | mod  |        | to  | st  | abi  |        | e   | et | e n  |        |
|        |        | re   | w   | or  |        | 41]  | .   | H   |        | eve   | r, u |        | ke  | th  | e    |        | ali | il | tes  |        |
|        |        | -ind | e   | pe  |        | ynt  | he  | tic |        | peec  | h u  |        | g t | he  | W    |        | en  | e  | vo   |        |
|        |        | ode  | r   | [4  |        | tar  | ge  | ts  |        | aker  | sin  |        | es  | pe  | ak   |        | sim | ke | ari  |        |
|        |        | ju   | dg  | ed  |        | e st | il  | l l |        | e g   | aps  |        | tw  | ee  | n    |        | od  | gh | d s  |        |
|        |        | oser | t   | ot  |        | ch.  | t   | hat |        | ref   | ere  |        | m   | u   | lti- |        | ea  | c  | r s  |        |
|        |        | h t  | he  | re  |        | see  | ng  | 1   |        | utt   | eran |        | s h | as  | th   |        | hi  | h  | est  |        |
|        |        | the  | tic | s   |        | usi  | he  | o   |        | er sy | ste  |        | , a | nd  | th   |        | is  | e  | ons  |        |
|        |        | can  | bi  | als |        | g t  | al  | ua  |        | n re  | sult |        | R   | eg  | ard  |        | g t | i  | e a  |        |
|        |        | as   | a   | as  |        | ev   | e   | ake |        | , w   | e co |        | s   | ee  | th   |        | th  |    | pro  |        |
|        |        | core | bje | m   |        | sp   | e   | sc  |        | ing   | and  |        | as  | co  | de   |        | nd  |    | ts b |        |
|        |        | e o  | u   | ct  |        | h th |     |     |        |       |      |        |     |     |      |        |     |    |      |        |
|        |        | the  | n   | ns  |        |      |     |     |        |       |      |        |     |     |      |        |     |    |      |        |
|        |        | usi  |     | g b |        |      |     |     |        |       |      |        |     |     |      |        |     |    |      |        |

|    | nonlinear linear nonlinear linear   | None   | None   | None   | None   | None   | None   | None   | None   | None   | None   | None    | None   | None   | None   | None   | None   | None   |
|:---|:------------------------------------|:-------|:-------|:-------|:-------|:-------|:-------|:-------|:-------|:-------|:-------|:--------|:-------|:-------|:-------|:-------|:-------|:-------|
|    |                                     |        |        |        |        |        |        |        |        |        |        |         |        |        |        |        |        |        |
|    |                                     |        |        |        |        |        |        |        |        |        |        |         |        |        |        |        |        |        |
|    |                                     |        |        |        |        |        |        |        |        |        |        |         |        | na     | tur    |        |        |        |
|    |                                     |        |        |        |        |        |        |        |        |        |        |         |        |        |        | al     |        |        |
|    |                                     |        |        |        |        |        |        |        |        |        |        | vocoded |        |        |        |        |        |        |
|    |                                     |        |        |        |        |        |        |        |        |        |        |         |        |        |        |        |        |        |
|    |                                     | ect    |        |        | al     |        | a      | t      |        | r      |        | ts      |        | lec    |        | d      |        |        |

6. REFERENCES [11] Moquan Wan, Gilles Degottex, and Mark JF Gales, “In-
tegrated speaker-adaptive speech synthesis,” in Proc.
[1] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike ASRU, 2017, pp. 705–711.
Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng
[12] Xixin Wu, Lifa Sun, Shiyin Kang, Songxiang Liu, Zhiy-
Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A.
ong Wu, Xunying Liu, and Helen Meng, “Feature
Saurous, Yannis Agiomyrgiannakis, and Yonghui Wu,
based adaptation for speaking style synthesis,” in Proc.
“Natural TTS synthesis by conditioning WaveNet on
Mel spectrogram predictions,” in Proc. ICASSP, 2018, ICASSP, 2018, pp. 5304–5308.
pp. 4779–4783.
[13] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and
[2] Jonathan Parker, Yannis Stylianou, and Roberto Cipolla, Lior Wolf, “Fitting new speakers based on a short un-
“Adaptation of an expressive single speaker deep neural transcribed sample,” arXiv preprint arXiv:1802.06984,
network speech synthesis system,” in Proc. ICASSP, 2018.
2018, pp. 5309–5313.
[14] Vassilios V Digalakis, Dimitry Rtischev, and
[3] Yuxuan Wang, Daisy Stanton, Yu Zhang, RJ Skerry- Leonardo G Neumeyer, “Speaker adaptation us-
Ryan, Eric Battenberg, Joel Shor, Ying Xiao, Fei Ren, ing constrained estimation of Gaussian mixtures,” IEEE
Ye Jia, and Rif A Saurous, “Style tokens: Unsuper- Trans. Speech Audio Process, vol. 3, no. 5, pp. 357–366,
vised style modeling, control and transfer in end-to-end 1995.
speech synthesis,” arXiv preprint arXiv:1803.09017,
[15] Mark J.F Gales, “Maximum likelihood linear transfor-
2018.
mations for HMM-based speech recognition,” Com-
[4] Jaime Lorenzo-Trueba, Gustav Eje Henter, Shinji
puter Speech & Language, vol. 12, no. 2, pp. 75–98,
Takaki, Junichi Yamagishi, Yosuke Morino, and Yuta
1998.
Ochiai, “Investigatingdifferentrepresentationsformod-
eling and controlling multiple emotions in DNN-based [16] JunichiYamagishi, TakaoKobayashi, YujiNakano, Kat-
speech synthesis,” Speech Commun., vol. 99, pp. 135– sumi Ogata, and Juri Isogai, “Analysis of speaker adap-
143, 2018. tation algorithms for HMM-based speech synthesis and
a constrained SMAPLR adaptation algorithm,” IEEE
[5] Katsuki Inoue, Sunao Hara, Masanobu Abe, Nobukatsu
Trans. Audio, Speech, Language Process, vol. 17, no. 1,
Hojo, and Yusuke Ijima, “An investigation to transplant
pp. 66–83, 2009.
emotionalexpressionsinDNN-basedTTSsynthesis,” in
Proc. APSIPA ASC, 2017, pp. 1253–1258. [17] Yuchen Fan, Yao Qian, Frank K Soong, and Lei He,
“Multi-speaker modeling and speaker adaptation for
[6] Ye Jia, Yu Zhang, Ron J Weiss, Quan Wang, Jonathan
DNN-based TTS synthesis,” in Proc. ICASSP, 2015,
Shen, FeiRen, ZhifengChen, PatrickNguyen, Ruoming
pp. 4475–4479.
Pang, Ignacio Lopez Moreno, et al., “Transfer learning
from speaker verification to multispeaker text-to-speech
[18] Zhiying Huang, Heng Lu, Ming Lei, and Zhijie Yan,
synthesis,” arXiv preprint arXiv:1806.04558, 2018.
“Linear networks based speaker adaptation for speech
synthesis,” in Proc. ICASSP, 2018, pp. 5319–5323.
[7] Hieu-Thi Luong, Shinji Takaki, Gustav Eje Henter, and
Junichi Yamagishi, “Adapting and controlling DNN-
[19] Nobukatsu Hojo, Yusuke Ijima, and Hideyuki Mizuno,
based speech synthesis using input codes,” in Proc.
“DNN-based speech synthesis using speaker codes,” IE-
ICASSP, 2017, pp. 4905–4909.
ICE T. Inf. Syst., vol. 101, no. 2, pp. 462–472, 2018.
[8] Zhizheng Wu, Pawel Swietojanski, Christophe Veaux,
[20] Lahiru Samarakoon and Khe Chai Sim, “Factorized
Steve Renals, and Simon King, “A study of speaker
hidden layer adaptation for deep neural network based
adaptation for DNN-based speech synthesis,” in Proc.
acoustic modeling,” IEEE/ACM Trans. Audio, Speech,
Interspeech, 2015, pp. 879–883.
Language Process, vol. 24, no. 12, pp. 2241–2250,
[9] Kei Akuzawa, Yusuke Iwasawa, and Yutaka Matsuo, 2016.
“Expressive speech synthesis via modeling expres-
sions with variational autoencoder,” arXiv preprint [21] Yong Zhao, Jinyu Li, Kshitiz Kumar, and Yifan Gong,
arXiv:1804.02135, 2018. “Extended low-rank plus diagonal adaptation for deep
and recurrent neural networks,” in Proc. ICASSP, 2017,
[10] Marc Delcroix, Shinji Watanabe, Atsunori Ogawa,
pp. 5040–5044.
Shigeki Karita, and Tomohiro Nakatani, “Auxiliary fea-
ture based adaptation of end-to-end asr systems,” in [22] Pawel Swietojanski and Steve Renals, “Learning hidden
Proc. Interspeech, 2018, pp. 2444–2448. unit contributions for unsupervised speaker adaptation

of neural network acoustic models,” in Proc. SLT, 2014, [34] Hieu-Thi Luong and Junichi Yamagishi, “Multimodal
pp. 171–176. speech synthesis architecture for unsupervised speaker
adaptation,” in Proc. Interspeech, 2018, pp. 2494–2498.
[23] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping,
and Yanqi Zhou, “Neural voice cloning with a few sam- [35] Aaron van den Oord, Sander Dieleman, Heiga Zen,
ples,” arXiv preprint arXiv:1802.06006, 2018. Karen Simonyan, Oriol Vinyals, Alex Graves, Nal
Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu,
[24] Rama Doddipatla, Norbert Braunschweiler, and Ran-
“Wavenet: A generative model for raw audio,” arXiv
niery Maia, “Speaker adaptation in DNN-based speech
preprint arXiv:1609.03499, 2016.
synthesis using d-vectors,” in Proc. Interspeech, 2017,
pp. 3404–3408. [36] Tomoki Hayashi, Akira Tamamori, Kazuhiro
Kobayashi, Kazuya Takeda, and Tomoki Toda,
[25] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan
“An investigation of multi-speaker training for wavenet
Wang, Daisy Stanton, Joel Shor, Ron J Weiss, Rob
vocoder,” in Proc. ASRU, 2017, pp. 712–718.
Clark, and Rif A Saurous, “Towards end-to-end prosody
transfer for expressive speech synthesis with Tacotron,”
[37] Masanori Morise, Fumiya Yokomori, and Kenji Ozawa,
arXiv preprint arXiv:1803.09047, 2018.
“World: a vocoder-based high-quality speech synthesis
system for real-time applications,” IEICE T. Inf. Syst.,
[26] Nagendra Kumar and Andreas G Andreou, “Het-
vol. 99, no. 7, pp. 1877–1884, 2016.
eroscedastic discriminant analysis and reduced rank
HMMs for improved speech recognition,” Speech Com-
[38] Yi-Chiao Wu, Kazuhiro Kobayashi, Tomoki Hayashi,
mun., vol. 26, no. 4, pp. 283–297, 1998.
Patrick Lumban Tobing, and Tomoki Toda, “Collapsed
speech segment detection and suppression for wavenet
[27] Daniel Povey, Luka´sˇ Burget, Mohit Agarwal, Pinar
vocoder,” in Proc. Interspeech, 2018, pp. 1988–1992.
Akyazi, Feng Kai, Arnab Ghoshal, Ondˇrej Glembek,
Nagendra Goel, Martin Karafia´t, Ariya Rastrow, et al.,
[39] Xin Wang, Shinji Takaki, and Junichi Yamagishi, “In-
“The subspace Gaussian mixture modela structured
vestigating very deep highway networks for parametric
model for speech recognition,” Computer Speech &
speech synthesis,” Speech Commun., vol. 96, pp. 1–9,
Language, vol. 25, no. 2, pp. 404–439, 2011.
2018.
[28] Xiaodong Cui, Vaibhava Goel, and George Saon,
[40] Li-Juan Liu, Zhen-Hua Ling, Yuan Jiang, Ming Zhou,
“Embedding-based speaker adaptive training of deep
and Li-Rong Dai, “Wavenet vocoder with limited train-
neural networks,” arXiv preprint arXiv:1710.06937,
ing data for voice conversion,” in Proc. Interspeech,
2017.
2018, pp. 1983–1987.
[29] Lahiru Samarakoon, Brian Mak, and Khe Chai Sim,
[41] Berrak Sisman, Mingyang Zhang, and Haizhou Li, “A
“Learning factorized transforms for unsupervised adap-
voice conversion framework with tandem feature sparse
tation of LSTM-RNN acoustic models,” in Proc. Inter-
representation and speaker-adapted wavenet vocoder,”
peech, 2017, pp. 774–748.
in Proc. Interspeech, 2018, pp. 1978–1982.
[30] Jian Xue, Jinyu Li, and Yifan Gong, “Restructuring of
deepneuralnetworkacousticmodelswithsingularvalue
decomposition.,” in Interspeech, 2013, pp. 2365–2369.
[31] Jian Xue, Jinyu Li, Dong Yu, Mike Seltzer, and Yi-
fan Gong, “Singular value decomposition based low-
footprint speaker adaptation and personalization for
deepneuralnetwork,” inProc.ICASSP,2014, pp.6359–
6363.
[32] Christophe Veaux, Junichi Yamagishi, and Simon King,
“The voice bank corpus: Design, collection and data
analysis of a large regional accent speech database,” in
Proc. O-COCOSDA/CASLRE, 2013, pp. 1–4.
[33] Christophe Veaux, Junichi Yamagishi, and Kirsten
MacDonald, “CSTR VCTK corpus: English multi-
speaker corpus for CSTR voice cloning toolkit,” 2017,
http://dx.doi.org/10.7488/ds/1994.