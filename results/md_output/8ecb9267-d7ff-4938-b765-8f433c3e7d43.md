DO NOT DISTRIBUTE OR PUBLICLY POST SOLUTIONS TO THESE LABS. MAKE ALL FORKS OF THIS
REPOSITORY WITH SOLUTION CODE PRIVATE.
Distributed Systems Labs and Framework
Ellis Michael
University of Washington
DSLabs is a new framework for creating, testing, model checking, visualizing, and debugging dis‑
tributed systems lab assignments.
Thebestwaytounderstanddistributedsystemsisbyimplementingthem. Andastheoldsayinggoes,
“practice doesn’t make perfect, perfect practice makes perfect.” That is, it’s one thing to write code
which usually works; it’s another thing entirely to write code which works in all cases. The latter en‑
deavor is far more useful for understanding the complexities of the distributed programming model
and specific distributed protocols.
Testing distributed systems, however, is notoriously difficult. One thing we found in previous itera‑
tionsofthedistributedsystemsclassatUWisthatmanystudentswouldwriteimplementationswhich
passed all of our automated tests but nevertheless were incorrect, often in non‑trivial ways. Some of
thesebugswouldonlymanifestthemselvesinlaterassignments,whileotherswouldgoentirelyunno‑
ticedbyourtests. Wewereabletomanuallyinspectstudents’submissionsanduncoversomeofthese
errors, but this approach to grading does not scale and does not provide the immediate feedback of
automated tests.
The DSLabs framework and labs are engineered around the goal of helping students understand and
correctly implement distributed systems. The framework provides a suite of tools for creating au‑
tomated tests, including model checking tests which systematically explore the state‑space of stu‑
dents’ implementations. These tests are much more likely to catch many common distributed sys‑
tems bugs, especially bugs which rely on precise orderings of messages. Moreover, when a bug is
found, these search‑based tests output a trace which generates the error, making debugging dramat‑
ically simpler. Finally, DSLabs is integrated with a visual debugging tool, which allows students to
graphically explore executions of their systems and visualize invariant‑violating traces found by the
model‑checker.
Programming Model
TheDSLabs frameworkis built aroundmessage‑passingstatemachines (also knownas I/Oautomata
or distributed actors), which we call nodes. These basic units of a distributed system consist of a set
1

of message and timer handlers; these handlers define how the node updates its internal state, sends
messages,andsetstimersinresponsetoanincomingmessageortimer. Thesenodesareruninsingle‑
threadedeventloops,whichtakemessagesfromnetworkandtimersfromthenode’stimerqueueand
call the node’s handlers for those events.
This model of computation is typically the one we use when introduce distributed systems for the
first time and the one we use when we want to reason about distributed protocols and prove their
correctness. The philosophy behind this framework is that by creating for students a programming
environment which mirrors the mathematical model distributed protocols are described in, we put
them on the best footing to be able to reason about their own implementations.
Testing and Model Checking
The lab infrastructure has a suite of tools for creating automated test cases for distributed systems.
These tools make it easy to express the scenarios the system should be tested against (e.g., varying
client workloads, network conditions, failure patterns, etc.) and then run students’ implementations
onanemulatednetwork(itisalsopossibletoreplacetheemulatednetworkinterfacewithaninterface
to the actual network).
While executing certain scenarios is useful in uncovering bugs in students’ implementations, it is dif‑
ficult to test all possible scenarios that might occur. Moreover, once these tests uncover a problem, it
is a challenge to discover its root cause. Because the DSLabs framework has its node‑centric view of
distributed computation, it enables a more thorough form of testing – model checking.
Model checking a distributed system is conceptually simple. First, the initial state of the system is
configured. Then, we say that one state of the system, s₂, (consisting of the internal state of all nodes,
the state of their timer queues, and the state of the network) is the successor of another state s₁ if it
can be obtained from s₁ by delivering a single message or timer that is pending in s₁. A state might
have multiple successor states. Model checking is the systematic exploration of this state graph, the
simplest approach being breadth‑first search. The DSLabs model‑checker lets us define invariants
thatshouldbepreserved(e.g.linearizability)andthensearchthoughallpossibleorderingofeventsto
make sure those invariants are preserved in students’ implementations. When an invariant violation
is found, the model‑checker can produce a minimal trace which leads to the invariant violation.
While model checking distributed systems is useful and has been used extensively in industry and
academia to find bugs in distributed systems, exploration of the state graph is still a fundamentally
hard problem – the size of the graph is typically exponential as a function of depth. To extend the
usefulness of model checking even further, the test infrastructure lets us prune the portion of the
state graph we explore for an individual test, guiding the search towards common problems while
still exploring all possible executions in the remaining portion of the state space.
2

TheDSLabsmodelisbuilttobeusablebystudentsandbeastransparentasispractical. Studentswill
be required to make certain accommodations for the model checker’s sake, but we try to limit these
and provide tools that help validate the model checker’s assumptions and debug model checking
performance issues. Moreover, the model checker itself is not designed with state‑of‑the‑art perfor‑
mance as its only goal. Building a model checker that can test student implementations of runnable
systems built in a general‑purpose language such as Java requires striking a balance between usabil‑
ity and performance.
Visualization
This framework is integrated with a visual debugger. This tool allows students to interactively ex‑
plore executions of the distributed systems they build. By exploring executions of their distributed
system, students can very quickly test their own hypotheses about how their nodes should behave,
helping them discover bugs in their protocols and gain a deeper understanding for the way their sys‑
tems work. Additionally, the tool is used to visualize the invariant‑violating traces produced by the
model‑checker.
Assignments
We currently have four individual assignments in this framework. In these projects, students incre‑
mentally build a distributed, fault‑tolerant, sharded, transactional key/value store! ‑ Lab 0 provides
a simple ping protocol as an example. ‑ Lab 1 has students implement an exactly‑once RPC protocol
on top of an asynchronous network. They re‑use the pieces they build in lab 1 in later labs. ‑ Lab 2
introduces students to fault‑tolerance by having them implement a primary‑backup protocol. ‑ Lab 3
asks students to take the lessons learned in lab 2 and implement Paxos. ‑ Lab 4 has students build a
sharded key/value store out of multiple replica groups, each of which uses Paxos internally for repli‑
cation. They finish by implementing a two‑phase commit protocol to handle multi‑key updates.
Parts of this sequence of assignments (especially labs 2 and 4) are adapted from the MIT 6.824 Labs.
The finished product is a system whose core design is very similar to production storage systems like
Google’s Spanner.
WehaveusedtheDSLabsframeworkandassignmentsindistributedsystemsclassesattheUniversity
of Washington.
Directory Overview
• framework/src contains the interface students program against.
3

• framework/tst contains in the testing infrastructure.
• framework/tst-self contains the tests for the interface and testing infrastructure.
• labscontainsasubdirectoryforeachlab. Thelabdirectorieseachhaveasrcdirectoryinitial‑
izedwithskeletoncodewherestudentswritetheirimplementations, as wellasa tstdirectory
containing the tests for that lab.
• handout-files contains files to be directly copied into the student handout, including the
main README and run-tests.py.
• grading contains scripts created by previous TAs for the course to batch grade submissions.
• www contains the DSLabs website which is built with Jekyll.
Themasterbranchofthisrepositoryisnotsetuptobedistributedtostudentsas‑is. TheMakefile
hastargetstobuildthehandoutdirectoryandhandout.tar.gz,whichcontainasingleJARwith
the compiled framework, testing infrastructure, and all dependencies. The handout branch of this
repository is an auto‑built version of the handout.
Contributing
The main tools for development are the same as the students’ dependencies — Java 17 and Python
3. You will also need a few utilities such as wget to build with the provided Makefile; MacOS users
willneedgtarandgcpprovidedbythecoreutilsHomebrewpackage,andgsedprovidedbyits
own Homebrew package.
IntelliJfilesareprovidedandincludeacodestyleusedbythisproject. InordertoprovideIntelliJwith
all of the necessary libraries, you must run make dependencies once after cloning the repository
andwheneveryouaddtoormodifytheproject’sdependencies. YouwillalsoneedtheLombokIntelliJ
plugin.
This project uses google-java-format to format Java files. You should run make format be‑
forecommittingchanges. IfyouwantIntelliJtoapplythesameformatting,youwillneedthegoogle
-java-format IntelliJ plugin, and you will need to apply the necessary post‑install settings in In‑
telliJ.
If you add fields to any student‑visible classes (all classes in the framework package as well as
SearchState and related classes), you should take care to ensure that toString prints the cor‑
rect information and that the classes are cloned correctly. See dslabs.framework.testing.
utils.Cloningformoredetails. AlsoseeLombok’s@ToStringannotationformoreinformation
about customizing its behavior. In particular, note that transient and static fields are ignored
by default by all cloning, serialization, and toString methods.
4

Acknowledgements
The framework and labs have been improved thanks to valuable contributions from: ‑ Alex Saveau
‑ Andrew Wei ‑ Arman Mohammed ‑ Doug Woos ‑ Guangda Sun ‑ James Wilcox ‑ John Depaszthory ‑
Kaelin Laundry ‑ Logan Gnanapragasam ‑ Nick Anderson ‑ Paul Yau ‑ Sarang Joshi ‑ Thomas Ander‑
son
The lab assignments, especially labs 2 and 4, were adapted with permission from the MIT 6.824 labs
developed by Robert Morris and colleagues.
Contact
BugreportsandfeaturerequestsshouldbesubmittedusingtheGitHubissuestool. EmailEllisMichael
(emichael@cs.washington.edu) with any other questions.
If you use these labs in a course you teach, I’d love to hear from you!
5