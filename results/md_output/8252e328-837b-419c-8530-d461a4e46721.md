✩
A divide and conquer method for symbolic regression
Changtong Luoa,∗ , Chen Chena,b, Zonglin Jianga,b
a
State Key Laboratory of High Temperature Gas Dynamics, Institute of Mechanics,
7102 nuJ 72  ]EN.sc[  2v16080.5071:viXra
Chinese Academy of Sciences, Beijing 100190, China
b
School of Engineering Sciences, University of Chinese Academy of Sciences,
Beijing, 100049, China
Abstract
Symbolic regression aims to find a function that best explains the relation-
ship between independent variables and the objective value based on a given
set of sample data. Genetic programming (GP) is usually considered as an
appropriate method for the problem since it can optimize functional struc-
ture and coefficients simultaneously. However, the convergence speed of GP
might be too slow for large scale problems that involve a large number of
variables. Fortunately, in many applications, the target function is separable
or partially separable. This feature motivated us to develop a new method,
divide and conquer (D&C), for symbolic regression, in which the target func-
tion is divided into a number of sub-functions and the sub-functions are then
determined by any of a GP algorithm. The separability is probed by a new
proposed technique, Bi-Correlation test (BiCT). D&C powered GP has been
tested on some real-world applications, and the study shows that D&C can
help GP to get the target function much more rapidly.
Keywords: Mathematical modeling, Genetic programming, Symbolic
regression, Artificial intelligence, Divide and conquer
✩
This work has been supported by the National Natural Science Foundation of China
(Grant No. 11532014).
∗
Corresponding author
Email addresses: luo@imech.ac.cn (Changtong Luo), chenchen@imech.ac.cn
(Chen Chen), zljiang@imech.ac.cn (Zonglin Jiang)
Preprint submitted to Expert Systems with Applications July 23, 2018

1. Introduction
Symbolic regression (SR) is a data-driven modeling method which aims to
find a function that best explains the relationship between independent vari-
ables and the objective value based on a given set of sample data (Schmidt and Lipson,
2009). Genetic programming (GP) is usually considered as a good candi-
date for SR since it does not impose a priori assumptions and can optimize
function structure and coefficients simultaneously. However, the convergence
speed of GP might be too slow for large scale problems that involve a large
number of variables.
Many efforts have been devoted trying to improve the original GP (Koza,
2008) in several ways. Some works suggest replacing its tree-based method,
with an integer string (Grammar Evolution) (O’Neill and Ryan, 2001), or
a parse matrix (Parse-Matrix Evolution) (Luo and Zhang, 2012). These
techniques can simplify the coding and decoding process but help little on
improving the convergence speed. Some other works suggest confining its
search space to generalized linear space, for example, Fast Function eXtrac-
tion (McConaghy, 2011), and Elite Bases Regression (Chen et al., 2017).
These techniques can accelerate the convergence speed of GP, even by orders
of magnitude. However, the speed is gained at the sacrifice of losing the
generality, that is, the result might be only a linear approximation of the
target function.
Fortunately, in many applications, the target function is separable or
partially separable (see section 2 for definitions). For example, in gas dy-
namics (Anderson, 2006), the heat flux coefficient S of a flat plate could be
t
formulated as
S = 2.274 sin(θ) cos(θ)/ Re , (1)
t x
p p
and the heat flux q at the stagnation point of a sphere as
s
10−4v3
q = 1.83 ρ/R(1 h /h ). (2)
s w s
× −
p
In equation (1), the two independent variables, θ and Re , are both separable.
x
In equation (2), the first three variables v, ρ, and R are all separable, and
the last two variables, h and h , are not separable, but their combination
w s
(h , h ) is separable. The function in equation (2) is considered partially
w s
separable in this paper.
The feature of separability will be used in this paper to accelerate the
optimization process of symbolic regression. Some basic concepts on func-
tion separability are defined in Section 2. Section 3 describes the overall
2

work flow of the proposed method, divide and conquer. Section 4 presents
a special technique, bi-correlation test (BiCT), to determine the separability
of a function. Numerical results are given in Section 5, and the concluding
remarks are drawn in Section 6.
2. Basic concepts
The proposed method in this paper is based on a new concept referred to
as partial separability. It has something in common with existing separability
definitions such as reference (Berenguel et al., 2013) and (d’Avezac et al.,
2011), but is not exactly the same. To make it clear and easy to understand,
we begin with some illustrative examples. The functions as follows could all
be regarded as partially separable:
z = 0.8 + 0.6 ( u2 + cos(u) ) + sin(v + w) (v w) ; (3)
∗ ∗ −
2
z = 0.8 + 0.6 ( u + cos(u) ) sin(v + w) (v w) ; (4)
∗ − ∗ −
z = 0.8 + 0.6 ( u2 + cos(u) ) sin(v + w) (v w) ; (5)
∗ ∗ ∗ −
where the boxed frames are used to indicate sub-functions, u is separable with
respect to z, while v and w themselves are not separable, but their combi-
nation (v, w) is considered separable. A simple example of non-separable
function is f(x) = sin(x + x + x ).
1 2 3
More precisely, the separability could be defined as follows.
Rn
Definition 1. A scalar function with n continuous variables f(x) (f :
7→
R Rn)
, x is said to be partially separable if and only if it can be rewritten
∈
as
f(x) = c ϕ (I x) ϕ (I x) ϕ (I x) (6)
0 1 1 1 2 2 2 3 m m m
⊗ ⊗ ⊗ · · · ⊗
where the binary operator could be plus (+), minus ( ), times( ). I is a
i i
⊗ − ×
Rni×n.
sub-matrix of the identity matrix, and I The set I , I , , I
i 1 2 m
∈ { · · · }
m
Rn×n,
is a partition of the identity matrix I n = n. The sub-function
i
∈
i=1
Rni RP
ϕ is a scalar function such that ϕ : . Otherwise the function is
i i
7→
said to be non-separable.
3

In this definition, the binary operator, division (/), is not included in
⊗
for simplicity. However, this does not affect much of its generality, since the
sub-functions are not preset, and can be transformed as ϕ˜ ( )= 1/ϕ ( ) if only
i i
· ·
ϕ ( ) = 0.
i
· 6
A special case is that all variables are separable, which could be defined
as follows.
Rn
Definition 2. A scalar function with n continuous variables f(x) (f :
7→
R Rn)
, x is said to be completely separable if and only if it can be rewritten
∈
as equation (6) and n = 1 for all i = 1, 2, , m.
i
· · ·
3. Divide and conquer method
As above mentioned, many practical problems have the feature of sepa-
rability. To make use of this feature to accelerate the optimization process of
genetic programming, a new method, divide and conquer (D&C), is proposed.
It works as follows.
First, a separability detection process is carried out to find out whether
the concerned problem is separable (at least partial separable) or not. The
variables are identified one by one, and then their combinations. Once it (or
the variable combination) is identified as separable, a sub-function ϕ (x ) (or
i i
ϕ (I x) for variable combinations) will be assigned. In this way, the structure
i i
of target function f(x) could be divided into a set of sub-functions based on
the separability information: ϕ (I x), i = 1, 2, , m.
i i
· · ·
Then, the sub-functions ϕ (I x) (i = 1, 2, , m) are optimized and
i i
· · ·
determined one by one, using any of genetic programming algorithms, in-
cluding classical GP, Grammatical Evolution (O’Neill and Ryan, 2001) and
Parse-Matrix Evolution (Luo and Zhang, 2012). When optimizing one sub-
function, variables not involved ((I I )x) are fixed as constants. That is,
i
−
only a small number of variables (I x, which is only a subset of x , x , , x )
i 1 2 n
{ · · · }
need be considered. This means the sub-function determination should be
much easier than evolving the target function f(x) directly.
For example, in Equation (3), Equation (4), or Equation (5), the
sub-function ϕ (u) = u2 + cos(u) (or ϕ (v, w) = sin(v + w) (v w)) has
1 2
∗ −
less number of variables and complexity than the original function.
Thus, optimizing them one by one is much easier for GP.
Finally, these sub-functions are properly combined to form the target
function, which is referred to as a function recover process. This process
4

Separability Detection
Function Division
Sub−Function Determination
Function Recover
Figure 1: Work flow of divide and conquer for symbolic regression
is rather simple, and all traditional regression algorithms are qualified to
accomplish this mission.
The work flow of D&C could be described in Figure 1.
4. Bi-correlation test
4.1. Description
The main idea of the proposed divide and conquer (D&C) method is to
make use of the separability feature to simplify the search process. Therefore,
the most important and fundamental step (the separability detection process
in Fig. 1) is to determine whether the concerned problem is separable (at
least partial separable) or not. To fulfill this task, a special technique, Bi-
correlation test (BiCT), is provided in this section.
Consider independent variables x , x , , x and the dependent f as
1 2 n
· · ·
n+1 random variables, and the known data as sample points. Recall that
the linear relation and correlation coefficient of two random variables has the
following relations.
Lemma 1. The two random variables ξ and η are linearly related with cor-
relation coefficient 1 (i.e., ρ = 1) if and only if there exists two constants
ξη
a, b (b = 0), such that P η = a + bξ = 1.
6 { }
The correlation coefficient ρ could be estimated by the sample correlation
5

coefficient r, which is defined as follows.
N ¯
1 (ξ ξ) (η η¯)
i i
r = − −
n 1 σ · σ
X ξ η
− i=1
where N is the number of observations in the sample set, is the summation
¯
symbol, ξ is the ξ value for observation i, ξ is the samPple mean of ξ, η is
i i
the η value for observation i, η¯ is the sample mean of η, σ is the sample
ξ
standard deviation of ξ, and σ is the sample standard deviation of η.
η
Only continuous model functions are considered in this paper. As a result,
the conclusion of Lemma 1 could be simplified as follows.
The two random variables f and f are linearly related (f = a +
A B B
•
bf ) if and only if the sample correlation coefficient r = 1 for any given
A
sample set.
Studies shows that the functional separability defined in the above sec-
tion (See equation 6) could be observed with random sampling and linear
correlation techniques.
Without the loss of generality, a simple function with three variables
(f(x) = f(x , x , x ), x [a , b ], i = 1, 2, 3) is considered to illustrate the
1 2 3 i i i
∈
implementation of the bi-correlation test. To find out whether the first vari-
able x is separable, two correlation tests are needed.
1
The first correlation test is carried out as follows. A set of random sam-
ple points in [a , b ] are generated, then these points are extended to three
1 1
dimensional space with the rest variables (x and x ) fixed to a point A. We
2 3
get a vector of function values f(A) = f(x , A) = (fA, fA, , fA), where
1 1 2 N
· · ·
N is the number of sample points. Then these points are extended to three
dimensional space with fixed x and x to another point B, We get another
2 3
vector f(B) = f(x , B) = (fB, fB, , fB). It is obviously that the two
1 1 2 N
· · ·
vectors f(A) and f(B) will be linearly related if x is separable. However, it
1
is easy to show that this linear relation could NOT ensure its separability.
Then it comes to the second correlation test. Another set of random
sample points in [a , b ] [a , b ] are generated, then these points are extended
2 2 3 3
×
to three dimensional space with the rest variable(s) (x in this case) fixed to
1
a point C, and get a vector f(C) = f(C, x , x ). Similarly, another vector
2 3
f(D) = f(D, x , x ) is obtained. Again, the two vectors f(C) and f(D) needs
2 3
to be linearly related to ensure the separability of x .
1
6

4.2. Proposition
Without the loss of generality, suppose we have a scalar function f(x)
Rn R Rn,
with n continuous variables (f : , x Ω and Ω = [a , b ]
1 1
7→ ∈ ⊂ ×
[a , b ] [a , b ]), and need to find out whether the first m variable
2 2 n n
× · · · ×
combination (x , x , , x ) are separable. Let the matrix X be a set of N
1 2 m 1
· · ·
Rm,
random sample points from the subset [a , b ] [a , b ] [a , b ]
1 1 2 2 m m
× ×· · ·× ⊂
and
(1) (1) (1)
x x x
1 2 m
· · ·
 (2) (2) (2) 
x x x
X = 1 2 m .
· · ·
1
 
· · · · · · · · · · · ·
 
(N) (N) (N)
 x x x 
1 2 m
 · · · 
The rest variables x , x , , x are fixed to two given points A and
m+1 m+2 n
· · ·
Rn−m,
B in the subset [a , b ] [a , b ] [a , b ] i.e.,
m+1 m+1 m+2 m+2 n n
× × · · · × ⊂
x = (x (A), x (A), , x (A)), x = (x (B), x (B), , x (B)).
A m+1 m+2 n B m+1 m+2 n
· · · · · ·
1
x (A) x (A) x (A)
m+1 m+2 n
· · ·
1   x (A) x (A) x (A) 
Let the matrix X (A) = x = m+1 m+2 · · · n , and
2 A
 
· · ·  · · · · · · · · · x· · (A· 
 x (A) x (A) ) 
1  m+1 m+2 n
 · · · 
 
1
1 
X (B) = x .
2 B
 
· · ·
1 
 
Let the extended matrix X = X X (A) , and X = X X (B) .
A 1 2 B 1 2
Let f be the vector of which(cid:2) the i-th ele(cid:3)ment is the (cid:2)function valu(cid:3)e of
A
the i-th row of matrix X , i.e., f = f(X ), and f is similarly defined,
A A A B
f = f(X ).
B B
Lemma 2. The two vectors f and f are linearly related if the function
A B
f(x) is separable with respect to the first m variable combination (x , x , , x ).
1 2 m
· · ·
Proof. Since the first m variable combination (x , x , , x ) are separable,
1 2 m
· · ·
from definition 1, we have f(x) = ϕ (x , x , , x ) ϕ (x , x , , x ).
1 1 2 m 2 m+1 m+2 n
· · · ⊗ · · ·
Accordingly, the vector f = f(X ) = ϕ (X ) ϕ (XA) = ϕ (X ) k ,
A A 1 1 2 2 1 1 A
⊗ ⊗
where is a component-wise binary operation, and k = ϕ (x ) is a scalar.
A 2 A
⊗
7

Similarly, the vector f = ϕ (X ) k . As a result,
B 1 1 B
⊗
k /k f if is times
A B B
· ⊗
f =  k k + f if is plus
A A B B
− ⊗

k k + f if is minus
B A B
− ⊗
which means the two vec tors f and f are linearly related.
A B
On the other hand, if the first m variables are fixed to two given points
C and D, and the rest of n m variables are randomly sampled. A similar
−
proposition could be concluded as follows. Let
1
x (C) x (C) x (C)
1 2 m
· · ·
1   x (C) x (C) x (C) 
X (C) = x (C) x (C) x (C) = 1 2 · · · m ,
1 1 2 m
· · ·
 
· · ·(cid:2) (cid:3)  x· · (C· x· · (C· · · · x· · (C· 
 ) ) ) 
1  1 2 m
 · · · 
 
1
1 
X (D) = x (D) x (D) x (D) ,
1 1 2 m
· · ·
 
· · ·(cid:2) (cid:3)
1 
 
(1) (1) (1)
x x x
m+1 m+2 n
· · ·
 (2) (2) (2) 
x x x
X = m+1 m+2 · · · n , the N n matrix X = X (C) X ,
2 C 1 2
×
 
· · · · · · · · · · · · (cid:2) (cid:3)
 
(N) (N) (N)
 x x x 
m+1 m+2 n
 · · · 
and X = X (D) X . Let f be the vector of which the i-th element is
D 1 2 C
the functio(cid:2)n value of the(cid:3) i-th row of matrix X , i.e., f = f(X ), and f
C C C D
is similarly defined, f = f(X ).
D D
Lemma 3. The two vectors f and f are linearly related if the function
C D
f(x) is separable with respect to the first m variable combination (x , x , , x ).
1 2 m
· · ·
Proof. Since the first m variable combination (x , x , , x ) are separable,
1 2 m
· · ·
from definition 1, we have f(x) = ϕ (x , x , , x ) ϕ (x , x , , x ).
1 1 2 m 2 m+1 m+2 n
· · · ⊗ · · ·
Accordingly, the vector f = f(X ) = ϕ (XC) ϕ (X ) = k ϕ (X ),
C C 1 1 2 2 C 2 2
⊗ ⊗
where is a component-wise binary operation, and the scalar k = ϕ (x ).
C 1 C
⊗
Similarly, the vector f = k ϕ (X ). As a result,
D D 2 2
⊗
k /k f if is times
f = C D · D ⊗
C (cid:26) k k + f if is plus or minus
C D D
− ⊗
which means the two vectors f and f are linearly related.
C D
8

The above lemmas show that two function-value vectors must be linearly
related if the target function has the separability feature, while the separable
variables (or their complement variables) are fixed. These are necessary
conditions for the separability identification of target function. The sufficient
and necessary conditions are given as follows.
Theorem 1. The function f(x) is separable with respect to the first m vari-
able combination (x , x , , x ) if and only if both of the flowing statements
1 2 m
· · ·
are true.
(1) Any two function-value vectors with fixed (x , x , , x ) are linearly re-
1 2 m
· · ·
lated;
(2) Any two function-value vectors with fixed (x , x , , x ) are lin-
m+1 m+2 n
· · ·
early related.
Proof. From Lemma 2, and Lemma 3, we can conclude that the necessary
conditions of the theorem hold. The sufficient conditions can be proved by
contradiction. Suppose the separable form f (x) = ϕ (x , x , , x )
1 1 2 m
· · · ⊗
ϕ (x , x , x ) can not be derived from the above two conditions.
2 m+1 m+2 n
· · ·
Thus, there is at least one non-separable variable presented in both sub-
functions, ϕ and ϕ . Without loss of generality, we assume x to be this
1 2 m
non-separable variable. That is,
f (x) = ϕ (x , x , , x ) ϕ (x , x , x , x ). (7)
1 1 2 m 2 m m+1 m+2 n
· · · ⊗ · · ·
Similarly, the process of sampling for the first correlation test can be given
as
(1) (1) (1)
x x x
1 2 m
· · ·
(2) (2) (2)
 x x x 
m
1 2
X = · · · , (8)
. . .
1 . . .
 . . . 
 
x(N) x(N) x(N)
 
 1 2 · · · m 
(1) (A) (A) (A)
x x x x
m m+1 m+2 n
· · ·
(2) (A) (A) (A)
 x x x x 
˜(A) m m+1 m+2 m
X = · · · , (9)
. . . .
2 . . . .
 . . . . 
 
x(N) x(A) x(A) x(A)
 
 m m+1 m+2 · · · m 
9

and
(1) (B) (B) (B)
x x x x
m m+1 m+2 n
· · ·
(2) (B) (B) (B)
 x x x x 
˜(B) m m+1 m+2 m
X = · · · . (10)
. . . .
2 . . . .
 . . . . 
 
x(N) x(B) x(B) x(B)
 
 m m+1 m+2 m 
· · ·
′ ˜(A) ′ ˜(B)
Let the extended matrix X = X X , and X = X X .
A 1 2 B 1 2
h i h i
Thus,
′ ′ ˜(A)
f = f (X ) = ϕ (X ) ϕ X = ϕ (X ) α, (11)
A A 1 1 2 2 1 1
⊗ ⊗
(cid:16) (cid:17)
and
′ ′ ˜(B)
f = f (X ) = ϕ (X ) ϕ X = ϕ (X ) β (12)
B B 1 1 2 2 1 1
⊗ ⊗
(cid:16) (cid:17)
˜(A)
are defined, where α and β are function-value vectors of ϕ X and
2 2
(cid:16) (cid:17)
˜(B)
ϕ X respectively. As a result,
2 2
(cid:16) (cid:17)
′
γ f if is times
B
′ · ′ ⊗
f =  α β + f if is plus (13)
A B
 − ′ ⊗
β α + f if is minus
B
− ⊗

where γ = (α /β , α /β , , α /β ). From the lemmas, we know that, two
1 1 2 2 N N
′ ′ · · · ′
vectors f and f are linearly related if they are in the relation of f =
A B A
′
k f +k , where k and k are constant scalars, k = 0. But, from the above
1 B 2 1 2 1
· 6
discussion, the components of all the three vectors, γ, α β and β α, are
− −
(1) (2) (N)
not constant, due to the randomness of sample points x , x , , x .
m m m
· · ·
(cid:16) (cid:17)
′ ′
This contradicts the supposition that the two vectors f and f are linearly
A B
related, and so Equation (7) cannot hold.
4.3. Notes on BiCT
The proposed technique is called bi-correlation test (BiCT) since two
complementary correlation tests are simultaneously carried out to determine
whether a variable or a variable-combination is separable.
The above process is illustrated with two sub-functions, and it could be
extended to determine the separability of a function with more sub-functions.
However, if the binary operators , , , in equation (6) are mutually
1 2 m
⊗ ⊗ · · · ⊗
different with mixed times and plus or minus, the extension might be a little
10

difficult. This issue will be left for the future work. Hereafter, we assume
that the binary operators in equation (6) are the same, i.e, = times or
i
⊗
= plus or minus for all i = 1, 2, , m, for simplicity. In this case, the
i
⊗ · · ·
extension process is very easy and omitted here.
To enhance the stability and efficiency of the algorithm, the distribution
of sample points should be as uniform as possible. Therefore, controlled sam-
pling methods such as Latin hypercube sampling (Beachkofski and Grandhi,
2002) and orthogonal sampling (Steinberg and Lin, 2006) are preferred for
sample generation.
For the correlation test, any of correlation methods could be used. That
is, Pearson’s r method, Spearman’s rank order correlation, and Kendall’s τ
correlation are all effective for BiCT.
Take the function f(x) = 0.8 + 0.6 (x2 + cos(x )) sin(x + x ) (x
1 1 2 3 2
∗ ∗ ∗ −
x ), x [ 3, 3]3, as an example, the first sample set consists of 13 uni-
3
∈ −
formly distributed points in [ 3, 3], and the second sample set consists of
−
169 uniformly distributed points in [ 3, 3]2. The correlation tests could be
−
fA
illustrated as in Fig. 2. As can be seen that the function-value vectors
fB
and are linearly related (Fig. 2(b)), in which the variable x and x are
2 3
fixed when considering the first variable x (Fig. 2(a)). Similarly, to find
1
out the separability of variable combination (x , x ), the first variable x is
2 3 1
fC fD
fixed (Fig. 2(c)). The corresponding function-value vectors and are
linearly related (Fig. 2(d))
The D&C method with BiCT technique is described with functions of
explicit expressions. While in practical applications, no explicit expression is
available. In this case, some modifications need to adapt for D&C method.
In fact, for data-driven modeling problems, a surrogate model of black-box
type could be established as the underlying target function (Forrester et al.,
2008) in advance. Then the rest steps are the same as above discussions.
5. Numerical results
5.1. Analysis on Computing time
The computing time (t) of a genetic programming with the proposed
divide and conquer (D&C) method consists three parts:
t = t + t + t (14)
1 2 3
where t is for the separability detection, t for sub-function determination,
1 2
and t for function recover. Note that both the separability detection and
3
11

6  
fA 1
4 fB
0
2
−1
fD 0 Bf −2
f
−2
−3
−4 −4
−6  −5
−3 −2 −1 0 1 2 3 1 2 3 4 5 6
x A
1 f
A B
(a) (x ,x ) are fixed (b) f f
2 3
↔
150
100
50
0
−50
−100
−150
−100 −50 0 50 100
C
f
C D
(c) x are fixed (d) f f
1
↔
Figure 2: Demo of separability detection process of BiCT
12

Table 1: A mapping table for parse-matrix evolution
a -5 -4 -3 -2 -1 0 1 2 3 4 5
·1
T √ ln cos / - skip + * sin exp ( )2
· ·
a , a -5 -4 -3 -2 -1 0 1 2 d
·2 ·3
· · ·
expr λ λ f f f 1.0 x x x
2 1 2 1 1 2 d
· · ·
a -1 0 1
·4
f f skip f f
k 1 2
→
function recover processes are double-precision operations and thus cost much
less time than the sub-function determination process. That is, t t .
2
≈
It is obviously that the CPU time for determining all sub-functions (t )
2
is much less than that of determining the target function directly (t ). Next,
d
a typical genetic programming, parse matrix evolution (PME), is taken as
the optimization driver (other GP algorithms should also work) to show the
performance of D&C.
Suppose that the dimension of the target function is d, the height of
the parse-matrix h, and the mapping Table as in Table 1, then the parse-
matrix entries a 5, 4, ..., 4, 5, a 5, 4, , d, (j = 1, 2), and
·1 ·j
∈ − − ∈ − − · · ·
a 1, 0, 1. Thus the parse-matrix (a ) have as many as (11 (6 + d)
·4 ij h×4
∈ − ∗ ∗
(6 + d) 3)h possible combinations. Thus the CPU time of determining the
∗
target function directly satisfies
h
t (11 (6 + d) (6 + d) 3) . (15)
d
∼ ∗ ∗ ∗
This means that the searching time of determining a target function will
increase exponentially with model complexity. Using D&C, only the sub-
functions are needed to determinate, and each sub-function has less dimen-
sion d and less complexity h. Therefore, it will cost much less CPU time.
In fact, by D&C powered GP, the CPU time will increase only linearly with
dimensions, provided that the target function is completely separable.
Take equation (2) in Section 1 as an example. Without D&C, to search
directly, the control parameters of PME should be set as follows: d = 5,
h 9. From equation (15), the order of required CPU time t = O(2.58 1032).
d
≥ ·
Using D&C powered PME, the required CPU time will be much less.
In fact, after the separation detection, the function is divided into for sub-
functions as follows.
q = 1.83 10−4 v3 √ρ 1/√ R (1 h /h )
s w s
× · · · −
13

For the sub-function v3, 1/√R, and 1 h /h , the control parameters of
w s
−
PME should be set as d = 1, h 2. For the sub-function √ρ, d = 1, h 1.
≥ ≥
As a result, t 4 O(2.61 106) = O(107) by equation (15), which means the
2
≈ ∗ ·
D&C method could reduce the computational effort by orders of magnitude.
5.2. Program timing
Next, two illustrative examples are presented to show how much time the
D&C technique could save in practical applications.
Again, equation (1) and equation (2) are set as the target function, re-
spectively. For equation (1), the sample set consists 100 observations uni-
formly distributed in [1,10] degree and [1000, 10000] (i.e., θ=1:10; Re =
x
1000:1000:10000). The angle θ is fixed to 5 degree while detecting the sub-
function f (Re ), and the Renold number Re is fixed to 5000 while detecting
1 x x
the sub-function f (θ).
2
For equation (2), the sample set consists 30000 observations uniformly
distributed in a box in R5 (i.e., v = 500 : 100 : 1000; ρ = 0.0001 : 0.0001 :
0.001; R = 0.01 : 0.01 : 0.1; h = 10000 : 10000 : 50000; h = 100000 :
w s
100000 : 1000000). The velocity of free-stream v, density of air ρ, radius of
nose R, Wall enthalpy h , and total enthalpy h are fixed to 800 m/s2, 0.0005
w s
kg/m3, 0.05 m, 20000 J/kg, and 200000 J/kg, respectively, while detecting
the sub-functions.
In both tests, the program stops when the current model is believed good
enough: 1 R2 < 1.0 10−10, where R2 = 1 SSE can be regarded as the
− · − SST
fraction of the total sum of squares that has ‘explained by’ the model. To
suppress the affect of randomness of PME, 10 runs are carried out for each
target function, and the averaged CPU time on a PC using a single CPU
core (Intel(R) Core (TM) i7-4790 CPU @3.60GHz) is recorded to show its
performance. The test results (see Table 2, and Table 3) show that D&C
technique can save CPU time remarkably. For equation (1), PME needs
about 12 minutes and 26 seconds to get an alternative of the target function
without D&C technique, while the D&C powered PME needs only about 11.2
seconds, which is much faster than the original algorithm. Similar conclusion
could also seen from the test results of equation (2) (see Table 3). Note that
the total time of D&C powered PME includes t , t and t (See Equ. 14),
1 2 3
and t + t 0.2 for Equ. (1), 0.3 for Equ. (2).
1 3
≈
14

Table 2: Performance of PME on detecting Equ. (1) (with and without D&C)
Target Function CPU time Result Expression
For D&C powered PME
f (Re ) 3s St = 0.1978/√Re
1 x x
f (θ) 8s∗ St = 0.03215 θ 0.01319 θ3
2
∗ − ∗
Total time 11.2s
PME without D&C
∗
f(Re , θ) 12m26s St = 2.274 θ cos(0.9116 θ)/√Re
x x
∗ ∗ ∗
Total time 746s
∗
PME failed to get the exact result, but always result in an alternative
function with fitting error of zero in double precision (i.e., 1 R2 = 0.0).
−
Table 3: Performance of PME on detecting target Equ. (2)(with and without D&C)
Target Function CPU time Result Expression
For D&C powered PME
f (v) 3s q = 1.647 10−5 v3
1 s
· ∗
f (ρ) 2s q = 3.77 105 √ρ
2 s
· ∗
f (R) 4s q = 6.49 103 0.08442/R
3 s
· ∗
f (h , h ) 9s q = 9370 9370p h /h
4 w s s w s
− ∗
Total time 18.3s
PME without D&C
f(v, ρ, R, hw, hs) 85m43s q = 0.000183 v3 ρ/R (1 h /h )
s w s
∗ ∗ ∗ −
Total time 5143s p
15

6. Conclusion
The divide and conquer (D&C) method for symbolic regression has been
presented. The main idea is to make use of the separability feature of the
underling target function to simplify the search process. In D&C, the target
function is divided into a number of sub-functions based on the information
of separability detection, and the sub-functions are then determined by any
of a genetic programming (GP) algorithms.
The most important and fundamental step in D&C is to identify the
separability feather of the concerned system. To fulfill this task, a special
algorithm, bi-correlation test (BiCT), is also provided for separability detec-
tion in this paper.
The study shows that D&C can accelerate the convergence speed of GP
by orders of magnitude without losing the generality, provided that the target
function has the feature of separability, which is usually the case in practical
engineering applications.
References
References
Anderson, J., 2006. Hypersonic and High-Temperature Gas Dynamics (2nd
ed.). American Institute of Aeronautics and Astronautics, Inc., Virginia.
Beachkofski, B., Grandhi, R., APRL 2002. Improved distributed hypercube
sampling. In: 43rd AIAA/ASME/ASCE/AHS/ASC Structures, Structural
Dynamics, and Materials Conference, Structures, Structural Dynamics,
and Materials and Co-located Conferences. AIAA paper no. 2002-1274.
Denver, Colorado.
URL https://doi.org/10.2514/6.2002-1274
Berenguel, L., Casado, L. G., Garca, I., Hendrix, E. M. T., Messine, F.,
2013. On interval branch-and-bound for additively separable functions with
common variables. Journal of Global Optimization 56 (3), 1101–1121.
Chen, C., Luo, C., Jiang, Z., 2017. Elite bases regression: A real-time algo-
rithm for symbolic regression.
URL https://arxiv.org/abs/1704.07313
16

d’Avezac, M., Botts, R., Mohlenkamp, M. J., Zunger, A., 2011. Learning
to predict physical properties using sums of separable functions. SIAM
Journal on Scientific Computing 33 (6), 3381–3401.
Forrester, A., Sobester, A., Keane, A., 2008. Engineering design via surrogate
modelling: a practical guide. John Wiley & Sons, Hoboken, New Jersey.
Koza, J. R., 2008. Genetic programming: on the programming of computers
by means of natural selection. MIT Press, Cambridge, MA.
Luo, C., Zhang, S.-L., 2012. Parse-matrix evolution for symbolic regression.
Engineering Applications of Artificial Intelligence 25 (6), 1182–1193.
McConaghy, T., 2011. FFX: Fast, Scalable, Deterministic Symbolic Regres-
sion Technology. Springer New York, New York, NY, pp. 235–260.
O’Neill, M., Ryan, C., Aug. 2001. Grammatical evolution. IEEE Trans. Evol.
Comp 5 (4), 349–358.
URL http://dx.doi.org/10.1109/4235.942529
Schmidt, M., Lipson, H., 2009. Distilling free-form natural laws from exper-
imental data. Science 324 (59236), 81–85.
Steinberg, D. M., Lin, D. K. J., 2006. A construction method for orthogonal
latin hypercube designs. Biometrika 93 (2), 279–288.
17