Question Answering with Deep Neural 
Networks for Semi-Structured 
Heterogeneous Genealogical Knowledge 
Graphs 
Editor(s): Name Surname, University, Country 
Solicited review(s): Name Surname, University, Country 
Open review(s): Name Surname, University, Country 
Omri Suissaa,  Maayan Zhitomirsky-Geffeta and Avshalom Elmalecha  
aDepartment of Information Science, Bar Ilan University, omrivm@gmail.com, Israel 
Abstract. With the rising popularity of user-generated genealogical family trees, new genealogical information systems have 
been developed. State-of-the-art natural question answering algorithms use deep neural network (DNN) architecture based on 
self-attention networks. However, some of these models use sequence-based inputs and are not suitable to work with graph-
based structure, while graph-based DNN models rely on high levels of comprehensiveness of knowledge graphs that is nonex-
istent in the genealogical domain. Moreover, these supervised DNN models require training datasets that are absent in the gene-
alogical domain. This study proposes an end-to-end approach for question answering using genealogical family trees by: 1) 
representing genealogical data as knowledge graphs, 2) converting them to texts, 3) combining them with unstructured texts, 
and 4) training a transformer-based question answering model. To evaluate the need for a dedicated approach, a comparison 
between the fine-tuned model (Uncle-BERT) trained on the auto-generated genealogical dataset and state-of-the-art question-
answering models was performed. The findings indicate that there are significant differences between answering genealogical 
questions and open-domain questions. Moreover, the proposed methodology reduces complexity while increasing accuracy and 
may have practical implications for genealogical research and real-world projects, making genealogical data accessible to experts 
as well as the general public.  
Keywords: Question answering, Genealogy, Neural Networks, Knowledge graph, Natural Language Processing, Transformers, 
Cultural heritage 
1.  Introduction  structured graph format named GEDCOM (GEnealog-
ical Data COMmunication)1. Most genealogical infor-
The popularity of "personal heritage", user-gener- mation systems also provide natural search capabili-
ated genealogical family tree creation, has increased  ties (a search engine) to find relatives and related fam-
in recent years, driven by new digital services, such as  ily trees. While the user interface [4, 10, 56, 75, 103] 
online family tree sharing sites, family tree creation  and user interactions [45, 69] with genealogical infor-
software, and even self-service DNA analysis by com- mation systems are well researched, to the best of our 
panies like Ancestry and My Heritage. These genea- knowledge, there is no research on natural question-
logical information systems allow users worldwide to  answering in the genealogical domain for genealogical 
create, upload and share their family tree in a semi- information systems. 
 
1 https://www.gedcom.org/ 

As humans, we are accustomed to asking questions  graph. It contains a mix of a structured graph and un-
and  receiving  answers  from  others.  However,  the  structured texts with multiple nodes and edge types, 
standard search engines and information retrieval (IR)  where nodes may include structured data on a specific 
systems require users to find answers from a list of  person node (e.g., person's birthplace), structured data 
documents. For example, for the question “How many  on a specific family node (e.g., marriage date), rela-
children does Kate Kaufman have?", the system will  tions between nodes, and unstructured text sequences 
retrieve a list of documents containing the words "chil- (e.g., bio notes of a person). Such a mix of structured 
dren" and "Kate Kaufman". Unlike search engines and  heterogeneous  graph data  and  unstructured  text  se-
IR  systems,  natural  question  answering  algorithms  quences is not the type of input that state-of-the-art 
aim to provide precise answers to specified questions  models,  like  BERT  [22]  and  other  sequence-based 
[47]. Thus, if a user is searching a genealogical data- DNN models, are designed to work with. 
base for the family tree of Kate Kaufman2, a built-in  Therefore, the main objective of the proposed study 
question answering system will not return a list of pos- is to design and empirically validate  an end-to-end 
sible matches but will provide a short and precise an- pipeline  and  a  novel  methodology  for  question-an-
swer to various natural language questions. For in- swering DNN using graph-based genealogical family 
stance, for a question such as "Where was Kate's father  trees combined with unstructured texts.  
born?", a genealogical question answering system will  The research questions addressed in this study are: 
return  the  answer  "Hesse,  Germany".  Genealogical  1.  What is the effect of the training corpus 
centers and museums seek to create a unique and per- domain (i.e., open-domain vs. genealog-
sonal experience for visitors using chatbots [73] and 
ical data) and the consanguinity scope on 
even  holographic  projections  of  private  or  famous 
the accuracy of neural network models in 
people [78]. Hence, one practical implication of such 
the genealogical question answering task? 
a genealogical question answering system can be pos-
2.  How  to  traverse  a  genealogical  data 
ing natural questions to a museum holographic char-
graph while preserving the meaning of 
acter, or even a holographic restoration of a person 
the genealogical relationships and family 
from a family tree. Imagine walking into a genealogi-
roles? 
cal center and talking to your great-grandmother, ask-
ing her questions about your family history and herit- 3.  What is the effect of the question type on 
age. The underlying technology for such a conversa- the DNN models' accuracy in the genea-
tion (inter alia) is based on the ability to answer natural  logical question answering task? 
questions on the GEDCOM data of genealogical fam-
ily trees. The current state-of-the-art method for solv- The main contributions of the study are: 
ing  such  a  task  is  based  on  deep  neural  networks  1.  A new automated method for question 
(DNN). 
answering  dataset  generation  derived 
DNN models for open-domain natural question an-
from  family  tree  data,  based  on  the 
swering achieved high accuracy in multiple studies 
knowledge graph representation of gene-
[15, 102, 116, 118, 119, 120, 127]. Training DNN 
alogical data and its automatic conver-
models  for  question  answering  requires  a  golden 
sion into a free text; 
standard dataset constructed from questions, answers, 
2.  A new graph traversal method for genea-
and corresponding texts from which these answers can 
logical data; 
be extracted. An extensive golden standard dataset for 
the natural question answering task widely used for  3.  A  fine-tuned  question  answering  DNN 
training such models is Stanford Question Answering  model for the genealogical domain, Un-
Dataset (SQuAD) [90, 91]. However, in the field of  cle-BERT,  based  on  BERT3 [22]  that 
genealogy, there are no standard training datasets of 
outperforms state-of-the-art DNN mod-
questions and answers similar to SQuAD.  
els (trained for answering open-domain 
Generating a genealogical training dataset for ques-
questions) for various question types. 
tion answering DNN is challenging, since genealogi-
cal data constitutes a semi-structured heterogeneous 
 
2https://dbs.anumu- 3 https://huggingface.co/bert-base-uncased 
seum.org.il/skn/en/c6/e22164995/Personalities/Kauf-
man_Kate 

  online collection of family trees with over a billion6 
unique  individuals  worldwide.  Family  trees  can  be 
2. Related work  created from various sources, such as family trees up-
loaded by private users (UGC) [6], clinical reports and 
This section covers related work in the fields rele- DNA records [20, 105], biographical register [64], and 
vant to this research: genealogical family trees, neural  even books [27]. Family tree records contain valuable 
network architecture, and question answering using  information about individuals and their genealogical 
neural networks.  relationships, information that is useful for historical 
research  and  preservation  [46],  population  and 
2.1. Genealogical family trees  migration research [84], and even medical research 
[124, 126]. The user-generated content family trees 
phenomena, also called "personal heritage", combines 
Genealogical family trees have become popular in 
the study of the history of one's ancestors with local 
recent years. Both non-profit organizations and com-
and social history [6]. Figure 1 illustrates the degrees 
mercial companies allow users worldwide to upload 
and update their family tree online. For example, com- of relationships between two people in the genealogi-
mercial  enterprises  like  Ancestry  and  My  Heritage  cal domain [12]. 
collect over 100 million4 and 48 million5 family trees,   
   
respectively;  FamilySearch  is  the  largest  non-profit 
 
4 https://support.ancestry.com/s/article/Searching- 5 https://www.myheritage.co.il/about-myheritage/ 
Public-Family-Trees  6 https://www.familysearch.org/en/about 

 
Fig. 1. Relation degrees in genealogy7. 
   
 
7 Figure created by WClarke (https://commons.wikimedia.org/wiki/User:WClarke) based on original by 
User:Sg647112c - Own work. From Wikipedia: https://en.wikipedia.org/wiki/Consanguinity. CC BY-SA 4.0: 
https://creativecommons.org/licenses/by-sa/4.0/  

2.1.1. The GEDCOM genealogical data standard   
 
The de facto standard in the field of genealogical 
family trees is the GEDCOM format  [36, 56]. The 
standard developed by The Church of Jesus Christ of 
Latter-day  Saints  in  1984,  and  the  latest  released 
version  (5.5.1)  that  was  drafted  in  1999  and  fully 
released  in  2019,  still  dominates  the  market  [42]. 
Other standards have been suggested as replacements, 
but none were extensively adopted by the industry. 
GEDCOM is an open format with a simple lineage-
linked structure, in which each record relates to either 
an individual or a family, and relevant information, 
such as names, events, places, relationships, and dates, 
appears in a hierarchical structure [36]. There are sev-
eral open online GEDCOM databases, including Ge-
nealogyForum [35], WikiTree8, GedcomIndex9, Anu 
Museum10, Ancestry.com, and others. 
In GEDCOM format, every person (individual) in 
the family tree is represented as a node that may con-
tain known attributes, such as first name, last name, 
birth date and place, death date and place, burial date 
and place, notes, occupation, and other information. 
Two individuals are not linked to one another directly. 
Each  individual  is  linked  to  a  family  node  as  a 
"spouse" (i.e., a parent) or a "child" in the family. 
Figure 2 shows a sub-graph corresponding to a Source 
Person (SP) whose data is presented in the GEDCOM 
file  in  Figure  3.  Each  individual  and  family  are 
 
assigned  a  unique  ID  –  a number  bracketed  by  @ 
symbols and a class name (INDI – individual, FAM – 
Fig. 2. Family tree structure. 
family). The source person is noted as SP (@I137@ 
INDI - Emily Williams in the GEDCOM file), families  Wyoming, USA, and was buried three days later in the 
as F and other persons as P. In this example, P3, P4,  same  place.  He  was  baptized  on  9  AUG  1877, 
P5, and P6 are the grandparents of SP; P1 and P2 are  although there is a note stating that it may be on the 12 
SP's parents in family F1 (@F79@ in the GEDCOM  of AUG 1877, and he was endowed with his wife. For 
file); P7 and P8 are SP's siblings; P10 (@I162@ INDI  practical  reasons,  the  GEDCOM  file  example  in 
– John Williams in the GEDCOM file) is SP's spouse  Figure  3  contains  only  a  small  portion  of  the  data 
from family F4 (@F73@ in the GEDCOM file), P12  presented in Figure 2.  
and P13 are SP's children; and P15, P16, and P17 are 
SP's grandchildren. Moreover, as seen in Figure 3, SP 
was a female, born on 28 MAY 1816 in New York, 
USA, who died on 7 FEB 1899 in Uinta, Wyoming, 
USA, and was buried three days later in the same place. 
Furthermore, SP was baptized on 1 JUN 1832 and was 
endowed on 30 DEC 1845 in TEMP NAUVO (maybe 
Nauvoo Temple11, Illinois). Her husband, P10, John 
Williams,  was  a  male,  born  on  16  MAY  1826  in 
Indiana, USA, who died on 25 SEP 1912 in Uinta,  
 
8 https://www.wikitree.com/  11 https://churchofjesuschristtemples.org/nauvoo-
9 http://gedcomindex.com/gedcoms.html  temple/ 
10 https://dbs.anumuseum.org.il 

result)  to  the  neurons  in  the  next  layer.  The 
information is passed over and changed from layer to 
layer until it becomes the output in the network's last 
layer.  The  conventional  learning  method  is 
backpropagation,  which  refers  to  learning  as  an 
optimization problem [122]. After each training cycle, 
a comparison between the network prediction (output) 
and  the  actual  expected  result  is  performed,  and  a 
"loss"  (i.e.,  the  gap)  is  calculated  to  estimate  the 
changes needed in the network operations (the weight 
of neuron's transformation). Changes in the network 
weights  are  usually  performed  using  the  Gradient 
Descent methods [7].  
In recent years, DNNs have become the state-of-
the-art method for text analysis in the cultural heritage 
space [110], and natural language question-answering 
systems based on DNN have become the state-of-the-
art method for solving the  question answering task 
[61]. The underlying task of question answering is 
Machine  Reading  Comprehension  (MRC),  which 
allows machines to read and comprehend a specified 
context passage for answering a question, similarly to 
language proficiency exams. Question answering, on 
the other hand, aims to answer a question without a 
specific context. These QA systems store a database 
containing a sizeable unstructured corpus and generate 
the  context  in  real-time  based  on  relevant  text 
passages  to  the  input  question  [138].  Due  to  the 
magnitude of comparisons needed between the query 
and each text passage in the corpus, and due to the 
number  of  calculations  (a  large  number  of 
multiplications of vectors and matrices) when a DNN 
model predicts the answer span for every given text 
passage, DNNs are not applied on the entire database 
of texts, but only on a limited number of passages. 
Hence,  when  a  user  asks  a  question,  the  system 
searches 12  the  database  for  K  passages  that  are 
relevant to the user question. The system will then use 
the DNN model to predict the answer span (start and 
end  positions)  for  each  text  passage  (from  the  K 
passages) with a confidence level. The answer with the 
 
highest confidence level is selected as the answer to be 
Fig. 3. (part of the) GEDCOM family tree file.  presented to the user. Thus, a typical pipeline (shown 
in Figure 4) of DNN for question answering will be a 
2.2. Question answering using DNN  compound of (1) two inputs - (a) a text passage (i.e., a 
document)  that  may  contain  the  answer,  and  (b)  a 
A DNN is a computational mathematical model that  question; and (2) two outputs: (a) the start index of the 
consists of several "neurons" arranged in layers. Each  answer in the text passage, and (b) the end index of the 
neuron  performs  a  computational  operation  and  answer in the text passage. The inputs are encoded into 
transmits  the  computed  information  (calculation  vectors  using  static  embeddings  methods,  such  as 
 
12 A common approach for finding relevant pas-
sages is reverse indexing [11, 53, 54, 71, 104] 

Word2Vec  [77]  and  GloVe  [86]  or  using  information  from  the  vectorial  representation. 
contextualized  embeddings  of  words  like  Channels of different sizes enable it to deal with n-
Bidirectional  Encoder  Representations  from  gram-like information in a sentence [57]. 
Transformers  (BERT)  [22],  Embeddings  from  Question answering task can also be modeled as a 
Language  Models  (ELMo)  [87]  and  other  methods  graph task (e.g., traversal, subgraph extraction). The 
[88].  One  of  the  main  advantages  of  contextual  data  can  be  represented  as  a  knowledge  graph 
embeddings is the ability to handle disambiguations of  (KGQA), where each node is an entity, and each edge 
words  and  entities  [81,  129].  The  input  vector  is  is a relation between two entities. When answering the 
transferred through the network, and the final layer  question, the algorithm finds the relevant entities for 
output vectors are the probability of every word to be  the question and traverses over the relations or uses 
the start or the end of the span (i.e., answer). The score  the  node’s  attributes  to  find  the  answer  node  or 
of every span is a combination of the start and end  attribute [13, 24, 134]. To work with graphs, Graph 
tokens’ probabilities. The most probable span is then  Neural  Networks  (GNN)  [94]  models  have  been 
translated  back  to  a  sequence  of  words  using  the  developed that operate directly on the graph structure. 
embedding method [22] (see section 3.2 for a more  GNN can be used for resolving answers directly from 
detailed  description).  Researchers  proposed  various  a knowledge graph by predicting an answer node from 
DNN-based  models  to  solve  the  task  of  finding  question nodes (i.e., entities) [29, 38, 72, 80, 101, 134]. 
(ranking)  an  answer  span (the  part of  the  text  that  The GNN model is similar to RNN in the sense that it 
contains the answer for the question) in the document  uses near nodes and relations (instead of previous and 
[22, 97, 118, 119, 133] or a single sentence [34, 62].  next token in RNN) to classify (i.e., label) each node. 
However,  these  models  cannot  directly  work  with 
2.2.1. Natural question answering using DNN  unstructured  or  semi-structured  data  or  rely  on  the 
architecture   ability to complete and update the knowledge graph 
Over the years, different deep learning layers have  from  free  texts  using  knowledge  graph  completion 
been developed with various abilities. Until recently,  tasks, such as relation extraction [8, 82, 128] or link 
the typical architecture for natural language questions  prediction [32, 52]. 
answering was based on Recurrent Neural Networks  An improved approach considered to be the state-
(RNN) such as Long Short Term Memory (LSTM)  of-the-art  in  many  NLP  tasks,  including  question 
[48] and Gated Recurrent Units (GRU) layers [17].  answering, is Transformers architecture [115], which 
RNN  layers  allow  the  network  to  "remember"  uses the attention mechanism with feed-forward layers 
previously  calculated  data  and  thus  learn  answers  (not RNNs); this kind of attention is also called Self 
regarding an entire sequence. These layers are used to  Attention Network (SAN). Well-known examples of 
construct different models, including a sequence-to- SANs are Bidirectional Encoder Representations from 
sequence model [112] that uses an encoder-decoder  Transformers (BERT) [22] and GPT-2 [89] models. 
architecture [17] that fits the question-answering task.  Several BERT-based models were developed in recent 
This  model  maps  a  sequence  input  to  a  sequence  years  [125],  achieving  state-of-the-art  performance 
output, like a document (sequence of words) and a  (accuracy)  in  different  question  answering  tasks. 
question (sequence of words) to an answer (sequence  These  include  RoBERTa  -  a  BERT  model  with 
of words) or to classify words (whatever the word is  hyperparameters and training data size tuning  [70]; 
the start or the end of the answer). RNN architecture  DistilBERT - a smaller, faster, and lighter version of 
often works with direct and reverse order sequences  BERT [93]; ELECTRA – a BERT-like model with a 
(bidirectional-RNN)  [96].  It  may  also  include  an  different  training  approach  [18].  Although  standard 
attention  mechanism  [115],  which  "decides"  (i.e.,  BERT-based models receive textual sequence as input, 
ranks) which parts in the sequence are more important  all  the  above  architectures  can  also  be  mixed.  For 
than others during the transformation of a sequence  example,  a  Graph  Convolutional  Network  (GCN) 
from one layer to another.   [114]  can  be  utilized  for  text  classification  by 
Another  typical  architecture  is  based  on  a  modeling the text as a graph and using the filtering 
Convolutional Neural Network (CNN). Unlike RNNs,  capabilities of a CNN [131].  
CNNs architecture does not have any memory state  There  are  several  question-answering  DNN 
that accumulates the information from the sequence  pipelines  based  on  knowledge  graphs  that  support 
data.  CNN  architecture  uses  pre-trained  static  semi-structured data (a mix of a structured graph and 
embeddings  where  each  CNN  channel  aggregates  unstructured texts) [29, 40, 134, 137]. As shown in 
Figure 5, a current state-of-the-art pipeline of this type, 

Deciphering Entity Links from Free Text (DELFT)  genealogical knowledge graph into text, which is then 
[134],  uses  the  knowledge  graph  to  extract  related  combined  with  unstructured  genealogical  texts  and 
entities  and  sentences,  filters  possible  textual  processed  by  BERT’s  contextual  embeddings. 
sentences using BERT, and then traverses a filtered  Converting the genealogical graph into text passages 
subgraph using a GNN. The pipeline starts with iden- can  be  performed  using  knowledge-graph-to-text 
tifying the entities in the question. Then, related enti- templates and methodologies [21, 26, 55, 76, 123], and 
ties (“candidates”) from the knowledge graph and rel- knowledge-graph-to-text machine learning and DNN 
evant sentences (“evidence relations”) from unstruc- models [5, 33, 63, 66, 68, 78, 79, 99, 106].  Template-
tured texts are extracted and filtered using BERT. A  based  knowledge-graph-to-text  methods  use 
new subgraph is generated using the question entities,  hardcoded or extracted linguistic rules or templates to 
the  filtered  evidence  relations,  and  the  candidate  convert a subgraph into a sentence. Machine learning 
entities. Using this subgraph, a GNN model learns to  and DNN models can be trained to produce a text from 
rank the most relevant node. Thus, the model obtains  knowledge-graph nodes. The input for a knowledge-
a  “trail”  from  the  question  nodes  to  a  possible  graph-to-text model is a list of triples of two nodes and 
candidate node (i.e., answer). The pipeline applies two  their  relation,  and  the  output  is  a  text  passage 
DNN models: a BERT model to rank the evidence  containing a natural language text with input nodes 
relations and a GNN model to traverse the graph (i.e.,  and their relations as syntactic sentences. To this end, 
predict the answer node).   DNN models are often trained using commonsense 
However,  these  methods,  using  the  unstructured  knowledge graphs of facts, such as ConceptNet [107], 
texts to create or complete the knowledge graph, rely  BabelNet [83], DBpedia [3], and Freebase [85], where 
heavily on well-defined semantics and fail to handle  nodes  are  entities,  and  the  edges  represent  the 
questions  with  entities  completely  outside  the  semantic relationships between them. Some models 
knowledge graph or questions that cannot be modeled  use  the  fact  that  knowledge  graphs  are  language-
within  the  knowledge  graph.  For  example,  agnostic  to  generate  texts  in  multi-languages  (e.g., 
Differentiable Neural Computer (DNC) [38] can be  [79]).    
used to answer traversal questions ("Who is John's 
great-great-grandfather?"), but not to answer content- 2.3. Questions and answers generation for DNN-
related questions when the answer is written in the  based question answering systems 
person's bio notes (e.g., "When did John's great-great-
grandfather  move  to  Florida?").  As  part  of  the  Training  of  a  DNN  question  answering  model 
evaluation experiments in this study, the performance  requires a set of text passages and corresponding pairs 
of the above mentioned DELFT pipeline, adapted to  of questions and answers. Multiple approaches exist 
the genealogical domain, was compared to that of the  for generation of questions (and answers): knowledge-
proposed pipeline.  graph-to-question  template-based  methodology 
   In  summary,  the  generic  question  answering  (similar to the context generation) [67, 98, 136, 140], 
pipelines described above cannot be applied as-is in  WH  questions (e.g., Where, Who, What, When, Why) 
the genealogical domain, without compromising on  rule-based  approach  [80],  knowledge  graph-based 
accuracy, for the following reasons: (1) The raw data  question generation [16, 50], and DNN-based models 
is  structured  as  graphs,  each  graph  contains  more  for generating additional types of questions [25, 49, 
information than a DNN model can handle in a single  117, 135]. The rule-based method uses part-of-speech 
inference  process  (each  node  is  equivalent  to  a  parsing of sentences using the Stanford Parser [59], 
document), (2)  A user may ask about different nodes  creates a tree query language and tree manipulation 
and different scopes of relations (i.e., different genea- [65],  and  applies  a  set  of  rules  to  simplify  and 
logical relation degrees); (3) There is a high number  transform the sentences to a question. To guarantee 
of  nodes  containing  a  relatively  small  volume  of  question quality, questions are ranked by a logistic 
structured  data  and  a  relatively  large  volume  of  regression model for question acceptability [44]. The 
unstructured textual data.  In addition, the vast amount 
DNN  question  generation  models  are  trained  on 
of  different  training  approaches,  hyperparameters  SQuAD [90, 91] or on facts from a knowledge graph 
tuning, and architectures indicate the complexity of  to predict the question and its correct answer from the 
the models and sensitivity to a specific domain and  context  (i.e.,  the  opposite  task  from  question 
sub-task.   answering)  using  bi-directional  [96]  LSTM  [48] 
    The question answering approach proposed in this  encoder-decoder [17] model with attention [115]. 
study simplifies the task pipeline by converting the 

This study adopted the format of the SQuAD da- must answer questions when possible and determine 
taset, which is a well-known benchmark for machine  when  no  answer  is  supported  by  the  paragraph,  in 
learning models on question answering tasks with a  which case they must abstain from answering. 
formal leaderboard13. SQuAD is a reading comprehen- SQuAD 2.0 is a JSON formatted dataset, presented 
sion dataset consisting of questions created by crowd  in Figure 6, where each topic (a Wikipedia article) has 
workers on a set of Wikipedia articles. The answers to  a title and paragraphs. Each paragraph contains a con-
the  questions  are  segments  of  text  from  the  corre- text (text passage) and questions (qas). Each question 
sponding reading passage (context), or the question  contains the question text, id, may contain answers (if 
might  be  unanswerable.  SQuAD  2.0  combines  it is answerable), may contain plausible answers, or be 
100,000 questions and answers and over 50,000 unan- marked  as  impossible.  Each  answer  is  constructed 
swerable  questions  written  adversarially  by  crowd  from a text and a start index (the word index) of the 
workers to look similar to answerable ones. To do well  answer in the text passage.  
on SQuAD 2.0, natural  question answering models   
 
Fig. 4. Typical open-domain question answering pipeline. 
 
  
Fig. 5. Typical knowledge graph question answering pipeline. 
 
13 https://rajpurkar.github.io/SQuAD-explorer/ 

  
Fig. 6. SQuAD 2.0 JSON format example. 
should contain questions with answers and free text 
passages from which the model can retrieve these an-
3. Methodology 
swers.  
Generating  a  training  dataset  from  genealogical 
While using DNNs for the open-domain question 
data is a three-step process. The result of the process 
answering  task  has  become  the  state-of-the-art 
is Gen-SQuAD, a SQuAD 2.0 format dataset tailored 
approach, automated question answering systems for 
to the genealogical domain. As shown in Figure 7, the 
genealogical  data  is  still  an  underexplored  field  of 
process includes the following steps: (1) decomposing 
research. This paper presents a new methodology for 
the  GEDCOM  graphs  to  CIDOC-CRM-based 14 
a DNN-based question answering pipeline for semi-
knowledge sub-graphs, (2) generating text passages 
structured  heterogeneous  genealogical  knowledge 
from the obtained knowledge sub-graphs, and (3) gen-
graphs. First, a training corpus that captures both the 
erating questions and answers from the text passages. 
structured and unstructured information in genealogi-
Finally, the context and matching questions and an-
cal graphs is generated. Then, the generated corpus is 
swers are saved in the SQuAD 2.0 JSON format. The 
used to train a DNN-based question answering model.  
following sections present in detail each step of the 
Gen-SQuAD generation process. 
3.1. Gen-SQuAD generation and graph traversal 
 
   
The first phase in the proposed methodology is to 
generate a training dataset using the text sequence en-
coding with a graph traversal algorithm. This dataset 
 
14 http://www.cidoc-crm.org/ 

 
 
Fig. 7. Gen-SQuAD generation.
3.1.1. Sub-graph extraction and semantic  is different from the pure graph-theory mathematical 
definition implemented in BFS [12]. For example, par-
representation 
ents are considered first-degree relations in genealogy 
While there are some DNN models that can accept 
large inputs [9, 58], due to computational resource  (based on the ontology), while they are considered to 
limitations, many DNN models tend to accept limited  be  second-degree  relations  mathematically,  since 
there is a family node between the parent and the child 
size inputs, usually ranging from 128 to 512 tokens 
(i.e., the parent and the child are not connected di-
(i.e., words) [141]. However, family trees tend to hold 
rectly), with siblings considered to be second-degree 
a lot of information, from names, places, and dates to 
free-text notes, life stories, and even manifests. There- relations in both genealogy and graph theory. Com-
fore, using the proposed methodology, it is not practi- bined BFS-DFS algorithms such as Random Walks 
[39] do not take into account domain knowledge and 
cal to build a model that will read an entire family tree 
sample nodes randomly. In the genealogical research 
as an input (sequence), and it is necessary to split the 
field, several traversal algorithms have been suggested 
family  tree  into  sub-trees  (sub-graphs).  Several ge-
for user interface optimization [56]. However, these 
neric graph traversal algorithms may be suitable for 
traversing a graph and extracting sub-graphs, such as  algorithms aim to improve interfaces and user experi-
Breadth-First-Search  (BFS)  and  Depth-First-Search  ence and are not suitable for complete data extraction 
(graph to text) tasks.  
(DFS). BFS’s scoping resembles a genealogy explora-
This paper presents a new traversal algorithm, Gen-
tion process that treats first relations between individ-
BFS, which is essentially the BFS algorithm adapted 
uals that are at the same depth level (relation degree) 
to the genealogical domain. The Gen-BFS algorithm 
in the family tree, moving from the selected node’s 
level to the outer level nodes. However, the definition  is formally defined as follows: 
of relation degrees in genealogy (i.e., consanguinity)   
 

Algorithm 1 
Gen-BFS algorithm. 
Input: Node (SP), Depth (D) 
Output: Traverse queue (TQ) 
Initialization: Node queue (NQ), Depth queue (DQ), Current depth 
(CD = 0), Nodes to depth increase (NTDI = 1), Next nodes to depth 
increase (NNTDI = 0)  
1.  NQ enqueue SP 
2.  while NQ is not empty 
3.        n = NQ dequeue 
4.        DQ enqueue n 
5.        if n is Person 
6.            kn = n→{fam } union n→{fam } 
child parent
7.        else 
8.            kn = n→{child }  union n→{parent } 
fam fam
9.        NNTDI = NNTDI + count (kn not in NQ) 
10.        NTDI = NTDI – 1 
11.        if NTDI = 0 
12.            if n is Person 
13.                CD = CD + 1 
14.                   if CD > D 
15.                       break while 
16.            NTDI = NNTDI 
17.            NNTDI = 0 
18.        for n in kn 
19.            if n not in NQ 
20.                NQ enqueue n 
21.  while DQ is not empty 
22.        dn = DQ dequeue 
23.        TQ enqueue dn 
24.        if dn is Person 
25.            for f in dn→{fam } 
parent
26.                for p in f→{parent } 
fam
27.                    if p not in DQ and p not in TQ 
28.                        TQ enqueue p 
29.  return TQ   
Fig. 8. Gen-BFS algorithm15. 
 
Where each node can be a Person or a Family, each 
Figure 8 illustrates the Gen-BFS traversal applied 
Person  node  has  two  links  (edges)  types:  fam  
child to the family tree presented in Figure 2. As shown in 
(FAMC in GEDCOM standard) and fam  (FAMS 
parent Figure 8, Gen-BFS is aware of the genealogical mean-
in GEDCOM standard), each Family has the opposite 
ing of the nodes and reduces the tree traversal's logical 
edge types: child  and parent . Where {fam } is 
fam fam child depth. It ignores families in terms of relation degree, 
the collection of all the families in which a person is 
considers SP's spouses as the same degree as SP and 
considered a child (biological family and adopted fam-
SP's parents and children as first degree, and keeps 
ilies), {fam } is the collection of all the families in 
parent siblings and grandparents as second-degree. In partic-
which a person is a parent (spouse) (i.e., all the per-
ular, lines 1-20 in Algorithm 1 represent a BFS-style 
son's marriages), {child } is a collection of all the 
fam traverse over the graph. In lines 5-8, the algorithm in-
persons that are considered to be children in a family 
troduces  domain  knowledge  and  adds  nodes  to  its 
and {parent } is a collection of all the persons con-
fam queue according to the node type. The code in lines 9-
sidered to be a parent in a family. For example, the SP 
17 ensures that the traversal will stop at the desired 
in Figure 2 is linked to two nodes. The link type to F1 
depth level. If the current node is a Person (line 12) 
is fam , and the link type to F4 is fam . The fam-
child parent and the current depth (CD) is about to get deeper than 
ily F1 in Figure 2 has two types of links. The link type 
the required depth (D), then the while loop will end 
to SP, P7, P8 is child , and the link type to P1 and P2 
fam (line 14). Otherwise, the Persons and Families in the 
is parent . 
fam
 
15 An algorithm step is noted as S. The degree of  lows: Zero-degree relation (self) - turquoise, First-de-
relation is noted as D. Relations are color-coded as fol- gree relations – black, and Second-degree relations – 
brown. 

current depth (kn) will be added to the node queue  Genealogical graphs contain instances of two explicit 
(NQ) and may (depending on the stop mechanism) be  classes: Person (E21 in CIDOC-CRM) and family that 
added to the depth queue (DQ). In line 21, the depth  can be represented as a Group (E74 in CIDOC-CRM); 
queue (DQ) holds all the Family nodes and most of the  and several implicit classes: Place (E53), Event (E5), 
Person  nodes  (except  for  spouses  of  the  last  depth  Death (E69), Birth (E67) and others. These implicit 
level’s Person nodes) within the desired depth level.  classes are not structured as separate entities in the 
For example, traversing with D = 1 over the family  GEDCOM standard, but need to be extracted from the 
tree in Figure 2 will result in DQ that contains SP and  GEDCOM  attributes.  Properties  matching  various 
her children and parents (F1, F4, P10, P1, P2, P12, and  GEDCOM  relations  can  also  be  easily  found  in 
P13). However, according to the genealogical defini- CIDOC-CRM,  e.g.,  the  relation  of  a  person  to  its 
tion of depth levels in a family relationship, the chil- children can be represented using P152 (is parent of).  
dren’s spouses, P11 and P14 (but not the grandchil-  
dren, F5 and F6, which belong to D = 2) should also 
be retrieved. Lines 21-28 address this issue and add 
the missing Person nodes, thus logically reducing the 
depth of the graph. 
Each family tree was split into sub-graphs using the 
Gen-BFS algorithm. New sub-graphs were created for 
each person as SP (source person) and its relations at 
different depth levels. Therefore, there is an overlap 
between the sub-graphs (a person can appear in sev-
eral sub-graphs), and the sub-graphs cover all the in-
dividuals  and relations  in  a given  family  tree.  The 
Gen-BFS traversal algorithm is used both for dataset 
generation and for selecting the scope of the user's 
query in the inference phase (i.e., when answering the 
question). 
Once extracted, each genealogical sub-graph was 
presented as a knowledge graph. This study adopted 
an event-based approach to data modeling presented 
in  the past  literature  ([2,  31,  113]).  As  in [113],  a 
  
formal representation of the GEDCOM heterogeneous 
graph  (excluding  the  unstructured  texts)  as  a  Fig. 9. GEDCOM individual’s knowledge graph in the CIDOC-
knowledge  graph  was  implemented  using  CIDOC- CRM-based format. 
CRM, but in a more specific manner (e.g., we used 
Figure 9 is an example of a representation of the 
concrete events and properties such as birth, brought 
GEDCOM  sub-graph  as  a  knowledge  graph.  As 
into  life  as  opposed  to  [113]  that  used  generic 
illustrated in the figure, the SP node is an instance of 
vocabulary). We chose to use CIDOC-CRM as it is a 
the class Person and has a relation (property) to a birth 
living standard (ISO 21127:2014) for cultural heritage 
event (E21=>P98=>E67) with a relation to the place, 
knowledge representation. CIDOC-CRM is designed 
Paris (E67=>P7=>E53) and a relation to the birth year 
as  “a  common  language  for  domain  experts”  and 
with the value 1950 (E67=>P4=>E52). Representing 
“allows  for  the  integration  of  data  from  multiple 
GEDCOM as a knowledge graph is a critical step as 
sources in a software and schema-agnostic fashion” 
the  dataset  generation  method  is  based  on  well-
[60]. It has been applied as a base model and extended 
established knowledge-graph algorithms, as described 
in many domains related to cultural heritage, and in 
next. 
this study, it was chosen as a basis for defining the 
genealogical domain ontology due to its standard and 
3.1.2. Text passage generation 
generic nature and event-based structure, that enables 
Next, a textual passage from each sub-graph is gen-
n-ary rather than binary relationships between entities 
erated, representing the SP's genealogical data based 
in  the  ontology,  as  required  for  representing 
on the graph-to-sequence. Text passages were gener-
genealogical and biographic data based on events in 
ated  using  a  knowledge-graph-to-text  DNN  model 
families and person’s lives (e.g., E67 represents a birth 
[68]  and  completed  (for  low  model  confidence  or 
event that connects a person, a place and a time span).  

missing  facts)  with  knowledge-graph-to-text  tem- To illustrate the problem, consider a user asking about 
plate-based methodology [76]. It is important to note  the  SP's  (John's)  grandfather:  "Where  was  John's 
that converting the obtained genealogical knowledge  grandfather born?" or "Where was Tim Cohen born?", 
sub-graphs to text is a more straightforward task than  where Tim Cohen refers to John’s grandfather. To an-
the open domain knowledge-graph-to-text or generic  swer both questions without multi-hop reasoning for 
commonsense  knowledge-graph-to-text  task,  since  resolution of multiple references to the same person, 
they are well structured and relatively limited in their  the graph-to-text template-based rules include patterns 
semantics. For example, the sub-graph presented in  that encapsulate both the SP's relationship type (John's 
Figure 9 can be converted to a sentence with template  grandfather) and the relative's name (Tim Cohen), thus 
rules or using DNN models. A rule example will be:  allowing the model to learn that Tim Cohen is John's 
[First Name] [Last Name] was born in [Birth Year] in  grandfather. There are three types of references to a 
[Birthplace] = "John Williams was born in 1950 in  person that allows the DNN model to resolve single or 
Paris".  multi-hop questions: 1) Direct referencing to a person 
  with his/hers first and last name (e.g., John Williams), 
Using a knowledge-graph-to-text DNN model [68]  2) Partial referencing to a person with his/hers first or 
and a  knowledge-graph-to-text templates methodol- last name (e.g., John), and 3) Multi-hop encapsulation, 
ogy [76], multiple variations of sentences conveying  i.e., referencing to a person with their relative name to 
the same facts (comprised of the same nodes and edges  the SP (e.g., Alexander's son). 
in the graph) were composed based on different tem- As a result of the above processing, multiple text 
plates and combined with the sentence paraphrasing  passages were created for each SP's sub-graph. Since 
using a DNN-based model (the model of [63]). Most  each sentence is standalone and contains one fact,  sen-
of  the  text  passages  were  generated  using  a  DNN  tences were randomly ordered within each text pas-
model. However, the template-based method added  sage. Thus, even if the passage is longer than the neu-
variations that the DNN model did not capture. Table  ral model's computing capability, the model will likely 
1 above presents examples of such sentences created  encounter  all  types of  sentences  during  its  training 
for the sub-graph in Figure 9.   process. These text passages were further encoded as 
Another  critical  challenge  resolved  by  this  ap- vectors (i.e., embeddings) to train a DNN model that 
proach is the multi-hop question answering problem,  learns contextual embeddings to predict the answer 
where the model needs to combine information from  (i.e., start and end positions in the text passage) for a 
several sentences to answer the question. Although  given question.  
there are multi-hop question answering models pre-  
sented in the literature [30, 74], their accuracy is sig-    
nificantly lower than a single-hop question answering. 

Table 1 
Genealogical-knowledge-graph-to-text context template example.  
Template-based rule example  Result  Reference type 
[First Name] [Last Name] was born in [Birth Year] in [Birth- John Williams was born in 1950 in  Direct 
place]  Paris 
[First Name] was born in [Birth Year] in [Birthplace]  John was born in 1950 in Paris  Partial 
[Name relative of SP] ([First Name] [Last Name]) was born  Alexander's son (John Williams)  Multi-hop encapsulation 
in [Birth Year] in [Birthplace]  was born in 1950 in Paris 
[First Name] was born in [Birthplace] in [Birth Year]  John was born in Paris in 1950  Partial 
[Relative First Name] [Relative Last Name] ([Relation to  Alexander Williams (John's father)  Multi-hop encapsulation 
SP]) was born in [Birth Year] in [Birthplace]  was born in 1927 in Nice. 
In [Birth Year] [First Name] was born  In 1950 John was born  Partial 
[Birthplace] was [First Name] 's birthplace  Paris was John's birthplace  Partial 
ysis tasks were adopted to define characteristic tem-
3.1.3. Generation of questions and answers   plates for natural language questions that a user may 
Using the generated text passages (contexts), pairs  ask about the SP or its relatives. Some of these ques-
of questions and answers were created. The answers  tions  can  be  answered  directly  from  the  structured 
were generated first, and then the corresponding ques- knowledge  graph  (e.g.,  “When  was  Tim’s  father 
tions were built for them as follows. Knowledge graph  born?”), while others can only be answered using the 
nodes and properties (relationships), as well as named  unstructured texts attached to the nodes (e.g., “Did 
entities  and  other  characteristic  keywords  extracted  Tim’s father have cancer?”). 
from  free  text  passages  were  used  as  answers.  To  A  DNN-based  model  for  generating  additional 
achieve extensive coverage, multiple approaches were  types of questions [25] was used to complement the 
used for generation of questions. First, a rule-based  rule-based  method.  The  neural  question  generation 
approach was applied for question generation from  model predicted questions from all the unstructured 
knowledge graphs [140] and a statistical question gen- texts in the GEDCOM data and produced 24% of the 
eration technique [44] was utilized for WH question  questions in the dataset (excluding duplicate questions 
generation from the unstructured texts in GEDCOM.   already created using the WH-based and rule-based 
Most of the questions (73%)  were created using  approaches). 
these methods. To identify the types of questions typ-  
ical of the genealogical domain and define rule-based  Table 2 
templates for their automatic generation, this study ex-
Knowledge-graph-to-text question template examples.  
amined the genealogical analysis tasks that users tend 
Template-based rule example  Result 
to perform on genealogical graphs [10]. These tasks 
include: (1) identifying the SP's ancestors (e.g., par- How many children did [First Name]  How many children 
[Last Name] have?  did John Williams 
ents,  grandparents)  or  descendants  (e.g.,  children, 
have? 
grandchildren), (2) identifying the SP's extended fam-
How many grandchildren did [Rela- How many grand-
ily (second-degree relations), (3) identifying family 
tive First Name] [Relative Last Name]  children did Alexan-
events, such as marriages, (4) identifying influential  ([Relation to SP]) have?  der Williams (John's 
individuals (e.g., by occupation, military rank, aca- father) have? 
Was [Birthplace] [First Name] 's  Was Paris John's 
demic  achievements,  number  of  children),  and  (5) 
birthplace?  birthplace? 
finding information about dates and places, such as the 
date of birth, and place of marriage [4, 10]. These anal-
 

| Template-based rule example                                  | Result                             | Reference type          |
|:-------------------------------------------------------------|:-----------------------------------|:------------------------|
| [First Name] [Last Name] was born in [Birth Year] in [Birth- | John Williams was born in 1950 in  | Direct                  |
| place]                                                       | Paris                              |                         |
| [First Name] was born in [Birth Year] in [Birthplace]        | John was born in 1950 in Paris     | Partial                 |
| [Name relative of SP] ([First Name] [Last Name]) was born    | Alexander's son (John Williams)    | Multi-hop encapsulation |
| in [Birth Year] in [Birthplace]                              | was born in 1950 in Paris          |                         |
| [First Name] was born in [Birthplace] in [Birth Year]        | John was born in Paris in 1950     | Partial                 |
| [Relative First Name] [Relative Last Name] ([Relation to     | Alexander Williams (John's father) | Multi-hop encapsulation |
| SP]) was born in [Birth Year] in [Birthplace]                | was born in 1927 in Nice.          |                         |
| In [Birth Year] [First Name] was born                        | In 1950 John was born              | Partial                 |
| [Birthplace] was [First Name] 's birthplace                  | Paris was John's birthplace        | Partial                 |

| Template-based rule example           | Result               |
|:--------------------------------------|:---------------------|
| How many children did [First Name]    | How many children    |
| [Last Name] have?                     | did John Williams    |
|                                       | have?                |
| How many grandchildren did [Rela-     | How many grand-      |
| tive First Name] [Relative Last Name] | children did Alexan- |
| ([Relation to SP]) have?              | der Williams (John's |
|                                       | father) have?        |
| Was [Birthplace] [First Name] 's      | Was Paris John's     |
| birthplace?                           | birthplace?          |

Finally, additional rules were manually compiled  dense layers (vectors)  with dimensions of the hidden 
using templates [1, 28] to create questions missed by  states in the model (S and E), (2) computing the prob-
previous  methods,  mainly  quantitative  and  yes-no  ability that each token in these layers (vectors) is the 
questions (as illustrated in Table 2). These questions  start (S) or end (E) of the answer, and finally (3) run-
were 3% of all the questions in the datasets. All answer  ning and tuning the baseline BERT model described 
indexes were tested automatically to ensure that the  above for learning S and E. The probability of a token 
answer text exists in the context passage. A random  being the start or the end of the answer is the dot prod-
sample of 120 questions was tested manually by the  uct between the token's numerical representation (i.e., 
researchers as a quality control process, and the ob- embeddings) in the last layer of BERT and the new 
served accuracy was virtually 100%. However, it is  output layers (vectors S or E), followed by a softmax 
still possible that DNN generated some errors. Never- activation over all the tokens. Then, using the genea-
theless,  even  in  this  case,  the  study’s  conclusions  logical training dataset, the model is trained to solve 
would not change, as such errors would have a similar  the task in the study’s domain. It should be noted that 
effect (same embeddings) on all the tested models.  generation methods for pre-trained static node embed-
  dings like node2vec [39] or TransE [14] treat triples as 
3.2 Fine-tuning the BERT-based DNN model for  the training instance for embeddings, which may be 
question answering   insufficient to model complex information transmis-
Fine-tuning a DNN model is the process of adapting  sion between nodes. Therefore, the information is en-
a model that was trained on generic data to a specific  coded from graph nodes into syntactic sentences and 
task and domain [22]. An initial DNN model is usually  then the original BERT approach [22] is applied to 
designed and trained to perform generic tasks on large  generate comprehensive contextual embeddings from 
domain-agnostic texts, like Wikipedia. In the case of  these sentences [43]. 
the open-domain question answering, the BERT base- Figure 11 summarizes the developed genealogical 
line model was pre-trained on English Wikipedia and  question answering pipeline. To simplify the task, the 
Books Corpus [139] using Masked Language Model- proposed architecture asks the user to first select the 
ing (MLM) and Next Sentence Prediction (NSP) ob- family tree from the corpus (future research can elim-
jectives [22]. The MLM methodology is a self-super- inate this step by embedding the family trees [37] and 
vised dataset generation method. For each input sen- ranking them based on similarity to the question [92]). 
tence, one or more tokens (words) are masked, and the  As demonstrated in the figure, the family tree corpus 
model's task is to generate the most likely substitute  (comprised of GEDCOM files) is processed into ques-
for each masked token. In this fill-in-the-blank task,  tion answering datasets for different scopes. The pro-
the model uses the context words surrounding a mask  cess starts when a user selects a specific person from 
token to try to predict what the masked word should  a family tree. Then the user indicates a  scope (a gene-
be. The NSP methodology is also a self-supervised da- alogical relation degree, as described in Figure 1) to 
taset generation method. The model gets a pair of sen- ask about (e.g., the SP itself, first-degree relative, sec-
tences and predicts if the second sentence follows the  ond-degree relatives) and asks a question ("What was 
first one in the dataset. MLM and NSP are effective  Alexander's father's military rank?"). The Gen-BFS al-
ways to train language models without annotations as  gorithm incorporates the SP and the scope to generate 
a basis for various supervised NLP tasks. Combining  a text passage that encapsulates the SP's scope aligned 
MLM and NSP training methods allow modeling lan- with the user intent (equivalent to finding the top K 
guages with both word-level relations and sentence- text passages in the open-domain question answering 
level relations understanding. The pre-trained BERT- pipeline). Finally, a fine-tuned DNN model, selected 
based question answering model was designed with 12  based on the requested relational degree (i.e., a model 
layers, 768 hidden nodes, 12 attention heads, and 110  trained to predict answers on the requested relational 
million parameters. Using such a pre-trained model,  degree), predicts the answer using the generated text 
DNN layers can be added to fit to a specific task [22].  passage and a question as inputs. 
As shown in Figure 10, a new BERT-based model,     
Uncle-BERT, was fine-tuned for genealogical ques-
tion answering as follows: (1) adding a pair of output 

 
Fig. 10. The DNN model fine-tuning process. 
 
 
Fig. 11. Genealogical question answering pipeline (the proposed architecture).
   

4. Experimental design  spouses) with 6,283,082 questions, Gen-SQuAD  us-
1
ing first-degree relations with 28,778,947 questions, 
This section describes the experimental dataset and  and Gen-SQuAD  using second-degree relations with 
2
training conducted to validate the proposed methodol- 75,281,088 questions. Although all generated datasets 
ogy for the genealogical domain.  contain millions of examples, only 131,072 randomly 
selected questions were used from each dataset when 
4.1. Datasets  training the Uncle-BERT models. These were enough 
for the models to converge. Therefore, the size of the 
In  this  research,  3,140  family  trees  containing  dataset did not impact the training results. 
1,847, 224 different individuals from the corpus of the  Each dataset was split into a training set (60%), a 
test set (20%), and an evaluation set (20%). To better 
Douglas E. Goldman Jewish Genealogy Center in Anu 
evaluate the success of the different question answer-
Museum16 were used. The Douglas E. Goldman Jew-
ing models, the 131,072 questions in each dataset were 
ish Genealogy Center contains over 5 million individ-
classified into twelve types. Examples of questions 
uals  and  over  30  million  family  tree  connections 
and their classification types are shown in Table 3. 
(edges) to families, places, and multimedia items. To 
Each question may refer to the SP's relationship type 
comply with the Israeli privacy regulation17 and the 
(e.g., Emily's grandson or by the direct name of the 
European general data protection regulation18 (GDPR), 
relative,e.g., Grace) and target one type of ontological 
only family trees for which the Douglas E. Goldman 
Jewish Genealogy Center in Anu Museum has been  entity as an answer (date, place, name, relationship 
granted consent or rights to publish online were used  type). Questions were classified into types based on 
in the dataset generation. Moreover, as far as possible,  the  template,  if generated  using  the  template-based 
all records containing living individuals have been re- method  (e.g.,  templates  using  place  attributes  were 
moved from the dataset. Furthermore, all personal in- classified as “place”, and date attributes as “date”), 
formation and any information that can identify a spe- based on the WH question (e.g., When questions were 
cific person in this paper's examples, including the ex- classified as “date”, and Where as “place”), if gener-
amples in the figures, have been altered to protect the  ated using the WH generation algorithm, or as general 
individuals' privacy.  information / named entity, if generated by the DNN 
From the filtered GEDCOM files belonging to the  model. Therefore, the information / named entity may 
above corpus, and after removing some files with pars- also  include  the other  types of  questions.  It  is  im-
ing or encoding errors, three datasets were generated:  portant to note that these questions are semantically 
similar to the open-domain questions in SQuAD [90,  
Gen-SQuAD  using zero relation degree (SP and its 
0
91] datasets. 
   
 
16 https://dbs.anumu-  
seum.org.il/skn/en/c6/e18493701  18 https://gdpr-info.eu/ 
17 https://www.gov.il/BlobFolder/legal-
info/data_security_regulation/en/PROTEC-
TION%20OF%20PRIVACY%20REGULA-
TIONS.pdf 

Table 3 
Question types.  
Question type / objective  Examples  Source 
Name  What is Emily's full name?  Rule-based 
What is Emily's last name?  Rule-based 
Date  When was Emily born?  Rule-based 
When did Emily get married?  Rule-based 
Place  Where was Emily buried?  Rule-based 
Where did Emily live?  Rule-based 
Information / named entity  Who was Emily's first boyfriend?  DNN 
Did Emily go to college?  DNN 
First-degree relation  Who was Emily's son?  DNN, rule-based 
Who was Jonathan?  DNN 
Second-degree relation  How many sisters did Emily have?  Rule-based 
How many brothers did Emily have?  Rule-based 
First-degree date  When was Emily's husband born?  Rule-based 
When was John born?  Rule-based 
First-degree place  Where was Emily's father born?  Rule-based 
Where was Alexander born?  Rule-based 
First-degree information / named entity  What was Emily's father's academic degree?  DNN 
What was Alexander's illness?  DNN 
Second-degree date  When did Emily's sister die?  Rule-based 
When did Yalma die?  Rule-based 
Second-degree place  Where was Emily's grandson born?  DNN 
Where was Grace born?  Rule-based  
Second-degree information / named entity  What was Emily's grandfather's rank in the military?  DNN 
Where was Tim's first internship as a lawyer?  DNN 
 
   

| Question type / objective                | Examples                                             | Source          |
|:-----------------------------------------|:-----------------------------------------------------|:----------------|
| Name                                     | What is Emily's full name?                           | Rule-based      |
|                                          | What is Emily's last name?                           | Rule-based      |
| Date                                     | When was Emily born?                                 | Rule-based      |
|                                          | When did Emily get married?                          | Rule-based      |
| Place                                    | Where was Emily buried?                              | Rule-based      |
|                                          | Where did Emily live?                                | Rule-based      |
| Information / named entity               | Who was Emily's first boyfriend?                     | DNN             |
|                                          | Did Emily go to college?                             | DNN             |
| First-degree relation                    | Who was Emily's son?                                 | DNN, rule-based |
|                                          | Who was Jonathan?                                    | DNN             |
| Second-degree relation                   | How many sisters did Emily have?                     | Rule-based      |
|                                          | How many brothers did Emily have?                    | Rule-based      |
| First-degree date                        | When was Emily's husband born?                       | Rule-based      |
|                                          | When was John born?                                  | Rule-based      |
| First-degree place                       | Where was Emily's father born?                       | Rule-based      |
|                                          | Where was Alexander born?                            | Rule-based      |
| First-degree information / named entity  | What was Emily's father's academic degree?           | DNN             |
|                                          | What was Alexander's illness?                        | DNN             |
| Second-degree date                       | When did Emily's sister die?                         | Rule-based      |
|                                          | When did Yalma die?                                  | Rule-based      |
| Second-degree place                      | Where was Emily's grandson born?                     | DNN             |
|                                          | Where was Grace born?                                | Rule-based      |
| Second-degree information / named entity | What was Emily's grandfather's rank in the military? | DNN             |
|                                          | Where was Tim's first internship as a lawyer?        | DNN             |

4.2. Uncle-BERT fine-tuning  input length was greater than the Max question tokens, 
it was trimmed. Max sequence tokens are the maxi-
For fine-tuning Uncle-BERT19, the generated Gen- mum tokens to process from the combined context and 
SQuAD training datasets were used. Each context in  question inputs.  
the  Gen-SQuAD , Gen-SQuAD , and Gen-SQuAD   If the cumulative context and question length was 
0 1 2
datasets was lowercased and tokenized using Word- longer than the Max sequence tokens hyperparameter 
Piece [132].  value, the context was split into shorter sub-texts using 
  a sliding window technique; the Doc stride represents 
the sliding window overlap size. For example, con-
sider the following hyperparameters’ values: the max 
sequence tokens hyperparameter is 25, the doc stride 
hyperparameter is 6, and the following training exam-
ple:  “[CLS]  When  was  Matt  Adler's  father  born? 
[SEP] Matt's father (Noah Adler) was born in 1950 in 
London, England. Matt's father (Noah Adler) was a 
male. Matt's brother (Joanne Adler) was a male. Matt 
  Adler was born in 1975 in London, England. Matt's 
mother (Carol) was born in 1950. [CLS]”; the question 
Fig. 12. Uncle-BERT model input example. 
of the training example contains 7 tokens, and 18 to-
kens are left for the context. Therefore, the context 
Figure  12  presents  the  model's  input,  where  the 
will be split into three training examples: 1) “[CLS] 
[CLS] tag, which stands for classifier token, is the be-
When was Matt Adler's father born? [SEP] Matt's fa-
ginning of the input, followed by the first part of the 
ther (Noah Adler) was born in 1950 in London, Eng-
input - the question. The [SEP] tag, which stands for a 
land. Matt's father (Noah Adler) was a male [CLS]” 
separator, separates the first part of the input (i.e., a 
(i.e., tokens 1 to 18), 2) “[CLS]  When was Matt Ad-
question) and the second part – the context. [CLS] at 
ler's father born? [SEP] father (Noah Adler) was a 
the end indicates the end of the input. 
male. Matt's brother (Joanne Adler) was a male. Matt 
To evaluate the effect of the depth of the consan-
Adler was born in [CLS]” (i.e., tokens 12 to 30), 3) 
guinity  scope  on  the  model's  accuracy,  an  Uncle-
“[CLS] When was Matt Adler's father born? [SEP] a 
BERT model was trained for each of the three da-
male. Matt Adler was born in 1975 in London, Eng-
tasets:  Uncle-BERT   using  Gen-SQuAD ,  Uncle-
0 0
land. Matt's mother (Carol) was born in 1950. [CLS]” 
BERT  using Gen-SQuAD , and Uncle-BERT  using 
1 1 2
(i.e., tokens 24 to 42). The model will be trained with 
Gen-SQuAD . All models were trained with the same 
2
the same question on the three new examples; if the 
hyperparameters, that are shown in Table 4. 
answer span does not exist in an example, it is consid-
Table 4 
ered unanswerable. 
Uncle-BERT training hyperparameters.  
Max answer tokens is the maximum number of to-
Hyperparameters  Value 
kens that a generated answer can contain. Train size is 
Max question tokens  64  the number of examples used from the dataset during 
the training cycle.  
Max sequence tokens  512 
As is customary with the SQuAD benchmark, an F1 
Max answer tokens  30 
score was calculated to evaluate Uncle-BERT models: 
Doc stride  128 
 
Batch size  8  𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 ∗ 𝑟𝑒𝑐𝑎𝑙𝑙
𝐹1 = 2 ∗   
Learning rate  3e-5  𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 + 𝑟𝑒𝑐𝑎𝑙𝑙
 
Train size  131,072 
Precision equals the fraction of correct tokens out of 
Epocs  20 
the retrieved tokens (i.e., words that exist in both the 
 
predicted and the expected answer), and recall equals 
Max question tokens is the maximum number of to-
the fraction of the correct tokens in the retrieved (pre-
kens to process from the question input; if the question 
 
19  A  link  to  the  code:  https://github.com/om-
rivm/Uncle-BERT 

| Hyperparameters     | Value   |
|:--------------------|:--------|
| Max question tokens | 64      |
| Max sequence tokens | 512     |
| Max answer tokens   | 30      |
| Doc stride          | 128     |
| Batch size          | 8       |
| Learning rate       | 3e-5    |
| Train size          | 131,072 |
| Epocs               | 20      |

dicted) answer out of the tokens in the expected an-
swer. This metric allows measuring both exact and 
partial answers. 
5. Results 
To evaluate the accuracy of the proposed fine-tuned 
models, the Gen-SQuAD  dataset was used to repre-
2
sent a real-world use-case in which a user is investi-
gating her genealogical roots with  the genealogical 
scope of two relation degrees (generations)20. To com-
pare the model’s accuracy for each type of answer, an 
 
F1  score  was  calculated  to  evaluate  every  Uncle-
BERT  model  (i.e.,  Uncle-BERT   trained  on  Gen- Fig. 14. The three Uncle-BERT model’s train F1 score22. 
0
SQuAD , Uncle-BERT  trained on Gen-SQuAD , and 
0 1 1
Figures 13 and 14 show the training loss and F1 
Uncle-BERT  trained on Gen-SQuAD ). An overall 
2 2
scores of each of the three models. As expected, the 
accuracy  evaluation  of  the  three  models  was  per-
more complex the context and questions, the lower the 
formed by calculating the F1 score for a mix of ran-
F1 score. While on narrow persons’ contexts and ques-
dom questions of all types.  
tions (Gen-SQuAD ), the model achieved an F1 score 
0
of  99.84;  on  second-degree  genealogical  relations 
(Gen-SQuAD ), it achieved only an F1 score of 80.28. 
2
Furthermore, as can be observed in Table 5, com-
pared  to  the  Uncle-BERT   model  (trained  with 
2
broader contexts of second-degree genealogical rela-
tions), the Uncle-BERT , which was trained using in-
0
formation about the SP and its spouses, fails to answer 
questions of any kind, including questions about the 
SP alone. We hypothesize that the model overfits to 
narrow  contexts and therefore  cannot handle larger 
context (Gen-SQuAD ) "noise". This emphasizes the 
2
importance of the context size in the training data. Un-
cle-BERT successfully answers most of the question 
1 
types and even overtakes Uncle-BERT in place-re-
2 
lated questions. Except for place-related questions, it 
seems that a broader context improves the model's ac-
curacy (Uncle-BERT ). 
  2
Next, the best model, Uncle-BERT , was compared 
2
Fig. 13. The three Uncle-BERT model’s train loss21. 
to several state-of-the-art open-domain question-an-
swering DNN models. To this end, all the following 
models were trained using SQuAD 2.0 [90]: BERT 
[22],  Distilbert  [93],  RoBERTa  [70],  Electra  [18], 
DELFT [134]. Furthermore, to evaluate the effective-
ness of the proposed genealogical question answering 
pipeline compared to the state-of-the-art knowledge 
graph-based pipeline, the genealogical adaptation of 
 
20 Similar to Anu Museum user interface -  21 x-axis: number of epochs, y-axis: loss 
https://dbs.anumu- 22 x-axis: number of epochs, y-axis: F1 score 
seum.org.il/skn/en/c6/e8492037/Personalities/Weiz-
mann_Chaim 

the DELFT model, Uncle-DELFT , was created. Un- that these questions are more generic and have more 
2
cle-DELFT  based on BERT combined with the GNN  common  features  among  different  domains  than 
2,
graph traversal, was trained on Gen-SQuAD .  unique features in the genealogical domain. Further-
2
As can be observed in Table 6, the baseline BERT  more,  the  current  state-of-the-art  knowledge  graph 
model  trained  on  the  open-domain  SQuAD  2.0  pipeline (i.e., DELFT) achieved performance similar 
achieved  an  F1  score  of  83  on  the  open-domain   to simpler BERT-based models. This indicates that 
SQuAD 2.0 dataset [90]. However, on the genealogi- while it is beneficial for open-domain questions, it is 
cal domain dataset (Gen-SQuAD ) it achieved a sig- not as effective in the genealogical domain. This result, 
2 ,
nificantly lower F1 score (60.12) compared to the Un- combined with  the additional complexity of DELFT, 
cle-BERT   (81.45).  The  fact  that  Uncle-BERT makes it less satisfactory in this domain (except for 
2 2 
achieves a higher F1 score is not surprising since the  date questions, as mentioned above). 
model was trained on genealogical data, as opposed to  Interestingly, the “basic” BERT model outperforms 
the baseline BERT model trained on the open-domain  all the newer BERT-based models (except for Uncle-
question  data.  However,  when  comparing  Uncle- BERT ).  Furthermore,  the  fact  that  Uncle-BERT
2 1 
BERT  to Uncle-DELFT , it is clear that the perfor- achieved a higher F1 score on place type questions 
2 2
mance improvement is due to the proposed methodol- may indicate that place type questions may be more 
ogy and not just due to the richer or domain-specific  sensitive to "noise" or broad context. For example, 
training data. Moreover, the DELFT method is much  place  names  may  have  different  variations  for  the 
more complex than BERT, yet it achieved a lower  same entity (high "noise"), e.g., NY, NYC, New York, 
score even when trained on the same domain-specific  and New York City are all references to the same en-
data. The fact that the vast majority of entities (found  tity. This variety makes the model’s task more difficult, 
in both the “user” question and the expected answer)  thus adding broader contextual information and other 
exists only in the unstructured data makes it hard for  types  of  "noise"  (e.g.,  other  entities,  more  people 
the GNN to find the correct answer (i.e., to complete  names, and dates), which may reduce the model’s ac-
the graph). This finding emphasizes the uniqueness of  curacy. Another possible reason for Uncle-BERT ’s
2  
a genealogical question answering task compared to  lower accuracy on place type questions may be the fact 
the open-domain question-answering and the need for  that  Uncle-BERT was  trained  with  both  one-hop-
2 
the end-to-end pipeline and methodology for training  away and two-hop-away contexts while Uncle-BERT
1 
and using DNNs for this task, as presented in this pa- was  trained  only  with  one-hop-away  contexts.  The 
per. Since Uncle-BERT  achieved a higher accuracy  fact that the F1 score of the model is smaller on sec-
2
score than the more complex Uncle-DELFT  model, ond-degree place objective questions (1.39) than on 
2  
we conclude that the proposed method reduces com- first-degree (4.72) and zero-degree (10.01) place ob-
plexity while increasing accuracy.  jective questions may reinforce this indication. How-
As shown in Table 6, although some questions ap- ever, it is important to notice that in many cases, this 
pear in both Gen-SQuAD  and SQuAD 2.0 datasets,  factor will not affect the F1 score since the F1 score 
2
there is still a significant difference between open- does not use the position of the answer (start and end 
domain questions and genealogical questions. Except  index), but only the selected tokens compared to the 
for Uncle-DELFT in the case of date questions, all the  answer tokens. Since most children and parents live in 
2 
state-of-the-art  models  failed  to  answer  natural  the same place, either the parent’s place (e.g., birth-
genealogical  questions  compared  to  Uncle-BERT place) or the child’s place can be selected by the model 
2 
(and in many cases, even compared to Uncle-BERT ).  without affecting the F1 score. Table 7 presents some 
1
However,  Uncle-DELFT   was  successful  regarding  examples  of  answer  predictions  for place  objective 
2
date  questions.  This  may  imply  that  objective  date  questions by Uncle-BERT  and Uncle-BERT . These 
1   2
questions are harder to extract from unstructured texts  results suggest that higher accuracy can be achieved 
and the graph structure contributes to resolving such  by classifying the question types and using a different 
questions.  Moreover,  BERT's  success  on  SP’s  date  model for different question types and relation depths. 
questions (compared to Uncle-BERT ) may suggest     
2

Table 5 
Uncle-BERT models F1 score on Gen-SQuAD . 
2
Question objective  Uncle-BERT   Uncle-BERT   Uncle-BERT  
0 1 2
Name  44.53  95.14  97.64 
Date  21.60  52.48  55.10 
Place  27.54  88.53  78.52 
Information \ named entity  16.91  15.22  87.40 
First-degree relation  19.58  86.94  89.45 
Second-degree relation  20.66  63.45  82.52 
First-degree date  13.26  43.44  53.85 
First-degree place  34.17  86.55  81.83 
First-degree information / named entity  8.95  12.21  87.28 
Second-degree date  11.68  43.12  44.87 
Second-degree place  33.10  80.51  79.12 
Second-degree information / named entity  8.37  11.34  81.04 
Overall  19.73  69.92  81.45 
 
Table 6 
F1 scores of Uncle-BERT  and other state-of-the-art models on Gen-SQuAD . 
2 2
Question objective  BERT  Distilbert  RoBERTa  Electra  DELFT  Uncle-DELFT   Uncle-BERT  
2 2
Name  28.27  28.54  38.97  21.30  32.99  39.84  97.64 
Date  60.58  53.30  44.33  34.92  39.62  79.35  55.10 
Place  74.96  64.67  40.41  26.03  36.27  66.66  78.52 
70.58 
Information / named entity  71.20  66.79  34.96  31.72  40.91  87.40 
First-degree relation  65.20  62.41  55.32  49.10  34.42  46.48  89.45 
Second-degree relation  55.85  46.31  42.03  43.09  37.56  41.01  82.52 
First-degree date  46.45  48.16  42.54  37.45  40.58  64.84  53.85 
First-degree place  74.58  66.73  47.02  21.50  36.64  75.78  81.83 
First-degree information / named 
60.57  64.54  35.95  35.30  37.59  68.78  87.28 
entity 
Second-degree date  39.49  39.75  23.30  26.99  38.29  60.15  44.87 
Second-degree place  69.49  66.28  41.34  22.47  36.60  66.40  79.12 
Second-degree information / named  47.26 
60.19  62.38  34.37  37.15  35.70  81.04 
entity 
Overall  60.12  60.19  39.45  43.39  37.56  42.96  81.45 
 
Table 7 
Uncle-BERT ‘s and Uncle-BERT ‘s prediction examples 
1 2 
Question  Question  Context (relevant parts)  Correct  Uncle- Uncle-
objective  Answer  BERT   BERT  
1 2
Where was John  … John was born in Poland in 1866 … John grew up 
Place  Poland  in Poland  PL 
born?  in PL until he was… 
Where was John  … John died and was buried in Germany during … 
Place  Germany  Germany  France 
buried?  Kate (John’s daughter) was born in France… 
First-degree  Where did John’s  … Matt (John’s father) was born in Warsaw, Poland 
Warsaw  Warsaw  in Warsaw 
place  father get married?  … Matt married Elain in Warsaw… 
… Matt died at home in Poland surrounded… his fa-
First-degree  Where was Matt 
ther (John’s grandfather) was killed in Pruszkow in  Poland  Poland  Pruszkow 
place  killed? 
1850… 
Second-de- Where did John’s  … his father (John’s grandfather) was killed in Prusz-
Pruszkow  in Pruszkow  Pruszkow 
gree place  grandfather die?  kow in 1850… 

| Question objective                       |   Uncle-BERT |   Uncle-BERT |   Uncle-BERT |
|                                          |            0 |            1 |            2 |
|:-----------------------------------------|-------------:|-------------:|-------------:|
| Name                                     |        44.53 |        95.14 |        97.64 |
| Date                                     |        21.6  |        52.48 |        55.1  |
| Place                                    |        27.54 |        88.53 |        78.52 |
| Information \ named entity               |        16.91 |        15.22 |        87.4  |
| First-degree relation                    |        19.58 |        86.94 |        89.45 |
| Second-degree relation                   |        20.66 |        63.45 |        82.52 |
| First-degree date                        |        13.26 |        43.44 |        53.85 |
| First-degree place                       |        34.17 |        86.55 |        81.83 |
| First-degree information / named entity  |         8.95 |        12.21 |        87.28 |
| Second-degree date                       |        11.68 |        43.12 |        44.87 |
| Second-degree place                      |        33.1  |        80.51 |        79.12 |
| Second-degree information / named entity |         8.37 |        11.34 |        81.04 |
| Overall                                  |        19.73 |        69.92 |        81.45 |

| Question objective                |   BERT |   Distilbert |   RoBERTa |   Electra |   DELFT |   Uncle-DELFT |   Uncle-BERT |
|                                   |        |              |           |           |         |             2 |            2 |
|:----------------------------------|-------:|-------------:|----------:|----------:|--------:|--------------:|-------------:|
| Name                              |  28.27 |        28.54 |     38.97 |     21.3  |   32.99 |         39.84 |        97.64 |
| Date                              |  60.58 |        53.3  |     44.33 |     34.92 |   39.62 |         79.35 |        55.1  |
| Place                             |  74.96 |        64.67 |     40.41 |     26.03 |   36.27 |         66.66 |        78.52 |
| Information / named entity        |  71.2  |        66.79 |     34.96 |     31.72 |   40.91 |         70.58 |        87.4  |
| First-degree relation             |  65.2  |        62.41 |     55.32 |     49.1  |   34.42 |         46.48 |        89.45 |
| Second-degree relation            |  55.85 |        46.31 |     42.03 |     43.09 |   37.56 |         41.01 |        82.52 |
| First-degree date                 |  46.45 |        48.16 |     42.54 |     37.45 |   40.58 |         64.84 |        53.85 |
| First-degree place                |  74.58 |        66.73 |     47.02 |     21.5  |   36.64 |         75.78 |        81.83 |
| First-degree information / named  |  60.57 |        64.54 |     35.95 |     35.3  |   37.59 |         68.78 |        87.28 |
| entity                            |        |              |           |           |         |               |              |
| Second-degree date                |  39.49 |        39.75 |     23.3  |     26.99 |   38.29 |         60.15 |        44.87 |
| Second-degree place               |  69.49 |        66.28 |     41.34 |     22.47 |   36.6  |         66.4  |        79.12 |
| Second-degree information / named |  60.19 |        62.38 |     34.37 |     37.15 |   35.7  |         47.26 |        81.04 |
| entity                            |        |              |           |           |         |               |              |
| Overall                           |  60.12 |        60.19 |     39.45 |     43.39 |   37.56 |         42.96 |        81.45 |

| Question     | Question            | Context (relevant parts)                               | Correct   | Uncle-      | Uncle-    |
| objective    |                     |                                                        | Answer    | BERT        | BERT      |
|              |                     |                                                        |           | 1           | 2         |
|:-------------|:--------------------|:-------------------------------------------------------|:----------|:------------|:----------|
| Place        | Where was John      | … John was born in Poland in 1866 … John grew up       | Poland    | in Poland   | PL        |
|              | born?               | in PL until he was…                                    |           |             |           |
| Place        | Where was John      | … John died and was buried in Germany during …         | Germany   | Germany     | France    |
|              | buried?             | Kate (John’s daughter) was born in France…             |           |             |           |
| First-degree | Where did John’s    | … Matt (John’s father) was born in Warsaw, Poland      | Warsaw    | Warsaw      | in Warsaw |
| place        | father get married? | … Matt married Elain in Warsaw…                        |           |             |           |
| First-degree | Where was Matt      | … Matt died at home in Poland surrounded… his fa-      | Poland    | Poland      | Pruszkow  |
| place        | killed?             | ther (John’s grandfather) was killed in Pruszkow in    |           |             |           |
|              |                     | 1850…                                                  |           |             |           |
| Second-de-   | Where did John’s    | … his father (John’s grandfather) was killed in Prusz- | Pruszkow  | in Pruszkow | Pruszkow  |
| gree place   | grandfather die?    | kow in 1850…                                           |           |             |           |

Possible directions for future research may include: 
(1) investigating the tradeoff between rich context pas-
6. Conclusions and future work  sage generation and increasing the Gen-BFS scope, 
(2) integration with DNC or GNNs for dynamic scop-
This  study  proposed  and  implemented  a  multi- ing, (3) finding a method for classifying question types, 
phase  end-to-end  methodology  for  DNN-based  an- (4) investigating the contribution of each question type 
swering natural questions using transformers in the ge- to the accuracy of the model, and developing a model 
nealogical domain.   selection  or  multi-model  method  for  each  question 
The  presented  methodology  was  evaluated  on  a  type,  (5)  investigating  larger  contexts  (relation  de-
large  corpus  of  3,140  family  trees  comprised  of  grees) using models that can handle larger input (e.g., 
1,847, 224  different  persons.  The  evaluation  results  Longformer [58] or Reformer [9]), (6) extending the 
Gen-BFS algorithm to handle missing family relations 
show that a fine-tuned Uncle-BERT  model, trained 
2
by adding a knowledge graph completion step while 
on the genealogical dataset with second degree rela-
traversing the graph, (7) investigating the influence of 
tionships, outperformed all the open-domain state-of-
the order of verbalized sentences and especially the or-
the-art models. This finding indicates that the geneal-
der of person reference types, (8) investigating an ar-
ogy  domain  is  distinctive  and  requires  a  dedicated 
chitecture that will rank family trees (embedding the 
training dataset and fine-tuned DNN model. A com-
entire graph [37]) based on similarity to the question 
parison of the proposed knowledge-graph-to-text ap-
[92]) and eliminate the need for the user to select a 
proach  was  also  found  to  be  superior  to  the  direct 
family tree, (9) investigating the impact of spelling 
knowledge graph-based models, such as DELFT, even 
mistakes and out-of-vocabulary words on the quality 
after domain-adaptation, both in terms of accuracy and 
of the results, (10) and training other transformer mod-
complexity. This study also examined the effect of the 
els on genealogical data to further optimize question 
type of question on the accuracy of the question an-
answering DNN models for the genealogical domain. 
swering model. The date-related questions are differ-
ent as they can be answered with greater accuracy di-
rectly from the knowledge graph and may have more 
References 
generic  features  than  other  question  types,  while 
place-related  questions  are  more  sensitive  to  noise 
[1]  Abujabal, A., Yahya, M., Riedewald, M., & Weikum, G. 
than other question types. In addition, the evaluation 
(2017, April). Automated template generation for ques-
results of the three Uncle-BERT models showed that 
tion answering over knowledge graphs. In Proceedings 
the consanguinity scope of graph traversal used for  of the 26th international conference on world wide web 
generating a training corpus influences the accuracy of  (pp. 1191-1200). 
[2]  Artés, J.C., Conesa, J., & Mayol, E. (2012). Modeling 
the models.  
Genealogical Domain - An Open Problem. KEOD. 
In summary, this paper's contributions are: (1) a ge-
[3]  Auer, S., Bizer, C., Kobilarov, G., Lehmann, J., Cy-
nealogical knowledge graph representation of GED- ganiak, R., & Ives, Z. (2007). Dbpedia: A nucleus for a 
COM standard; (2) a dedicated graph traversal algo- web of open data. In The semantic web (pp. 722-735). 
Springer, Berlin, Heidelberg. 
rithm adapted to interpret the meaning of the relation-
[4]  Ball, R. (2017). Visualizing genealogy through a family-
ships in the genealogical data (Gen-BFS); (3) an auto-
centric  perspective.  Information  Visualization,  16(1), 
matically generated SQuAD-style genealogical train- 74-89. 
ing dataset (Gen-SQuAD); (4) an end-to-end question  [5]  Banarescu, L., Bonial, C., Cai, S., Georgescu, M., Grif-
fitt, K., Hermjakob, U., ... & Schneider, N. (2013, Au-
answering pipeline for the genealogical domain; and 
gust). Abstract meaning representation for sembanking. 
(5)  a  fine-tuned  question-answering  BERT-based 
In Proceedings of the 7th linguistic annotation workshop 
model for the genealogical domain (Uncle-BERT).  and interoperability with discourse (pp. 178-186). 
Although  the  proposed  end-to-end  methodology  [6]  Barratt, Nick. "From memory to digital record." Records 
management journal (2009). 
was implemented and validated for the question an-
[7]  Barzilai, & Borwein, J. M. (1988). Two-Point Step Size 
swering task, it can be applied to other NLP down-
Gradient Methods. IMA Journal of Numerical Analysis. 
stream tasks in the genealogical domain, such as entity  https://doi.org/10.1093/imanum/8.1.141 
extraction, text classification, and summarization. Re- [8]  Bastos, A., Nadgeri, A., Singh, K., Mulang, I. O., She-
karpour,  S.,  Hoffart,  J.,  &  Kaul,  M.  (2021,  April). 
searchers can utilize the study's results to reduce the 
RECON: Relation Extraction using Knowledge Graph 
time, cost, and complexity and to improve accuracy in 
Context in a Graph Neural Network. In Proceedings of 
the genealogical domain NLP research.   the Web Conference 2021 (pp. 1673-1685). 

[9]  Beltagy, I., Peters, M. E., & Cohan, A. (2020). Long- [25]  Du, X., Shao, J., & Cardie, C. (2017, July). Learning to 
former: The long document transformer. arXiv pre-print  Ask: Neural Question Generation for Reading Compre-
arXiv:2004.05150.  hension. In Proceedings of the 55th Annual Meeting of 
[10]  Bezerianos, A., Dragicevic, P., Fekete, J. D., Bae, J., &  the Association for Computational Linguistics (Volume 
Watson, B. (2010). Geneaquilts: A system for exploring  1: Long Papers) (pp. 1342-1352). 
large genealogies. IEEE Transactions on Visualization  [26]  Duboue, P. A., & McKeown, K. (2003). Statistical ac-
and Computer Graphics, 16(6), 1073-1081.  quisition of content selection rules for natural language 
[11]  Białecki, A., Muir, R., Ingersoll, G., & Imagination, L.  generation. 
(2012, August). Apache lucene 4. In SIGIR 2012 work- [27]  Embley, D. W., Liddle, S. W., Eastmond, T. S., Lonsdale, 
shop on open source information retrieval (p. 17).  D. W., Price, J. P., & Woodfield, S. N. (2017). Concep-
[12]  Blackstone, William (1750). An essay on collateral con- tual modeling in accelerating information ingest into 
sanguinity,  its  limits,  extent, and duration,  in  Tracts  family tree. In Conceptual Modeling Perspectives (pp. 
chiefly relating to the antiquities and laws of England.  69-84). Springer, Cham. 
Oxford: Clarendon Press, (3rd edition: 1771).  [28]  Fader, A., Zettlemoyer, L., & Etzioni, O. (2013, August). 
[13]  Bordes, A., Chopra, S., & Weston, J. (2014). Question  Paraphrase-driven learning for open question answering. 
Answering with Subgraph Embeddings. EMNLP.  In Proceedings of the 51st Annual Meeting of the Asso-
[14]  Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J.,  ciation for Computational Linguistics (Volume 1: Long 
& Yakhnenko, O. (2013). Translating embeddings for  Papers) (pp. 1608-1618). 
modeling multi-relational data. In Advances in neural in- [29]  Fang, Y., Sun, S., Gan, Z., Pillai, R., Wang, S., & Liu, J. 
formation processing systems (pp. 2787-2795).  (2020,  November).  Hierarchical  Graph  Network  for 
[15]  Cai, L., Zhou, S., Yan, X., & Yuan, R. (2019). A stacked  Multi-hop Question Answering. In Proceedings of the 
BiLSTM neural network based on coattention mecha- 2020 Conference on Empirical Methods in Natural Lan-
nism for question answering. Computational intelligence  guage Processing (EMNLP) (pp. 8823-8838). 
and neuroscience, 2019.  [30]  Feldman, Y., & El-Yaniv, R. (2019, July). Multi-Hop 
[16]  Chen, Y., Wu, L., & Zaki, M. J. (2020). Toward sub- Paragraph  Retrieval  for  Open-Domain  Question  An-
graph guided knowledge graph question generation with  swering. In Proceedings of the 57th Annual Meeting of 
graph neural networks. arXiv preprint arXiv:2004.06015.  the  Association  for  Computational  Linguistics  (pp. 
[17]  Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau,  2296-2309). 
D., Bougares, F., Schwenk, H., & Bengio, Y. (2014).  [31]  Francisco,  M.P.,  &  Pérez,  A.B.  (2010).  Projecte  de 
Learning Phrase Representations using RNN Encoder- recerca  bàsica  o  aplicada  PAC3  —  Tercera  Prova 
Decoder  for  Statistical  Machine  Translation.  d'avaluació continuada. 
https://doi.org/10.3115/v1/D14-1179  [32]  Galkin, M., Trivedi, P., Maheshwari, G., Usbeck, R., & 
[18]  Clark, K., Luong, M. T., Le, Q. V., & Manning, C. D.  Lehmann, J. (2020, November). Message Passing for 
(2019, September). ELECTRA: Pretraining Text Encod- Hyper-Relational Knowledge Graphs. In Proceedings of 
ers as Discriminators Rather Than Generators. In Inter- the 2020 Conference on Empirical Methods in Natural 
national Conference on Learning Representations.  Language Processing (EMNLP) (pp. 7346-7359). 
[19]  Cui, W., Xiao, Y., Wang, H., Song, Y., Hwang, S., &  [33]  Gardent, C., Shimorina, A., Narayan, S., & Perez-Beltra-
Wang, W. (2017). KBQA: Learning question answering  chini, L. (2017, July). Creating training corpora for nlg 
over QA corpora and knowledge bases. Proceedings of  micro-planning. In 55th annual meeting of the Associa-
the VLDB Endowment, 10(5), 565.  tion for Computational Linguistics (ACL). 
[20]  Dai, H. J., Lee, Y. Q., Nekkantti, C., & Jonnagaddala, J.  [34]  Garg, S., Vu, T., & Moschitti, A. (2020, April). Tanda: 
(2020).  Family  History  Information  Extraction  with  Transfer and adapt pre-trained transformer models for 
Neural  Attention  and  an  Enhanced  Relation-Side  answer sentence selection. In Proceedings of the AAAI 
Scheme: Algorithm Development and Validation. JMIR  Conference on Artificial Intelligence (Vol. 34, No. 05, 
Medical Informatics, 8(12), e21750.  pp. 7780-7788). 
[21]  Deemter, K. V., Theune, M., & Krahmer, E. (2005). Real  [35]  Gellatly, C. (2009). Trends in population sex ratios may 
versus template-based natural language generation: A  be explained by changes in the frequencies of polymor-
false opposition?. Computational linguistics, 31(1), 15- phic alleles of a sex ratio gene. Evolutionary biology, 
24.  36(2), 190-200. 
[22]  Devlin, J., Chang, M. W., Lee, K., & Toutanova, K.  [36]  Gellatly, C. (2015). Reconstructing historical popula-
(2019, June). BERT: Pre-training of Deep Bidirectional  tions from genealogical data files. In Population recon-
Transformers for Language Understanding. In Proceed- struction (pp. 111-128). Springer, Cham. 
ings of the 2019 Conference of the North American  [37]  Goyal, P., & Ferrara, E. (2018). Graph embedding tech-
Chapter of the Association for Computational Linguis- niques,  applications,  and  performance:  A  survey. 
tics: Human Language Technologies, Volume 1 (Long  Knowledge-Based Systems, 151, 78-94. 
and Short Papers) (pp. 4171-4186).  [38]  Graves, A., Wayne, G., Reynolds, M., Harley, T., Dani-
[23]  Dijkstra, E. W. (1959). A note on two problems in con- helka, I., Grabska-Barwińska, A., ... & Badia, A. P. 
nexion with graphs. Numerische mathematik, 1(1), 269- (2016). Hybrid computing using a neural network with 
271.  dynamic external memory. Nature, 538(7626), 471-476. 
[24]  Dong, L., Wei, F., Zhou, M., & Xu, K. (2015, July).  [39]  Grover, A., & Leskovec, J. (2016, August). node2vec: 
Question answering over freebase with multi-column  Scalable feature learning for networks. In Proceedings of 
convolutional neural networks. In Proceedings of the  the 22nd ACM SIGKDD international conference on 
53rd Annual Meeting of the Association for Computa- Knowledge discovery and data mining (pp. 855-864). 
tional Linguistics and the 7th International Joint Confer- [40]  Han, J., Cheng, B., & Wang, X. (2020, November). 
ence on Natural Language Processing (Volume 1: Long  Open Domain Question Answering based on Text En-
Papers) (pp. 260-269).  hanced Knowledge Graph with Hyperedge Infusion. In 

Proceedings of the 2020 Conference on Empirical Meth- [56]  Kim, N. W., Card, S. K., & Heer, J. (2010, May). Trac-
ods  in  Natural  Language  Processing:  Findings  (pp.  ing genealogical data with timenets. In Proceedings of 
1475-1481).  the International Conference on Advanced Visual Inter-
[41]  Harrah, D. (2002). The logic of questions. In Handbook  faces (pp. 241-248). 
of philosophical logic (pp. 1-60). Springer, Dordrecht.  [57]  Kim, Y. “Convolutional Neural Networks for Sentence 
[42]  Harviainen, J. T., & Björk, B. C. (2018). Genealogy,  Classification.” EMNLP (2014). 
GEDCOM, and popularity  implications.  Informaatio- [58]  Kitaev, N., Kaiser, L., & Levskaya, A. (2019, Septem-
tutkimus.  ber). Reformer: The Efficient Transformer. In Interna-
[43]  He, B., Zhou, D., Xiao, J., Jiang, X., Liu, Q., Yuan, N.  tional Conference on Learning Representations. 
J., & Xu, T. (2020, November). Integrating Graph Con- [59]  Klein, D., & Manning, C. D. (2002). Fast exact inference 
textualized Knowledge into Pre-trained Language Mod- with a factored model for natural language parsing. Ad-
els. In Proceedings of the 2020 Conference on Empirical  vances in neural information processing systems, 15, 3-
Methods in Natural Language Processing: Findings (pp.  10. 
2281-2290).  [60]  Koch, I., Freitas, N., Ribeiro, C., Lopes, C. T., & da Silva, 
[44]  Heilman, M., & Smith, N. A. (2010, June). Good ques- J. R. (2019, September). Knowledge Graph Implemen-
tion! statistical ranking for question generation. In Hu- tation of Archival Descriptions Through CIDOC-CRM. 
man Language Technologies: The 2010 Annual Confer- In International Conference on Theory and Practice of 
ence of the North American Chapter of the Association  Digital Libraries (pp. 99-106). Springer, Cham. 
for Computational Linguistics (pp. 609-617).  [61]  Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., 
[45]  Herskovitz, A. (2012). A Suggested Taxonomy of Gene- Parikh, A., Alberti, C., ... & Toutanova, K. (2019). Nat-
alogy as a Multidisciplinary Academic Research Field.  ural questions: a benchmark for question answering re-
Journal  of  Multidisciplinary  Research  (1947-2900),  search. Transactions of the Association for Computa-
4(3).  tional Linguistics, 7, 453-466. 
[46]  Hey, D. (Ed.). (2010). The Oxford companion to family  [62]  Lai, T., Tran, Q. H., Bui, T., & Kihara, D. (2019, No-
and local history. OUP Oxford.  vember). A Gated Self-attention Memory Network for 
[47]  Hirschman, L., & Gaizauskas, R. (2001). Natural lan- Answer Selection. In Proceedings of the 2019 Confer-
guage question answering: the view from here. natural  ence on Empirical Methods in Natural Language Pro-
language engineering, 7(4), 275.  cessing and the 9th International Joint Conference on 
[48]  Hochreiter, S., & Schmidhuber, J. J. (1997). Long short- Natural Language Processing (EMNLP-IJCNLP) (pp. 
term  memory.  Neural  Computation,  9(8),  1–32.  5953-5959). 
https://doi.org/10.1162/neco.1997.9.8.1735  [63]  Lebret, R., Grangier, D., & Auli, M. (2016, November). 
[49]  Hosking, T., & Riedel, S. (2019, June). Evaluating Re- Neural Text Generation from Structured Data with Ap-
wards for Question Generation Models. In Proceedings  plication to the Biography Domain. In Proceedings of 
of the 2019 Conference of the North American Chapter  the 2016 Conference on Empirical Methods in Natural 
of the Association for Computational Linguistics: Hu- Language Processing (pp. 1203-1213). 
man Language Technologies, Volume 1 (Long and Short  [64]  Leskinen, P., & Hyvönen, E. (2020). Reconciling and 
Papers) (pp. 2278-2283).  Using Historical Person Registers as Linked Open Data 
[50]  Indurthi, S. R., Raghu, D., Khapra, M. M., & Joshi, S.  in the AcademySampo Knowledge Graph. 
(2017, April). Generating natural language question-an- [65]  Levy, R., & Andrew, G. (2006, May). Tregex and Tsur-
swer pairs from a knowledge graph using a RNN based  geon:  tools  for  querying  and  manipulating  tree  data 
question generation model. In Proceedings of the 15th  structures. In LREC (pp. 2231-2234). 
Conference of the European Chapter of the Association  [66]  Li,  W.,  Peng,  R.,  Wang,  Y.,  &  Yan,  Z.  (2020). 
for Computational Linguistics: Volume 1, Long Papers  Knowledge  graph  based  natural  language  generation 
(pp. 376-385).  with adapted pointer-generator networks. Neurocompu-
[51]  Irshad, H., Montaser-Kouhsari, L., Waltz, G., Bucur, O.,  ting, 382, 174-187. 
Nowak,  J.  A.,  Dong,  F.,  ...  &  Beck,  A.  H.  (2014).  [67]  Lin, C. Y. (2008, September). Automatic question gen-
Crowdsourcing image annotation for nucleus detection  eration from queries. In Workshop on the question gen-
and segmentation in computational pathology: evaluat- eration shared task (pp. 156-164). 
ing experts, automated methods, and the crowd. In Pa- [68]  Liu, T., Wang, K., Sha, L., Chang, B., & Sui, Z. (2018, 
cific symposium on biocomputing Co-chairs (pp. 294- April).  Table-to-Text  Generation  by  Structure-Aware 
305).  Seq2seq Learning. In Proceedings of the AAAI Confer-
[52]  Ji, S., Pan, S., Cambria, E., Marttinen, P., & Yu, P. S.  ence on Artificial Intelligence (Vol. 32, No. 1). 
(2021). A Survey on Knowledge Graphs: Representation,  [69]  Liu, X., Zhu, Y., & Ji, S. (2020, August). Web Log Anal-
Acquisition, and Applications. IEEE Transactions on  ysis in Genealogy System. In 2020 IEEE International 
Neural Networks and Learning Systems.  Conference on Knowledge Graph (ICKG) (pp. 536-543). 
[53]  Johnson, J., Douze, M., & Jégou, H. (2019). Billion- IEEE. 
scale similarity search with gpus. IEEE Transactions on  [70]  Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... 
Big Data.  & Stoyanov, V. (2019). Roberta: A robustly optimized 
[54]  Jones, K. S., & Van Rijsbergen, C. J. (1976). Infor- bert  pretraining  approach.  arXiv  preprint 
mation retrieval test collections. Journal of documenta- arXiv:1907.11692. 
tion.  [71]  Luhn, H. P. (1957). A statistical approach to mechanized 
[55]  Kim,  J.,  &  Mooney,  R.  (2010,  August).  Generative  encoding and searching of literary information. IBM 
alignment and semantic parsing for learning from am- Journal of research and development, 1(4), 309-317. 
biguous supervision. In Coling 2010: Posters (pp. 543- [72]  Lukovnikov, D., Fischer, A., Lehmann, J., & Auer, S. 
551).  (2017, April). Neural network-based question answering 
over knowledge graphs on word and character level. In 

Proceedings  of  the  26th  international  conference  on  Association for Computational Linguistics: Human Lan-
World Wide Web (pp. 1211-1220).  guage Technologies, Volume 1 (Long Papers) (pp. 2227-
[73]  Lussier, A. A., & Keinan, A. (2018). Crowdsourced ge- 2237). 
nealogies and genomes. Science, 360(6385), 153-154.  [88]  Radford, A., Narasimhan, K., Salimans, T., & Sutskever, 
[74]  Ma, X., Zhu, Q., Zhou, Y., Li, X., & Wu, D. (2020).  I. (2018). Improving language understanding by genera-
Asking Complex Questions with Multi-hop Answer-fo- tive pre-training. 
cused Reasoning. arXiv preprint arXiv:2009.07402.  [89]  Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & 
[75]  McGuffin, M. J., & Balakrishnan, R. (2005, October).  Sutskever, I. (2019). Language models are unsupervised 
Interactive visualization of genealogical graphs. In IEEE  multitask learners. OpenAI blog, 1(8), 9. 
Symposium on Information Visualization, 2005. INFO- [90]  Rajpurkar, P., Jia, R., & Liang, P. (2018, July). Know 
VIS 2005. (pp. 16-23). IEEE.  What You Don’t Know: Unanswerable Questions for 
[76]  McRoy, S. W., Channarukul, S., & Ali, S. S. (2003). An  SQuAD. In Proceedings of the 56th Annual Meeting of 
augmented template-based approach to text realization.  the Association for Computational Linguistics (Volume 
Natural Language Engineering, 9(4), 381.  2: Short Papers) (pp. 784-789). 
[77]  Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., &  [91]  Rajpurkar, P., Zhang, J., Lopyrev, K., & Liang, P. (2016, 
Dean, J. (2013). Distributed representations of words  November). SQuAD: 100,000+ Questions for Machine 
and phrases and their compositionality. In Advances in  Comprehension of Text. In Proceedings of the 2016 
neural information processing systems (pp. 3111-3119).  Conference on Empirical Methods in Natural Language 
[78]  Mitra, A., & Baral, C. (2016, February). Addressing a  Processing (pp. 2383-2392). 
Question Answering Challenge by Combining Statisti- [92]  Reimers, N., & Gurevych, I. (2019, November). Sen-
cal Methods with Inductive Rule Learning and Reason- tence-BERT:  Sentence  Embeddings  using  Siamese 
ing. In AAAI (pp. 2779-2785).  BERT-Networks. In Proceedings of the 2019 Confer-
[79]  Moussallem, D., Gnaneshwar, D., Ferreira, T. C., &  ence on Empirical Methods in Natural Language Pro-
Ngomo, A. C. N. (2020, November). NABU–Multilin- cessing and the 9th International Joint Conference on 
gual Graph-Based Neural RDF Verbalizer. In Interna- Natural Language Processing (EMNLP-IJCNLP) (pp. 
tional  Semantic  Web  Conference  (pp.  420-437).  3982-3992). 
Springer, Cham.  [93]  Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). 
[80]  Mulang, I. O., Singh, K., & Orlandi, F. (2017, Septem- DistilBERT, a distilled version of BERT: smaller, faster, 
ber). Matching natural language relations to knowledge  cheaper and lighter. arXiv preprint arXiv:1910.01108. 
graph properties for question answering. In Proceedings  [94]  Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., 
of the 13th International Conference on Semantic Sys- & Monfardini, G. (2008). The graph neural network 
tems (pp. 89-96).  model. IEEE Transactions on Neural Networks, 20(1), 
[81]  Mulang, I.O., Singh, K., Prabhu, C., Nadgeri, A., Hoffart,  61-80. 
J.,  &  Lehmann,  J.  (2020).  Evaluating  the  Impact of  [95]  Schaffer, S., Gustke, O., Oldemeier, J., & Reithinger, N. 
Knowledge Graph Context on Entity Disambiguation  (2018).  Towards  chatbots  in  the  museum.  In  mo-
Models.  Proceedings of  the  29th  ACM  International  bileCH@ Mobile HCI. 
Conference on Information & Knowledge Management.  [96]  Schuster, M., & Paliwal, K. K. (1997). Bidirectional re-
[82]  Nadgeri,  A.,  Bastos,  A.,  Singh,  K.,  Mulang’,  I.  O.,  current neural networks. IEEE Transactions on Signal 
Hoffart, J., Shekarpour, S., & Saraswat, V. (2021, Au- Processing,  45(11),  2673–2681. 
gustus). KGPool: Dynamic Knowledge Graph Context  https://doi.org/10.1109/78.650093 
Selection for Relation Extraction. Findings of the Asso- [97]  Seo, M., Kembhavi, A., Farhadi, A., & Hajishirzi, H. 
ciation  for  Computational  Linguistics:  ACL-IJCNLP  (2016). Bidirectional attention flow for machine com-
2021, 535–548. doi:10.18653/v1/2021.findings-acl.48  prehension. arXiv preprint arXiv:1611.01603. 
[83]  Navigli, R., & Ponzetto, S. P. (2012). BabelNet: The au- [98]  Seyler, D., Yahya, M., & Berberich, K. (2015, May). 
tomatic construction, evaluation and application of a  Generating quiz questions from knowledge graphs. In 
wide-coverage multilingual semantic network. Artificial  Proceedings of the 24th  International  Conference on 
Intelligence, 193, 217-250.  World Wide Web (pp. 113-114). 
[84]  Otterstrom, S. M., & Bunker, B. E. (2013). Genealogy,  [99]  Sha, L., Mou, L., Liu, T., Poupart, P., Li, S., Chang, B., 
migration, and the intertwined geographies of personal  & Sui, Z. (2018, April). Order-Planning Neural Text 
pasts. Annals of the Association of American Geogra- Generation From Structured Data. In Proceedings of the 
phers, 103(3), 544-569.  AAAI Conference on Artificial Intelligence (Vol. 32, No. 
[85]  Pellissier Tanon, T., Vrandečić, D., Schaffert, S., Steiner,  1). 
T., & Pintscher, L. (2016, April). From freebase to wik- [100] Shandler, J. (2020). The Savior and the Survivor: Virtual 
idata: The great migration. In Proceedings of the 25th  Afterlives in New Media. Jewish Film & New Media, 
international conference on world wide web (pp. 1419- 8(1), 23-47. 
1428).  [101] Shao, L., Duan, Y., Sun, X., Gao, H., Zhu, D., & Miao, 
[86]  Pennington, J., Socher, R., & Manning, C. D. (2014, Oc- W. (2017, July). Answering Who/When, What, How, 
tober). Glove: Global vectors for word representation. In  Why  through  Constructing  Data  Graph,  Information 
Proceedings of the 2014 conference on empirical meth- Graph, Knowledge Graph and Wisdom Graph. In SEKE 
ods in natural language processing (EMNLP) (pp. 1532- (pp. 1-6). 
1543).  [102] Shao, T., Guo, Y., Chen, H., & Hao, Z. (2019). Trans-
[87]  Peters, M., Neumann, M., Iyyer, M., Gardner, M., Clark,  former-based neural network  for  answer  selection  in 
C., Lee, K., & Zettlemoyer, L. (2018, June). Deep Con- question answering. IEEE Access, 7, 26146-26156. 
textualized Word Representations. In Proceedings of the  [103] Sheng,  S.,  Zhou,  P.,  &  Wu,  X.  (2019,  November). 
2018 Conference of the North American Chapter of the  CEPV:  A Tree  Structure  Information Extraction and 
Visualization Tool for Big Knowledge Graph. In 2019 

IEEE  International  Conference  on  Big  Knowledge  [120] Wang, Z., Mi, H., Hamza, W., & Florian, R. (2016). 
(ICBK) (pp. 221-228). IEEE.  Multi-perspective context matching for machine com-
[104] Singhal,  A.  (2001).  Modern information  retrieval:  A  prehension. arXiv preprint arXiv:1612.04211. 
brief overview. IEEE Data Eng. Bull., 24(4), 35-43.  [121] Warby, S. C., Wendt, S. L., Welinder, P., Munk, E. G., 
[105] Smolenyak, M., & Turner, A. (2004). Trace your roots  Carrillo, O., Sorensen, H. B., ... & Mignot, E. (2014). 
with DNA: Using genetic tests to explore your family  Sleep-spindle detection: crowdsourcing and evaluating 
tree. Rodale.  performance  of  experts,  non-experts  and  automated 
[106] Song, L., Zhang, Y., Wang, Z., & Gildea, D. (2018, July).  methods. Nature methods, 11(4), 385. 
A Graph-to-Sequence Model for AMR-to-Text Genera- [122] Werbos, P. (1974). Beyond Regression: New Tools for 
tion. In Proceedings of the 56th Annual Meeting of the  Prediction and Analysis in the Behavioral Sciences. Har-
Association for Computational Linguistics (Volume 1:  vard University. 
Long Papers) (pp. 1616-1626).  [123] White, M., & Caldwell, T. (1998). EXEMPLARS: A 
[107] Speer, R., Chin, J., & Havasi, C. (2017, February). Con- practical, extensible framework for dynamic text gener-
ceptNet 5.5: An Open Multilingual Graph of General  ation. In Natural Language Generation. 
Knowledge. In Proceedings of the AAAI Conference on  [124] Williams, R. R., Hunt, S. C., Heiss, G., Province, M. A., 
Artificial Intelligence (Vol. 31, No. 1).  Bensen, J. T., Higgins, M., ... & Hopkins, P. N. (2001). 
[108] Stenzhorn, H. (2002). Xtragena natural language gener- Usefulness of cardiovascular family history data for pop-
ation system using xml-and java-technologies. In COL- ulation-based preventive medicine and medical research 
ING-02:  The  2nd  Workshop  on  NLP  and  XML  (the Health Family Tree Study and the NHLBI Family 
(NLPXML-2002).  Heart Study). The American journal of cardiology, 87(2), 
[109] Suissa,  O.,  Elmalech,  A.,  &  Zhitomirsky-Geffet,  M.  129-135. 
(2019). Toward the optimized crowdsourcing strategy  [125] Wolf, T., Chaumond, J., Debut, L., Sanh, V., Delangue, 
for OCR post-correction. Aslib Journal of Information  C., Moi, A., ... & Rush, A. M. (2020, October). Trans-
Management.  Vol.  72  No.  2,  pp.  179-197.  formers: State-of-the-art natural language processing. In 
https://doi.org/10.1108/AJIM-07-2019-0189.  Proceedings of the 2020 Conference on Empirical Meth-
[110] Suissa, O., Elmalech, A., & Zhitomirsky‐Geffet, M.  ods in Natural Language Processing: System Demon-
(2021). Text analysis using deep neural networks in dig- strations (pp. 38-45). 
ital humanities and information science. Journal of the  [126] Wolinsky, H. (2006). Genetic genealogy goes global: 
Association for Information Science and Technology.  Although useful in investigating ancestry, the applica-
[111] Sumner, J. L., Farris, E. M., & Holman, M. R. (2020).  tion of genetics to traditional genealogy could be abused. 
Crowdsourcing reliable local data. Political Analysis,  EMBO reports, 7(11), 1072-1074. 
28(2), 244-262.  [127] Xiong, C., Zhong, V., & Socher, R. (2016). Dynamic 
[112] Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence  coattention networks for question answering. arXiv pre-
to Sequence Learning with Neural Networks. Interna- print arXiv:1611.01604. 
tional Journal of Computer Vision.  [128] Xu, P., & Barbosa, D. (2019, Junie). Connecting Lan-
[113] Tuominen, J., Hyvönen, E., & Leskinen, P. (2017). Bio  guage and Knowledge with Heterogeneous Representa-
CRM:  A  Data  Model  for  Representing  Biographical  tions for Neural Relation Extraction. Proceedings of the 
Data for Prosopographical Research. BD.   2019 Conference of the North American Chapter of the 
[114] van den Berg, R., Kipf, T. N., & Welling, M. (2017).  Association for Computational Linguistics: Human Lan-
Graph Convolutional Matrix Completion. stat, 1050, 25.  guage Technologies, Volume 1 (Long and Short Papers), 
[115] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,  3201–3206. doi:10.18653/v1/N19-1323 
Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). At- [129] Yamada, I., Washio, K., Shindo, H., & Matsumoto, Y. 
tention is all you need. In Advances in neural infor- (2019).  Global  entity  disambiguation  with pretrained 
mation processing systems (pp. 5998-6008).  contextualized embeddings of words and entities. arXiv 
[116] Wang, S., & Jiang, J. (2016). Machine comprehension  preprint arXiv:1909.00426. 
using match-LSTM and answer pointer.(2017). In ICLR  [130] Yang, W., Xie, Y., Lin, A., Li, X., Tan, L., Xiong, K., ... 
2017: International Conference on Learning Representa- & Lin, J. (2019, June). End-to-End Open-Domain Ques-
tions, Toulon, France, April 24-26: Proceedings (pp. 1- tion Answering with BERTserini. In Proceedings of the 
15).  2019 Conference of the North American Chapter of the 
[117] Wang, S., Wei, Z., Fan, Z., Huang, Z., Sun, W., Zhang,  Association for Computational Linguistics (Demonstra-
Q., & Huang, X. J. (2020, November). PathQG: Neural  tions) (pp. 72-77). 
Question Generation from Facts. In Proceedings of the  [131] Yao, L., Mao, C., & Luo, Y. (2019, July). Graph convo-
2020 Conference on Empirical Methods in Natural Lan- lutional networks for text classification. In Proceedings 
guage Processing (EMNLP) (pp. 9066-9075).  of the AAAI Conference on Artificial Intelligence (Vol. 
[118] Wang, S., Yu, M., Guo, X., Wang, Z., Klinger, T., Zhang,  33, No. 01, pp. 7370-7377). 
W., Chang, S., Tesauro, G., Zhou, B., & Jiang, J. (2018).  [132] Yonghui, W., Schuster, M., Chen, Z., Le, Q. V., Norouzi, 
R3: Reinforced Ranker-Reader for Open-Domain Ques- M., Macherey, W., ... & Macherey, K. (2016). Bridging 
tion Answering. AAAI.  the gap between human and machine translation. arXiv 
[119] Wang, W., Yang, N., Wei, F., Chang, B., & Zhou, M.  preprint arXiv:1609.08144. 
(2017, July). Gated self-matching networks for reading  [133] Zhang, Z., Wu, Y., Zhou, J., Duan, S., Zhao, H., & Wang, 
comprehension and question answering. In Proceedings  R. (2020). SG-Net: Syntax-Guided Machine Reading 
of the 55th Annual Meeting of the Association for Com- Comprehension. In AAAI (pp. 9636-9643). 
putational Linguistics (Volume 1: Long Papers) (pp.  [134] Zhao, C., Xiong, C., Qian, X., & Boyd-Graber, J. (2020, 
189-198).  April). Complex Factoid Question Answering with a 
Free-Text  Knowledge  Graph.  In  Proceedings  of  The 
Web Conference 2020 (pp. 1205-1216). 

[135] Zhao, Y., Ni, X., Ding, Y., & Ke, Q. (2018). Paragraph- survey on open-domain question answering. arXiv pre-
level neural question generation with maxout pointer and  print arXiv:2101.00774. 
gated  self-attention  networks.  In  Proceedings  of  the  [139] Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Ur-
2018 Conference on Empirical Methods in Natural Lan- tasun, R., Torralba, A., & Fidler, S. (2015). Aligning 
guage Processing (pp. 3901-3910).  books and movies: Towards story-like visual explana-
[136] Zheng, W., Yu, J. X., Zou, L., & Cheng, H. (2018).  tions by watching movies and reading books. In Pro-
Question answering over knowledge graphs: question  ceedings of the IEEE international conference on com-
understanding via template decomposition. Proceedings  puter vision (pp. 19-27). 
of the VLDB Endowment, 11(11), 1373-1386.  [140] Zou, L., Huang, R., Wang, H., Yu, J. X., He, W., & Zhao, 
[137] Zhong, W., Tang, D., Duan, N., Zhou, M., Wang, J., &  D. (2014, June). Natural language question answering 
Yin, J. (2019, October). Improving question answering  over RDF: a graph data driven approach. In Proceedings 
by commonsense-based pre-training. In CCF Interna- of the 2014 ACM SIGMOD international conference on 
tional Conference on Natural Language Processing and  Management of data (pp. 313-324). 
Chinese Computing (pp. 16-28). Springer, Cham.  [141] Ganhotra, J., & Joshi, S. (2021). Does Dialog Length 
[138] Zhu, F., Lei, W., Wang, C., Zheng, J., Poria, S., & Chua,  matter for Next Response Selection task? An Empirical 
T. S. (2021). Retrieving and reading: A comprehensive  Study. arXiv preprint arXiv:2101.09647.
 