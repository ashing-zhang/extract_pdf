Picture What you Read
Ignazio Gallo1, Shah Nawaz1, Alessandro Calefati1, Riccardo La Grassa1, and Nicola Landro1
1Department of Theoretical and Applied Science, University of Insubria, Varese, Italy
{ignazio.gallo,snawaz,a.calefati,rlagrassa}@uninsubria.it
Abstract—Visualization refers to our ability to create an image
in our head based on the text we read or the words we hear.
It is one of the many skills that makes reading comprehension
possible. Convolutional Neural Networks (CNN) are an excellent
tool for recognizing and classifying text documents. In addition,
it can generate images conditioned on natural language. In this
work, we utilize CNNs capabilities to generate realistic images
representative of the text illustrating the semantic concept. We
conducted various experiments to highlight the capacity of the
9102 peS 9  ]VC.sc[  1v36650.9091:viXra
proposed model to generate representative images of the text
descriptions used as input to the proposed model.
I. INTRODUCTION
Recent years have seen a surge in multimodal data con-
taining various media types. Typically, users combine text,
image, audio or video to sell a product over an e-commence
Fig. 1: A Neural Model reading natural language (“Heavy
platform or express views on social media. The combination
Duty All Purpose Hammer - Forged Carbon Steel Head”) can
of these media types has been extensively studied to solve
generates a representative image (“Hammer”).
various tasks including classification [1], [2], [3], cross-modal
retrieval [4] semantic relatedness [5], [6], image captioning [7],
[8], multimodal named entity recognition [9], [10] and Visual
– We propose a new loss function to transform a text
Question Answering [11], [12]. In addition, multimodal data
description into a representative image;
fueled an increased interest in generating images conditioned
– The proposed model generates images conditioned on
on natural language [13], [14]. In recent years, generative
technical e-commerce specifications. Moreover, it gener-
models based on conditional Generative Adversarial Network
ates images never seen before;
(GAN) have remarkably improved text to image generation
– An end-to-end convolutional model capable of classifying
task [15], [16]. Furthermore, generative models based on
the text and at the same time generating a representative
Variational Autoencoders are employed to generate images
image of the text. The generated image can be used as text
conditioned on natural language [17], [18]. Generally, image
encoding or as a realistic image representing the object
generation from natural language is divided into phases: the
described in the input document;
first phase learns the distribution from which the images are to
– We propose a model that can also be used to transform
be generated while the second phase learns a generator, which
a multimodal dataset into a single dataset of images.
in turn produces the image conditioned on a vector from this
distribution.
II. RELATED WORK
In this work, we are interested in transforming natural
language in the form of technical e-commerce product specifi- Recently, image generation conditioned on natural language
cations directly into image pixels. For example, image pixels has drawn a lot of attention from the research community.
may be generated from the text description such as “Heavy Various approaches have been proposed based on Variational
Duty All Purpose Hammer - Forged Carbon Steel Head” Autoencoders [17], [18], Auto-regressive models [19] and op-
as shown in Fig. 1. We assume we are given technical timization techniques [20]. Similarly, GANs based approaches
specifications of a set of images available on e-commence have noticeably improved image synthesis conditioned on
platforms, and train the generator block, available inside our natural language. These approaches consist of a generator and
model, from the pixel distribution. We propose to use an ‘up- a discriminator that compete in a two player minimax game:
convolutional’ generative block for this task and show that it the discriminator tries to distinguish real data from generated
is capable of generating realistic e-commerce images. Fig. 3 images, and the generator tries to trick the discriminator.
shows some generated images conditioned on technical prod- In the proposed model, the generator and discriminator are
uct specification along with the original images. Following are part of the same model and are linked by a single loss
the main contributions of our work: function, in order to generate images within the classification

Fig. 2: A schematic representation of the proposed model. The model extracts features from the text document using
an embedding layer and three different convolutive filters. Through different deconvolutive layers the textual features are
transformed into an image representative of the text. Finally, through convolutive layers, the image and the encoded text are
classified.
process. In the following paragraph, we reviewed couple of the Neural networks typically produce class probabilities by
ground breaking approaches on image generation conditioned using a “softmax” output layer that converts the logit, a ,
i
on natural language. computed for each class into a probability p , by comparing a
i i
Reed et al. [13] proposed to learn both generator and with the other logits. Softmax function takes an n-dimensional
discriminator conditioned on captions. Zhu et al. [16] proposed vector of real numbers and transforms it into a vector of real
a generative method to generate synthesized visual features number in range [0,1] which add upto 1.
using the noisy text descriptions about an unseen class. Xu et
exp(a )
i
al. [21] proposed attentional generative network to synthesize p = (1)
i (cid:80)
exp(a )
fine-grained details at different subregions of the image by j j
paying attentions to the relevant words in the natural language
“Cross entropy” indicates the distance between what the model
description. Zhang et al. [22] decomposed text to image
believes the output distribution should be (y ), and what the
i
generation in two stages: the first stage GAN sketches the
original distribution really is. It is defined as
basic shape and color of the object condition on the natural
(cid:88)
language, resulting in low resolution image. While the second L (y,p) = − y log(p ) (2)
0 i i
stage GAN takes first stage results and natural language to i
generate high-resolution images with photo-realistic details.
Cross entropy measure is a widely used alternative of squared
Furthermore, various approaches have exploited the capabil-
error. If we want to minimize the pixel-by-pixel distance
ity of ‘up-convolutional’ network to generate realistic images.
between the input image I , associated with the text document
i
Dosovitskiy et al. [23] trained a deconvolutional network with
and a CNN’s features layer F that has the same dimensions
i
several layers of convolution and upsampling to generate 3D
as the image I , then we can apply the following formula
i
chair renderings given object style, viewpoint and color. In
(cid:88)
this work we are interested in generating new images through L (F,I) = (F − I )2 (3)
1 i i
up-sampling but we limit this generative process to a medium i
resolution.
or the following mean version
III. MODEL DESCRIPTION 1 (cid:88)
Lˆ (F,I) = (F − I )2 (4)
1 i i
Our goal is to train a neural network to generate accurate e- N
i
commerce images from a low-level and noisy text description.
F is the output of the last transposed convolutions – also
We develop an effective loss function to transform a text i
called fractionally strided convolutions – used to upsampling
document into a representative image and at the same time
the text features to attain a feature layer having the same size
exploits the information content of the image and the text, to
of the image I.
solve a classification problem.
The final loss function we used in this work is the following
Formally, we assume that we are given a dataset of examples
D = {t ,...,t } with targets O = {(y ,I ),...,(y ,I )}.
1 N 1 1 N N L = L (y,p) + λL (F,I) (5)
0 1
The inputs t are text descriptions describing the objects
i
showed in the images I . The targets are tuples consisting but we also performed experiments replacing the L with the
i 1
ˆ
of two elements: the class label y in one-hot encoding and a L .
i 1
ˆ ˆ
RGB image I . L = L (y,p) + λL (F,I) (6)
i 0 1

Generated Original Input Text In this last case the contribution of the lambda parameter has
a different effect with the same λ values, this because the
draper expert knipex 27723
ˆ
interval of variability of L and L are very different. For
side trimmer for electronics 1 1
example, using the Eq. 6 as loss function, we need much larger
for cutting head satin with-
ˆ
λ values to take advantage of the contribution of L and obtain
out facet 115 mm 1
in F a realistic image, representative of the object described
in the text t .
spax universal screw half i
round head t star plus 4 cut The λ parameter is important to balance the contribution
partial shiny thread galva- of L 0 against L 1. Setting λ = 0 we minimize only L 0 and
nized with a2j galvanization therefore the feature layer F, representing the input text, will
be very different from the image I. In Fig. 5 we have a
screws mustad panel vitals graphical representation of the image learned by minimizing
bronzed 3x20 mm. conf. Eq. 5, using various λ values and it is important to note that
500 pcs universal counter- when λ = 0 the model does not generate realistic images.
sunk flat head screw suit- Learning proceeds by minimizing the loss function L via
able for screwing ... Adam optimizer [24]. Since we are using the combination of
sicutool padlocks 2090p 50 two loss functions to modify the weights of the entire neural
width body 50 mm network, we know that within the image F that we generate,
it contains text representations. In fact, in many cases it may
happen that the F image cannot be interpreted as one of the
objects belonging to a particular class but the classification is
sicutool copper hammers correct.
2718 800 total weight 800 We experimented with a network for generating images of
copper hammers size 100 × 100. The structure of the generative network is
shown in Fig. 2. Conceptually, the network we propose can be
seen as a convolutive classification model for text documents,
sicutool nippers for elec- that incorporates a generative network. The starting point of
tronics and fine mechanics the proposed model is the classification model of sentences
557gf lenght total mm 125 proposed by Kim [25] to transform a text document into a
wire cutters for electronics features vector. This is followed by a set of 4 deconvolutive
and fine mechanics layers that transform textual features into an RGB image
sicutool circular saws with that we can then generate. Finally, we used a sequence of
teeth shown in hartmetall 4 convolutive layers that transform the generated image into
4840g 150b type null 150b features for the classification problem.
mm 150 thickness mm 2 8
hole mm 16 teeth nr 20
A. Text features
valex wrench 20 x 22mm
The input to our model are sequences of words
inclined forks of 15 body in
[w ,...,w ] from each input document t, where each word
chrome vanadium steel with i |t|
is drawn from a vocabulary V . Words are represented by
polished finish. size 20x22
distributional vectors w ∈ R1×d looked up in a word embed-
mm length 235 mm
dings matrix W ∈ Rd×|V|. This matrix is formed by simply
syrom adhesive tape in tex-
concatenating embeddings of all words in V .
tile tes special 38 mm x 2 7
m black tightly woven plas- For each input text t, we build a matrix S ∈ Rd×|t|,
ticized tape to repair bind- where each row i represents a word embedding w i at the
ing edges etc. corresponding position i in the document t. To capture and
compose features of individual words in a given text from low-
oem 20 pcs sticker number
level word embeddings into higher level semantic concepts,
6 mm 50 black pvc sticker
the neural network applies a series of transformations to the
number 6
input matrix S using convolution, non-linearity and pooling
operations. The convolution operation between S and a filter
F ∈ Rd×m of height m results in a vector c ∈ R|t|+m1. In
Fig. 3: Some examples of generated images from test dataset,
our model we used three groups of 128 different kernels in
associated with a correct classification. In some cases, the
parallel, having dimensions d×3, d×4 and d×5. In this way
generated images are slightly different from the respective
we obtained three feature maps c , having different lengths.
i
expected image but the object shown is very similar.
Note that the convolution filter is of the same dimensionality

d = 128 as the input sentence matrix, so this is like a 1-
D convolution operation. To allow the network to learn an Bestλparameterestimatation
0.95
appropriate threshold, we also added a bias vector b ∈ Rn for
each feature map c i. 0.90 88.7% 89.1% 88.7% 88.4% 88.1% 88.2%
87.5% 87.1%
Each convolutional layer is followed by the Rectified Linear 86.3% 86.3%
0.85
Unit (ReLU) non-linear activation function, applied element-
wise. ReLU speeds up the training process [26], defined as ycaruccA 0.80
max(0,x) to ensure that feature maps are always positive. To 75.7%
0.75
capture the most important feature – one with the highest value
– for each feature map, we apply a max-overtime pooling 0.70
operation [27] over each feature map. In this way, for each
0.65
particular filter we take the maximum value as the feature
corresponding to this particular filter. The convolutional layer 0.60
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0
utilizing the activation function and the pooling layer acts as
λ
a non-linear feature extractor.
Fig. 4: 11 different executions made on the validation set of
Up to this point we have described the process by which
the Ferramenta dataset, to estimate the best value for the λ
a single feature is extracted from a single filter. The set of
parameter of the proposed loss function in Eq. 5. The best
these individual features are linked into a single layer and then
value obtained is for λ = 0.8.
connected to a subsequent fully-connected layer that has the
purpose of connecting the textual features with the next block
TABLE I: Information on multi-modal datasets used in this
of deconvolution layers used to transform textual features into
work. A multi-modal dataset consists of an image and accom-
an image.
panying text description. The last column indicates the text
description language.
B. Up-sampling
Dataset #Cls Train Test Lang.
The purpose of the up-sampling block is to transform the
Ferramenta 52 66,141 21,869 IT
features extracted from the text into image format that best
represents the description contained in the text.
We use 4 deconvolution layers, each of which doubles the
The last layer is the output layer that has a number of
size of the input features. In practice, we start with 512 features
neurons equal to the number of classes of the problem that
maps of size 7×7 and then move to a second layer with 256
we want to learn.
features maps of size 13×13, followed by a new layer having
128 features maps o size 25×25, another layer with 64 features IV. DATASETS
maps of size 50×50 and finally, a last layer F with 3 features
In multimodal dataset, modalities are obtained from multiple
maps of size 100 × 100. The up-sampling blocks consist of
input sources. Dataset used in this work consists of images
the nearest-neighbor up-sampling followed by a 5 × 5 stride
and accompanying text descriptions. We select Ferramenta [3]
1D convolution. Batch normalization and ReLU activation are
multimodal dataset that are created from e-commerce website.
applied after every convolution except the last one where we
Table I shows information on this dataset. Ferramenta multi-
used a sigmoid to guarantee that the values of features maps
modal dataset [3] is made up of 88,010 adverts split in
F were all in the range [0,1]. This last layer can be interpreted
66,141 adverts for train set and 21,869 adverts for test set,
directly as an image.
belonging to 52 classes. Ferramenta dataset provides a text and
a representative image for each commercial advertisement. It
C. Classification
is interesting to note that text descriptions in this dataset are
The last block is similar to a convolutive neural network in Italian Language.
that feeds the features F to 4 successive convolutive layers. Another dataset used in our work is the Oxford-102 Flowers
Each of these layers produces features maps of size 50 × 50, dataset [28] containing 8,189 flow images in 102 categories.
25 × 25, 13 × 13 and 7 × 7 respectively. Starting from the Each image in this dataset is annotated with 10 descriptions
largest layer, we used 2 × 2 stride and the following number provided by [29]. Because this dataset has class-disjoint train-
of 5 × 5 filters: 64, 32, 16 and 8 respectively. ing and test sets, with 82 train+val and 20 test classes, we
The convolutional layers are followed by two fully con- randomly shuffled all the classes and split back into training
nected layers having 1024 and 512 neurons respectively. and test. In this way, all the classes available in the training
For regularization we employ dropout before the output set are also present in the test set.
layer with a constraint on l -norms of the weight vectors.
2
V. EXPERIMENTS
Dropout prevents co-adaptation of hidden units by randomly
dropping out a proportion p of the hidden units during foward The proposed approach transforms text descriptions into a
backpropagation. representative image. We use standard CNN hyperparameters.

| 86.3%88.7% 87.5%89.1%88.7%88.4%88.1% 87.1%88.2%   | None   | None   | None   | None   | None   | None   | None   | None   | None   | None   | None   | None   | None   | None   | None   | None   | None   | None   | None   | None   | None   |
| 86.3%                                             |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |
| 75.7%                                             |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |
|:--------------------------------------------------|:-------|:-------|:-------|:-------|:-------|:-------|:-------|:-------|:-------|:-------|:-------|:-------|:-------|:-------|:-------|:-------|:-------|:-------|:-------|:-------|:-------|
|                                                   |        |        |        |        |        |        | 87.5%  |        |        |        |        |        |        |        |        |        | 87.1%  |        |        |        |        |
| 0.0                                               |        |        | 0.2    |        | 0.4    |        | 0.6    |        | 0.8    |        | 1.0    |        | 1.2    |        | 1.4    |        | 1.6    |        | 1.8    |        | 2.0    |
|                                                   |        |        |        |        |        |        |        |        |        |        | λ      |        |        |        |        |        |        |        |        |        |        |
| 11                                                |        | d      | iffe   | r      | ent    | e      | xec    | u      | tio    | n      | s m    | a      | de     | h      | on t   | h      | e v    | a      | lida   | t      | ion    |
| am                                                |        | e      | nta    | e      | data   | s      | et,    | t      | o e    | s      | tim    | a      | te t   | ti     | e b    | e      | st     | v      | alue   | .      | for    |
| er o                                              |        | I      | f th   | s      | pr     | o      | pos    | e      | d l    | o      | ss     | f      | unc    | a      | on     | i      | n E    | t      | q. 5   | s      | Th     |
| btain                                             |        | l      | ed i   | s      | fo     | r      | λ=     | o      | 0.8    | .      | ulti-  | i      | mod    | o      | l d    | a      | tase   | a      | s u    | an     | ed     |
| I:                                                |        | t      | nfor   | u      | mati   | o      | n      | a      | n m    | c      | ons    | s      | sts    | l      | fa     | n      | im     | d      | ge     | e      | da     |
| mu                                                |        | l      | ti-m   |        | oda    | l      | dat    | .      | set    | e      | la     |        | t co   |        | um     | n      | in     |        | icat   |        | s th   |
| tex                                               |        |        | de     |        | crip   | t      | ion    |        | Th     |        |        |        |        |        |        |        |        |        |        |        | um     |
| ion                                               |        |        | ang    |        | age    | .      |        |        |        |        |        |        |        |        |        |        |        |        |        |        | ble    |
| F                                                 |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |
| ast                                               |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |
| eq                                                |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |
| t to                                              |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |
|                                                   |        | D      | atase  | t      |        |        | #C     | l      | s      |        | Trai   | n      |        |        | Test   |        |        | L      | ang.   |        |        |
|                                                   | F      |        | erram  |        | enta   |        | 5      | 2      |        |        | 66,1   | 4      | 1      | 2      | 1,86   | 9      |        |        | IT     |        |        |
|                                                   |        |        | laye   |        | is     |        | the    |        | out    |        | ut l   |        | yer    |        | tha    |        | ha     |        | a      |        |        |
|                                                   |        |        | al t   |        | th     |        | nu     |        | mbe    |        | of     |        | clas   |        | es     |        | f th   |        | pr     |        |        |
|                                                   |        |        | ear    |        | .      |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |        |

λ = 0 λ = 0.2 λ = 0.4 λ = 0.6 λ = 0.8 λ = 1
λ = 1.2 λ = 1.4 λ = 1.6 λ = 1.8 λ = 2 Expected
Fig. 5: Some examples of generated images from Ferramenta test dataset. The models trained for each λ by minimizing Eq. 5,
were trained for 20 epochs only, to speed up the experiment.
The initial learning rate is set to 0.001 along with Adam as of generated images beside the images we expected and the
optimizer. In our experiments, accuracy is used to measure text passed as input. As you can see from the figure, many of
classification performance. the images generated are identical to those we expect to find,
The purpose of the first experimental phase is to analyze while some of them represent the same object but arranged
the generative capacity of our model. We conducted following differently (see for example the screw and the pliers with the
experiments with this aim in mind: (1) estimate of the best λ red handle). We observed that some images have no visual
parameter for Eq. 5, (2) qualitative analysis of the generated meaning and do not represent any of the objects in the training
images F, (3) ability of the model to generate new images set, even if in many of these cases the classification is correct.
according to the description given in input. This means that the information extracted from the text is still
present in the image which is then used by the last block to
As a second group of experiments we have analyzed the
classify.
capacity of the proposed model to generate embedding in
image format of the input text. We conducted following In generalization, the proposed model has the ability to
experiments with this second aim in mind: (1) estimate of the generate images that it has never seen in training and this
best lambda parameter to obtain the most significant encoding, ability is directly correlated with the words we feed in input. In
(2) extraction of a new dataset of encoded text in image format this experiment we tried to mix the tokens of two descriptions
to compute the classification accuracy using a well-known belonging to different categories to highlight the capacity. As
CNN. can be seen from Fig. 8, by taking some tokens from two
different descriptions and feeding them to the neural model,
The first experiment concerned the estimation of the best
in some cases this produces images that are a combination of
λ value to be used in the proposed loss function described
the objects representing the two descriptions. For example, in
in Eq. 5. To achieve this, we first extracted a validation
the same Fig. 8 you can see an image of an object that is the
set from our training set and on this we calculated the
composition of a screw and a clamp. This is because in the
classification accuracy to extract the best value to assign to
description given as input to the model, the most important
the lambda parameter. Fig. 4 shows the results of all the
tokens of both objects are present.
experiments conducted on the validation set. The accuracy
results reported in this figure were obtained by averaging the Using the loss function of Eq. 6, which has a much more
accuracy values of 5 runs. As can be seen from the figure, restricted range of variability, it is possible to give more
the best value we obtained is for λ = 0.8. To visually emphasis to the encoding of the input text in image format. To
analyze the effect of the λ parameter, we also performed find the best λ parameter that produces the best encoding we
a quick test by training 11 different models for 20 epochs varied the parameter and for each of its values we computed
using λ ∈ {0,0.2,0.4,0.6,0.8,1,1.2,1.4,1.6,1.8,2}. Then the classification accuracy on the test set of the Ferramenta and
we compared some of the resulting images as shown in Fig. 5. Flowers datasets. Fig. 7 shows the encodings and the accuracy
The best defined image is for λ = 0.8 while for λ = 0 we obtained when the parameter changes. On these results we
have an abstract visual representation since the second part of performed the last experiment using the λ = 6 parameter.
the loss function described in Eq. 3 has been removed. In this latest experiment we extracted two new image
As a second experiment, we first trained a model using the datasets using two different models trained on the two datasets.
entire training set and then visually analyzed generated images In Fig. 6 you can see some examples of images generated,
with our model on the test set. Fig. 3 shows some examples alongside the original image of the dataset. It can be seen

| 0   | 1   | 2   | 3   | 4   | 5   | 6   | 7   | 8   | 9   | 10   |
|:----|:----|:----|:----|:----|:----|:----|:----|:----|:----|:-----|
|     |     |     |     |     |     |     |     |     |     |      |

| 0   | 1   | 2   | 3   | 4   | 5   | 6   | 7   | 8   | 9   | 10   |
|:----|:----|:----|:----|:----|:----|:----|:----|:----|:----|:-----|
|     |     |     |     |     |     |     |     |     |     |      |

how different source images correspond to a different text
encoding. To analyze the information content of these two
new datasets we have trained two CNN AlexNet to calculate
their classification accuracy. For the Ferramenta dataset we got
93.68% while for the Flowers dataset we got 99.05%. The first
result is slightly higher than the one published in [30] while
the second result obtained on the Flowers dataset is incredibly
high. The reason we got such high accuracy is because our
dataset contains 10 different text descriptions associated with
the same image. Having divided the training set randomly into
training and test, the same images can be found both in the
training set and in the test set. Ultimately this means that the
new datasets created do not only encode information extracted
from the text but also from images.
VI. CONCLUSION
In this work we have proposed a new approach to generate
an image that is representative of a noisy text description
available in natural language. The approach we proposed uses
a new loss function in order to simultaneously minimize the
classification error and the distance between the desired image
and a features map of the same model. The qualitative results
are very interesting but, for the moment, we have ignored the
classification performances because this was not our focus of
the present work. In the future we want to exploit the same
idea to try to improve the classification accuracy that can be
Fig. 6: Columns (a) and (b) show some examples of images obtained with a single convolutive neural model.
extracted from Flowers and Ferramenta datasets, respectively. Another interesting aspect emerged from this work is that
The columns to the right of (a) and (b) show the text encodings the same approach we proposed can be used to encode in
extracted as features layer F using λ = 6 in Eq. 6. image format both the information contained in the input
text and the information extracted from the image associated
with the text. This feature is very interesting to be able
to incorporate multimodal information into a single image
dataset. In this way, multimodal information can be processed
directly by a single CNN normally used to process only
images.
REFERENCES
[1] J. Arevalo, T. Solorio, M. Montes-y Go´mez, and F. A. Gonza´lez,
“Gated multimodal units for information fusion,” arXiv preprint
arXiv:1702.01992, 2017.
[2] D. Kiela, E. Grave, A. Joulin, and T. Mikolov, “Efficient large-scale
multi-modal classification,” Proceedings of AAAI 2018, 2018.
[3] I. Gallo, A. Calefati, and S. Nawaz, “Multimodal classification fusion in
real-world scenarios,” in Document Analysis and Recognition (ICDAR).
IEEE, 2017, pp. 36–41.
[4] L. Wang, Y. Li, and S. Lazebnik, “Learning deep structure-preserving
image-text embeddings,” in Proceedings of the IEEE conference on
Fig. 7: The top row contains encodings for the same image computer vision and pattern recognition, 2016, pp. 5005–5013.
[5] D. Kiela and L. Bottou, “Learning image embeddings using con-
belonging to the Ferramenta dataset when the model was
volutional neural networks for improved multi-modal semantics,” in
trained using Eq. 6 with λ parameters showed on the top.
Proceedings of the 2014 Conference on Empirical Methods in Natural
The images on bottom row are created using an image of Language Processing (EMNLP), 2014, pp. 36–45.
[6] C. W. Leong and R. Mihalcea, “Going beyond text: A hybrid image-
the Flowers dataset. Below each image is the test accuracy
text approach for measuring word relatedness.” in IJCNLP, 2011, pp.
obtained with the corresponding λ parameter.
1403–1407.
[7] A. Karpathy and L. Fei-Fei, “Deep visual-semantic alignments for
generating image descriptions,” in Proceedings of the IEEE conference
on computer vision and pattern recognition, 2015, pp. 3128–3137.

|    |    |    |    |
|:---|:---|:---|:---|
|    |    |    |    |
|    |    |    |    |
|    |    |    |    |
|    |    |    |    |

| 0   | 1   | 2   |
|:----|:----|:----|
|     |     |     |

| 0   | 1   | 2   |
|:----|:----|:----|
|     |     |     |

| 0   | 1   | 2   |
|:----|:----|:----|
|     |     |     |

| 0   | 1   | 2   | 3   | 4   |
|:----|:----|:----|:----|:----|
|     |     |     |     |     |

| 0   | 1   | 2   |
|:----|:----|:----|
|     |     |     |

| 0   | 1   |
|:----|:----|
|     |     |

Fig. 8: A set of F images generated by the proposed model. F and F have generated starting from text documents t and
k i j i
t respectively. All other F images were generated by combining different percentages of tokens extracted simultaneously
j k
from t and t . The two colored rectangles below F images are indicative of the percentages of tokens from t and t used
i j k i j
to generate the F images.
k
[8] K.Xu,J.Ba,R.Kiros,K.Cho,A.Courville,R.Salakhudinov,R.Zemel, tional Conference on Machine Learning-Volume 70. JMLR. org, 2017,
and Y. Bengio, “Show, attend and tell: Neural image caption generation pp. 2912–2921.
with visual attention,” in International conference on machine learning, [20] A. Nguyen, J. Clune, Y. Bengio, A. Dosovitskiy, and J. Yosinski, “Plug
2015, pp. 2048–2057. & play generative networks: Conditional iterative generation of images
[9] Q. Zhang, J. Fu, X. Liu, and X. Huang, “Adaptive co-attention network in latent space,” in Proceedings of the IEEE Conference on Computer
for named entity recognition in tweets,” in Thirty-Second AAAI Confer- Vision and Pattern Recognition, 2017, pp. 4467–4477.
ence on Artificial Intelligence, 2018. [21] T. Xu, P. Zhang, Q. Huang, H. Zhang, Z. Gan, X. Huang, and
X. He, “Attngan: Fine-grained text to image generation with attentional
[10] O. Arshad, I. Gallo, S. Nawaz, and A. Calefati, “Aiding intra-text repre-
generativeadversarialnetworks,”inProceedingsoftheIEEEConference
sentations with visual context for multimodal named entity recognition,”
on Computer Vision and Pattern Recognition, 2018, pp. 1316–1324.
arXiv preprint arXiv:1904.01356, 2019.
[22] H. Zhang, T. Xu, H. Li, S. Zhang, X. Wang, X. Huang, and D. N.
[11] A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell, and
Metaxas, “Stackgan: Text to photo-realisticimage synthesis with stacked
M. Rohrbach, “Multimodal compact bilinear pooling for visual question
generative adversarial networks,” in Proceedings of the IEEE Interna-
answering and visual grounding,” Proceedings of Empirical Methods in
tional Conference on Computer Vision, 2017, pp. 5907–5915.
Natural Language Processing, EMNLP 2016, pp. 457–468, 2016.
[23] A.Dosovitskiy,J.T.Springenberg,M.Tatarchenko,andT.Brox,“Learn-
[12] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and
ing to generate chairs, tables and cars with convolutional networks,”
L. Zhang, “Bottom-up and top-down attention for image captioning and
IEEE transactions on pattern analysis and machine intelligence, vol. 39,
visual question answering,” in CVPR, vol. 3, no. 5, 2018, p. 6.
no. 4, pp. 692–705, 2016.
[13] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee,
[24] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
“Generative adversarial text to image synthesis,” arXiv preprint
arXiv preprint arXiv:1412.6980, 2014.
arXiv:1605.05396, 2016.
[25] Y. Kim, “Convolutional neural networks for sentence classification,” in
[14] M. Cha, Y. Gwon, and H. Kung, “Adversarial nets with perceptual losses Proceedings of the 2014 Conference on Empirical Methods in Natural
for text-to-image synthesis,” in 2017 IEEE 27th International Workshop Language Processing, EMNLP, 2014, pp. 1746–1751.
on Machine Learning for Signal Processing (MLSP). IEEE, 2017, pp. [26] V.NairandG.E.Hinton,“Rectifiedlinearunitsimproverestrictedboltz-
1–6. mann machines,” in Proceedings of the 27th international conference on
[15] S. Hong, D. Yang, J. Choi, and H. Lee, “Inferring semantic layout machine learning (ICML-10), 2010, pp. 807–814.
for hierarchical text-to-image synthesis,” in Proceedings of the IEEE [27] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and
Conference on Computer Vision and Pattern Recognition, 2018, pp. P. Kuksa, “Natural language processing (almost) from scratch,” Journal
7986–7994. of machine learning research, vol. 12, no. Aug, pp. 2493–2537, 2011.
[16] Y. Zhu, M. Elhoseiny, B. Liu, X. Peng, and A. Elgammal, “A gener- [28] M.-E. Nilsback and A. Zisserman, “Automated flower classification over
ative adversarial approach for zero-shot learning from noisy texts,” in alargenumberofclasses,”in2008SixthIndianConferenceonComputer
Proceedings of the IEEE Conference on Computer Vision and Pattern Vision, Graphics & Image Processing. IEEE, 2008, pp. 722–729.
Recognition, 2018, pp. 1004–1013. [29] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee,
[17] E. Mansimov, E. Parisotto, J. Ba, and R. Salakhutdinov, “Generating “Generative adversarial text to image synthesis,” in Proceedings of the
images from captions with attention,” in ICLR, 2016. 33rd International Conference on International Conference on Machine
Learning - Volume 48, ser. ICML’16. JMLR.org, 2016, pp. 1060–1069.
[18] E. Mansimov, E. Parisotto, J. L. Ba, and R. Salakhutdinov, “Generating
[30] I. Gallo, A. Calefati, S. Nawaz, and M. K. Janjua, “Image and en-
images from captions with attention,” arXiv preprint arXiv:1511.02793,
coded text fusion for multi-modal classification,” in 2018 International
2015.
Conference on Digital Image Computing: Techniques and Applications
[19] S. Reed, A. van den Oord, N. Kalchbrenner, S. G. Colmenarejo,
(DICTA), Dec 2018, pp. 1–7.
Z. Wang, Y. Chen, D. Belov, and N. de Freitas, “Parallel multiscale
autoregressive density estimation,” in Proceedings of the 34th Interna-

|    |    | None   |    | None   | None   |    |    | None   | None   |    |
|:---|:---|:-------|:---|:-------|:-------|:---|:---|:-------|:-------|:---|
|    |    |        |    |        |        |    |    |        |        |    |
|    |    |        |    |        |        |    |    |        |        |    |

|    |    | None   |    | None   | None   |    |    | None   | None   |    |
|:---|:---|:-------|:---|:-------|:-------|:---|:---|:-------|:-------|:---|
|    |    |        |    |        |        |    |    |        |        |    |
|    |    |        |    |        |        |    |    |        |        |    |