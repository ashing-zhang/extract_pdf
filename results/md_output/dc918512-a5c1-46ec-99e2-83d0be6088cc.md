INCREMENTAL BINARIZATION ON RECURRENT NEURAL NETWORKS
FOR SINGLE-CHANNEL SOURCE SEPARATION
Sunwoo Kim, Mrinmoy Maity, Minje Kim
Indiana University
Department of Intelligent Systems Engineering
9102 guA 32  ]SA.ssee[  1v89880.8091:viXra
Bloomington, IN 47408
kimsunw@indiana.edu, mmaity@iu.edu, minje@indiana.edu
ABSTRACT computation within each GRU cell is:
This paper proposes a Bitwise Gated Recurrent Unit (BGRU) net-
r(l)(t) = σ W(l)x(l−1)(t)+ U(l)h(l)(t − 1)
work for the single-channel source separation task. Recurrent Neu- r r
ral Networks (RNN) require several sets of weights within its cells, z(l) σ(cid:16) W(l)x(l−1) U(l)h (cid:17)
(t) = (t)+ (t − 1)
z z
whichsignificantlyincreasesthecomputational costcomparedtothe
(1)
fully-connected networks. To mitigate this increased computation, h˜(l) φ(cid:16) W(l)x(l−1) U(l) r(l) ⊙(cid:17) h(l)
(t) = (t)+ (t) (t− 1)
we focus on the GRU cells and quantize the feedforward procedure h h
with binarized values and bitwise operations. The BGRU network h(l) (t) = z((cid:16)l) (t) ⊙h(l) (t− 1) + (1(cid:0)− z(l) (t))⊙ h˜(l) (t) (cid:1)(cid:17)
is trained in two stages. The real-valued weights are pretrained and
transferred to the bitwise network, which are then incrementally bi- where l = {1,...,L + 1} denotes the layer index and t =
narized to minimize the potential loss that can occur from a sud- {1,...,T} is the time index. r ,z ,h˜ , and h are reset gate,
t t t t
den introduction of quantization. As the proposed binarization tech-
update gate, candidate hidden state, and updated hidden state re-
nique turns only a few randomly chosen parameters into their binary RK(l)
spectively all of dimension with K(l) as the number of units
versions, it gives the network training procedure a chance to gen-
W(l) RK(l)×K(l−1) U(l) RK(l)×K(l)
at layer l. ∈ and ∈ are
tlyadapt tothe partlyquantized versionof thenetwork. Iteventually r r
the weight matrices for the input x(l)(t) and previous hidden state
achievesthefullbinarizationbyincrementallyincreasingtheamount
of binarization over the iterations. Our experiments show that the h(l)(t − 1) at the reset gate. Similarly, W z, U z, W h, and U h
proposed BGRU method produces source separation results greater are corresponding weights for the update gate and candidate state.
than that of a real-valued fully connected network, with 11-12 dB The σ and φ refer to the logistic sigmoid and hyperbolic tangent
mean Signal-to-Distortion Ratio (SDR). A fully binarized BGRU activation functions. The bias term is omitted for simplicity. Note
still outperforms a Bitwise Neural Network (BNN) by 1-2 dB even that h(l)(t) is fed to the next layer as an input, x(l)(t).
with less number of layers. For a single feedforward step, the RNN requires multiple sets of
weights and performs operations in(1)for T timesteps. Withdeeper
Index Terms— Speech Enhancement, Recurrent Neural Net-
RNNs, the computational cost rises rapidly in terms of K and L.
works, Gated Recurrent Units, Bitwise Neural Networks
This paper presents an efficient method to reduce the computational
and spatial complexity of the GRU network for the source separa-
1. INTRODUCTION tion problem while maintaining high performance results. We ex-
tend from the idea of Bitwise Neural Networks (BNN) [16] [17] and
Neural network-based approaches to source separation tasks have low-precision RNNs [18]. The model we propose is a Bitwise GRU
been becoming more prevalent [1, 2, 3]. Fully connected deep neu- (BGRU)networkthatreducesnetworkcomplexitybyre-definingthe
ral networks (DNN) have shown to be capable of learning complex originally real-valued inputs and outputs, weights, and operations in
mapping functions from a large set of noisy signals and their corre- a bitwise fashion. By limiting the network to bipolar binary values,
sponding ideal binarymask (IBM)targetoutputs [4,5,6]. Recurrent the space complexity of the network can be significantly reduced.
neural networks (RNN),which are structured to be more effective in In addition, all real-valued operations during the feedforward proce-
applications involving sequential or temporal data, have also shown dure can be replaced with bitwise logic, which further reduces both
to excel in the same task [7, 8, 9, 10, 11]. The RNN is able to attain spatial and time complexity [19, 20, 21, 22].
thesuperior performance byutilizinga sharedhidden state andgates Transforming real-valuedweights intobipolar binaries resultsin
within its hidden cells that guide the memory and learning over a heavy quantization loss [23, 24]. To alleviate this effect, the weights
sequence of inputs [12]. The most practical method to train RNNs are converted into binary values through a gentle training proce-
iswithtruncated Backpropagation Through Time (BPTT)[13]. This dure. In this paper, we introduce an incremental training method
bounded-history approximation method simplifies computation by for weights of the BGRU network that holds onto the quality of the
limiting itself to a fixed scope of T timesteps [14]. source separation model. Experimental results for single-channel
The most efficient cell structure that is robust to the gradient source separationtasksshow thattheBGRUmodel showsincremen-
vanishing problem is the Gated Recurrent Unit (GRU)cell [15]. The tal and predictable loss depending on the amount of binarization and
still performs better than a real-valued Fully-Connected Network
This project was supported by Intel Corporation. (FCN).

2. BITWISE GATED RECURRENT UNITS (BGRU) The product between two binarized values (e.g. between the (i,j)-
th element of W¯ (l) and the j-th element of x ¯(l−1)(t)) is equivalent
r
2.1. Background: Bitwise Neural Networks
to the XNOR operations, a cheaper binary operation than the corre-
¯
sponding floating-point operation. Also, the use of sign functions φ
Binarizationhasbeenexploredasamethodofnetworkcompression.
anda hardstepfunctionσ¯ inplace ofthe hyperbolic tangent andsig-
BinaryConnect [17], binarized neural networks [19], trained ternary
moidfunctions alsoexpedite theprocess because theycanbeusually
quantization [25], and Bitwise Neural Networks (BNN) [26] have
implemented by a pop counter.
implemented a binarized or ternarized neural network in bipolar bi-
naries (with zeros in the ternarized case) for network compression.
They emphasize that replacing real-valued operations with bitwise 2.2.3. Scaled sparsity and Bernoulli masks
versions greatly reduces the network’s complexity. In particular, the
We define two types of masks that are applied on various parts of
BNN training process is assisted by initializing the binarized net-
the network. The scaled sparsity mask is a two-in-one solution to
workwithpretrained weights. The weights are compressed ina real-
introduce both scaling parameters and sparsity into weights during
valued network with the hyperbolic tangent activation function in
the binarization process. To binarize the weight matrices the scaled
order to better approximate their binary versions. Further quantiza-
sparsity mask B is created using a predefined sparsity parameter
tion is performed in the BNN, where the inputs are quantized using
0 < ρ < 1. First, we find a per-layer cutoff value β and the scal-
Quantization-and-Disperson, which uses Lloyd-Max’s quantization
ing parameter µ that meet the following equations:
toconverteachfrequencymagnitudeofthenoisyinputspectruminto
4bits withbipolar binary features [27]. Inthe domain of source sep- |W(l) K(l−1)K(l)ρ
S = {(i,j) : | > β}, |S| =
aration, BNN’s have been applied by predicting Ideal Binary Masks i,j
1 (5)
(IBM) as target outputs [16]. (l)
µ = |W |,
While the BNN significantly reduces the space and time com- |S| i,j
(i,j)∈S
plexity of the network, the conversion from real-values to bipolar X
binaries inevitably produces quantization error. One method to re- where S is the set of weight indices whose absolute values are larger
duce this penalty is the concept of sparsity [16]. Sparsity can be than the cutoff value and |S| denotes the number of such weights.
introduced to bitwise networks by converting the pretrained weights Therefore, for a given sparsity value ρ, we first sort the weights in
withsmallervaluesto0’s. Thethresholdfordeterminingthesparsity their absolute values and then find the cutoff that results in S with
iscalculatedwithapredefinedboundary β. Therelaxedquantization the predefined size. Using β and µ, we set the mask elements as
process for a weight element w is: follows:
µ if |W | > β
i,j
+1 if w > β B = (6)
i,j
0 otherwise
(
w¯ = −1 if w < −β (2)

The other type of mask is a random Bernoulli matrix C with a
 0 otherwise
parameter 0 < π < 1 as the amount of binarization. The value of
where w¯ represents the binarized variable. Another way to miti- π is initially chosen as a small value (e.g. 0.1 for 10% binarization)

gate the quantization error is by multiplying a scaling factor µ to the and gradually increased up to 1.0, which means the network is com-
bipolar-binarized weights, so that the quantized values approximate pletely binarized. The created masks are applied on weights W to
the original values more closely [25]. create the partly binarized matrix W:
2.2. Feedforward in BGRU W = φ¯ (W ) ⊙ B ⊙ C + φ(W ) ⊙ (1 − C ). (7)
c
2.2.1. Notation and setup The purpose(cid:0)of B with µ(cid:1)values is to lessen the quantization er-
c
¯
ror from the binarization. The φ operator will transform all values
Forthe following sections of the paper, we specify discrete variables
into bipolar binary values, which would be too intensive of a tran-
with a bar notation, i.e. x¯. Depending on the context, this could be
formation because the distribution of the first round weights are all
a binary variable with 0 and 1 (e.g. gates), a bipolar binary variable
relatively close to 0. Thus, by multiplying the remaining nonzero
with +1 and −1 (e.g. binarized hidden units), or a ternary variable
bipolar values after applying sparsity with µ, the values are scaled
(e.g. sparse bipolar binary weights). The binary versions of logistic
down to the average value of the non-sparse portion, which is a bet-
sigmoid and hyperbolic tangent activation functions are:
ter representative for the nonzero elements. Note that feedforward is
sgn(x)+ 1 still bitwise thanks to the symmetry of B and by skipping zeros.
σ¯(x) = ∈ {0,1}, φ¯ (x) = sgn(x) ∈ {−1,+1} (3)
2 The Bernoulli mask C enables a gradual transition from real-
valued weights and operations to bitwise versions. This mask is ap-
respectively where sgn(x) is a sign function [19, 26].
plied on the bitwise and real-valued elements in a complementary
way to control the proportion of binarization in the network. W in
2.2.2. The feedforward procedure
(7) is binarized only partly with the proportion set by π. Note that
In the BGRU, the feedforward process is defined as follows: for the real-valued weights we are using a tanh compressed vecrsion
φ(W ) for the purpose of regularization (see Section 2.3.1 for more
¯r(l) W¯ (l)x ¯(l−1) U¯(l)h¯(l)
(t) = σ¯ (t)+ (t − 1)
r r details).
(cid:16) (cid:17) C is used to control the binarization of the other network ele-
z ¯(l) (t) = σ¯ W¯ (l)x ¯(l−1) (t)+ U¯(l)h¯(l) (t − 1)
z z ments such as gates and hidden units, too. For the candidate hidden
(4)
¯
h¯ ˜(l) (t) = φ¯(cid:16) W¯ (l)x ¯(l−1) (t)+ U¯(l) ¯r(l) (t)⊙ h¯(cid:17) (l) (t − 1) units h˜ , for example, the activations are performed as:
h h
h¯(l) (t) = z ¯((cid:16) l) (t)⊙ h¯(l) (t − 1) + (1(cid:0) − ¯z(l) (t)) ⊙ h¯ ˜(l) (t) (cid:1)(cid:17) h˜ = h¯ ˜ ⊙ C + h˜ ⊙ (1 − C ), (8)
b

The gates are also partially binarized in this way. The mask C is the introduction of the masks and bitwise functions, the feedforward
generated at each iteration for the weights as in (7) and then at each step for the hidden candidate state becomes:
timestep for the activation functions of GRU cells as in (8). This
W(l) (φ¯ (W(l) ⊙B C φ(W(l) C
= ) ) ⊙ + ) ⊙ (1 − )
ensures that the gradients of the bitwise terms are evenly distributed h h h
gradually for all weights at each levels of π. Without even distri- U(l) (φ¯ (U(l) B ⊙C φ(U(l) −C
= ) ⊙ ) + ) ⊙(1 )
ch h h
bution, certain elements of the graph that do not participate in the
(11)
bitwise procedure begin focusing on compensating for the quantiza- V = W(l)x(l−1) (t) + U(l) ¯r(l) (t)⊙ h¯(l) (t − 1)
b h h
tion loss from the other bitwise elements. This needs to be avoided
(l)
since as π is increased to 1.0 these elements need to be quantized h˜ (t) = φ c¯ (V ) ⊙ C + φ(V b)⊙(cid:0) (1 − C ) (cid:1)
eventually.
where V is an intermediary term. The Bernoulli parameter π is in-
b
cremented gradually to determine C until the network is completely
2.3. Training BGRU Networks binarized at π = 1.0.
Backpropagation: The derivatives of non-differentiable activa-
The objective is to accept binarized mixture signal inputs and pre-
tion functions are overwritten with the derivatives of their relaxed
dict the corresponding IBMs. The inputs are binarized using the
φ¯′ φ′ σ¯′ σ′
counterparts such that = and = . This simplifies the
Quantization-and-Dispersion technique [26], and the target outputs
gradients for (11). The gradients are computed as (10) with an addi-
are bipolar binary IBM predictions which are later converted to 0’s
tional factor for the masks which are:
and 1’s for the source separation task. We follow the typical two-
W(l) W(l) B C C
round training scheme from BNNs, too. ∇ = ∇ ⊙ ( ⊙ + (1 − ))
h h
(12)
U(l) U(l) B C C
∇ = ∇ ⊙ ( ⊙ + (1 − ))
h h
2.3.1. First round: Pretraining φ-compressed weights
The gradients are computed similarlyfor the gates. The calculations
The GRU network is first initialized with real-valued weights and in (12) show that the network is the same as the first round network
thentrainedonquantizedbinaryinputs. Duringtraining,the weights except with the addition of masking factors. Only the real-valued
are wrapped with the hyperbolic tangent activation function, φ. This weights are updated with the gradients during training.
has the effect of bounding the range of weights between −1 and +1
as well as regularization. In the second round, the sign function,
3. EXPERIMENTS
¯
φ is applied on the weights instead, hence the first round network
can be perceived as its softer version. For example, the feedforward
3.1. Experimental Setups
procedure in (1) for only the hidden candidate state at layer l and
timestep t becomes: For the experiment, we randomly subsample 12 speakers for train-
ing and 4 speakers for testing from the TIMIT corpus. For both
h˜(l) W(l) x(l−1) U(l) ¯r(l) (t)⊙h¯(l) subsamples, we select half of the speakers as male and the other
(t)=φ φ (t)+φ (t−1) (9)
h h
half as female. There are 10 short utterances per speaker recorded
(cid:16) (cid:17)
(cid:0) (cid:1) (cid:0) (cid:1)(cid:0) (cid:1) with a 16kHz sampling rate. Each utterances are mixed with 10
The φ-compressed weights are applied similarly for the reset and
different non-stationary noise signals with 0 dB Signal-to-Noise Ra-
update gates.
tio (SDR), namely {birds, casino, cicadas, computer keyboard, eat-
Backpropagation: Withthe introduction of φ on the weight ma-
ing chips, frogs, jungle, machine guns, motorcycles, ocean} [28].
trices, the derivative with respect to φ is added onto the backpropa-
In total, we have 227,580 training examples and 81,770 test exam-
gation due to the chain rule. For example, the gradients for (9) are
ples from 1,200 and 400 mixed utterances, respectively. We apply a
computed as:
Short-Time Fourier Transform (STFT)witha Hann window of 1024
andhopsizeof256. Toquantize the spectraintobipolarbinaries, we
δ (t) = δ(l) (t)⊙ (1 − z (t)) ⊙ 1 − h˜(l) (t)2
h˜ apply a 4-bit QaD procedure and convert them into n × (4 × 513)
T (cid:0) (cid:1) dimension matrices. These vectors are used as inputs to the BGRU
∇W(l) = δ (t) · x(l−1) (t) ⊤ ⊙ 1 −φ2 W(l) (10)
h h˜ h systems. The truncated BPTT length used was T = 50. We found
(cid:16)Xt=0 (cid:17) (cid:16) (cid:1)(cid:17) ρ = 0.8 to perform well in our experiment. We used the Adam
(cid:0) (cid:1) (cid:0)
T optimizer for both first and second rounds with the same beta pa-
∇U(l) = δ (t) · ¯r(l) (t) ⊙h¯(l) (t− 1) ⊙ 1 −φ2 U(l)
h h˜ h rameters, β 1 = 0.4 and β 2 = 0.9. Minibatch size is set as 10 for 10
(cid:16)Xt=1 (cid:1)(cid:17) (cid:16) (cid:1)(cid:17) mixed utterances constructed from 1 clean signal mixed with the 10
(cid:0) (cid:0)
noise signals. We train two types of networks that predict the IBMs
where δ(l)(t) is the backpropagation error for the training sample at
with respect to the noisy quantized input:
layer l and timestep t. The gradients are similarly defined for the
• Baseline with binary input: The baseline network is constructed
weights in the gates.
with a single GRU layer with K = 1024 units. The inputs to the
network are 4 × 513 dimension 4-bit QaD vectors and predicted
2.3.2. Second round: BGRU
outputs are 513 dimension IBMs. We use the first round training
algorithm to train the baseline network. For regularization, we
The BGRU network is initialized with the real-valued weights from
apply dropout rate of 0.05 for the input layer and 0.2 for the GRU
the first round, which are pretrained to be optimal for the source
layers.
separation task. The real-valued weights are saved for the backprop-
agation step and used to construct bitwise weights for the feedfor- • TheproposedBGRU:Weinitializetheweightswiththepretrained
ward procedure using both the mean-scaled sparsity mask B and weights and use the second round training algorithm to train the
Bernoulli mask C. The bitwise activation functions, σ¯ and φ¯ are BGRU network. Weincrease the π parameter by0.1startingfrom
applied during the feedforward as well. Again as an example, with 0.1 to 1.0. The learning rates are reduced for each increase in π.

16 16
1st Round SDR: 16.12 dB 1st Round SDR: 16.12 dB
14 14
12 12
RDS RDS
10 10
8 8
6 π = 0.1 π = 0.2 π = 0.3 π = 0.4 π = 0.5 π = 0.6 π = 0.7 π = 0.8 π = 0.9 π = 1.0 6 π = 0.1 π = 0.2 π = 0.3 π = 0.4 π = 0.5 π = 0.6 π = 0.7 π = 0.8 π = 0.9 π = 1.0
0 2000 4000 6000 8000 10000 0 200 400 600 800 1000
Epochs Epochs
(a) Results from1000 epochs for π < 1.0 and 100 epochs for π = 1.0 (b) Results from 100 epochs for each π
Fig. 1. Second round testing results on incremental levels of π. Figures (a) and (b) show the effects of running different number of iterations.
SDR improvement becomes stagnant and even starts to drop. How-
Table 1. Speech denoising performance of the proposed BGRU-
ever, in this way the network can prevent a greater drop in perfor-
based source separation model compared to FCN, BNN, and GRU
mance at the next increase in π. At π = 1.0, we only train for 100
networks
epochs and perform early stopping because the network is less ro-
Systems Topology SDR STOI
bust and degrades in performance after more than 100 epochs. Also,
1024×2 10.17 0.7880
since the network has finishedtraining for the source separation task
FCN with original input
2048×2 10.57 0.8060
at π = 1.0, further training is unnecessary. On the contrary, Fig-
1024×2 9.80 0.7790
ure 1(b) shows that training for less number of iterations, e.g. 100
FCN with binary input
2048×2 10.11 0.7946
epochs, produces a greater drop at each increment of π.
1024×2 9.35 0.7819 Thedropinperformance fromareal-valuednetworktoabitwise
BNN
2048×2 9.82 0.7861 version is quite comparable between a FCN with BNN and GRU
GRU with binary input 1024×1 16.12 0.9459 with BGRU. The loss is much greater in the BGRU network (16.12
π=0.1 15.50 0.9393 dB to 11.76 dB SDR) than in the case of BNN (10.11 dB to 9.82 dB
π=0.2 15.17 0.9361 SDR). Yet, the performance of a single-layer fully bitwise BGRU
π=0.3 14.90 0.9324 network with 1024 units (11.76 dB SDR and 0.8740 STOI) is still
π=0.4 14.58 0.9292 greater than that of a double-layer BNN with 2048 units (9.82 dB
π=0.5 14.32 0.9252 SDR and 0.7861 STOI), and also greater than that of a unquantized
BGRU 1024×1
π=0.6 14.02 0.9217 double-layer FCN with real-valued inputs and 2048 units (10.57 dB
π=0.7 13.66 0.9174 SDR and 0.8060 STOI). We discuss the space complexity of the
π=0.8 13.30 0.9104 BGRU network compared to a FCN and BNN. Considering that a
π=0.9 12.70 0.9019 GRU layer contains 3 sets of weights, the single layer BGRU net-
π=1.0 11.76 0.8740 work contains 3 × (1024 × 1) number of weights. This number is
stillless than a FCNor BNNof topology 2048×2. Weintroduced a
real-valued scaling factor µ, but it reduces down to bipolar binaries
once training is done, so it does not add additional costs.
3.2. Discussion
In the future, we plan to extend the network structure to deeper
Table 1 shows results for the BGRU along with other systems for ones. Also, more scheduled annealing of the π values is another
comparison. The metrics displayed are Signal-to-Distortion Ratio option to investigate.
(SDR)[29] and Short-Time Objective Intelligibility(STOI)[30]. At
each increase in π, there is a distinct drop in SDR and STOI due
4. CONCLUSION
to the loss in information as we increase the number of elements
undergoing binarization. Since the initial weights transferred from
In this paper, we proposed an incremental binarization procedure to
the first round are optimal, we restrict the weights from updating
binarize a RNN with GRU cells. The training is done in two rounds,
too drastically by dampening the learning rate at each increase in π.
first in a weight compressed network and then in an incrementally
Wedidnot observe substantial difference from reducingthe learning
bitwise version with the same topology. The pretrained weights of
rate before π = 0.8, however the performance becomes sensitive as
the first round are used to initialize the weights of the bitwise net-
the rate of binarization nears 1. In Figure 1(a) it can be seen that
work. For the BGRU cells, we redefined the feedforward procedure
from π = 0.8 the performance begins to decrease more than during
with bitwise values and operations. Due to the sensitivity in train-
previous π values.
ing the BGRU network, the bitwise feedforward pass is performed
The BGRU network is trained for an extended number of itera-
gently using two types of masks that determine the level of sparsity
tions so it propagates the corrections and adjusts to the quantization
and rate of binarization. With 4-bit QaD quantized input magnitude
injected into the network. We trained 1000 epochs for each π values
spectraandIBMtargets,theBGRUatfullbinarizationperformswell
except at π = 1.0. Figure 1(a) shows that this many iterations is not
for the speech denoising job with a minimal computational cost.
always beneficial within the same session with a fixed π, because

| 0                                                                                 | 1                                                                                 |
|:----------------------------------------------------------------------------------|:----------------------------------------------------------------------------------|
| 16 1st Round SDR: 16.12 dB                                                        | 16 1st Round SDR: 16.12 dB                                                        |
| 14                                                                                | 14                                                                                |
| 12                                                                                | 12                                                                                |
| RDS                                                                               | RDS                                                                               |
| 10                                                                                | 10                                                                                |
| 8                                                                                 | 8                                                                                 |
| 6 π = 0.1 π = 0.2 π = 0.3 π = 0.4 π = 0.5 π = 0.6 π = 0.7 π = 0.8 π = 0.9 π = 1.0 | 6 π = 0.1 π = 0.2 π = 0.3 π = 0.4 π = 0.5 π = 0.6 π = 0.7 π = 0.8 π = 0.9 π = 1.0 |
| 0 2000 4000 6000 8000 10000                                                       | 0 200 400 600 800 1000                                                            |
| Epochs                                                                            | Epochs                                                                            |

|         |         |         |         |         |         |         |          |         | π = 1.0   |
|:--------|:--------|:--------|:--------|:--------|:--------|:--------|:---------|:--------|:----------|
| π = 0.1 | π = 0.2 | π = 0.3 | π = 0.4 | π = 0.5 | 1st R   | ound SD | R: 16.12 | dB      |           |
|         |         |         |         |         | π = 0.6 | π = 0.7 | π = 0.8  | π = 0.9 |           |

|         |         |         |         |         |         |         |          |         |         |
|:--------|:--------|:--------|:--------|:--------|:--------|:--------|:---------|:--------|:--------|
| π = 0.1 | π = 0.2 | π = 0.3 | π = 0.4 | π = 0.5 | 1st R   | ound SD | R: 16.12 | dB      | π = 1.0 |
|         |         |         |         |         | π = 0.6 | π = 0.7 | π = 0.8  | π = 0.9 |         |

5. REFERENCES [15] K. Cho, B. Van Merrie¨nboer, C. Gulcehre, D. Bahdanau,
F. Bougares, H. Schwenk, and Y. Bengio, “Learning phrase
[1] Y.Xu,J.Du,L.-R.Dai,andC.-H.Lee, “Anexperimentalstudy representations using RNN encoder-decoder for statistical ma-
onspeech enhancement basedon deepneural networks,” IEEE chine translation,” arXiv preprint arXiv:1406.1078, 2014.
Signal processing letters, vol. 21, no. 1, pp. 65–68, 2014.
[16] M. Kim and P. Smaragdis, “Bitwise neural networks for effi-
[2] P. Huang, M. Kim, M. Hasegawa-Johnson, and P. Smaragdis, cient single-channel source separation,” in2018 IEEEInterna-
“Joint optimization of masks and deep recurrent neural net- tional Conference on Acoustics, Speech and Signal Processing
works for monaural source separation,” IEEE/ACM Transac- (ICASSP). IEEE, 2018, pp. 701–705.
tions on Audio, Speech, and Language Processing, vol. 23, no.
[17] M. Courbariaux, Y. Bengio, and J. P. David, “BinaryCon-
12, pp. 2136–2147, 2015.
nect: Training deep neural networks with binary weights dur-
[3] A. A. Nugraha, A. Liutkus, and E. Vincent, “Multichannel au- ing propagations,” in Advances in neural information process-
dio source separation with deep neural networks.,” IEEE/ACM ing systems, 2015, pp. 3123–3131.
Trans. Audio, Speech & Language Processing, vol. 24, no. 9,
[18] Joachim Ott, Zhouhan Lin, Ying Zhang, Shih-Chii Liu, and
pp. 1652–1664, 2016.
Yoshua Bengio, “Recurrent neural networks with limited nu-
[4] Y. Wang and D. Wang, “Towards scaling up classification-
merical precision,” arXiv preprint arXiv:1608.06902, 2016.
based speech separation,” IEEE Transactions on Audio,
[19] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and
Speech, and Language Processing, vol. 21, no. 7, pp. 1381–
Y.Bengio, “Binarizedneural networks,” inAdvances inneural
1390, 2013.
information processing systems, 2016, pp. 4107–4115.
[5] J. Le Roux, J. R. Hershey, and F. Weninger, “Deep NMF for
[20] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi, “Xnor-
speech separation,” in Acoustics, Speech and Signal Process-
net: Imagenet classification using binary convolutional neu-
ing (ICASSP), 2015 IEEE International Conference on. IEEE,
ral networks,” in European Conference on Computer Vision.
2015, pp. 66–70.
Springer, 2016, pp. 525–542.
[6] E. M. Grais, M. U. Sen, and H. Erdogan, “Deep neural net-
[21] G. Govindu, L. Zhuo, S. Choi, and V. Prasanna, “Analysis of
works for single channel source separation,” in Acoustics,
high-performance floating-point arithmetic on fpgas,” in null.
Speech and Signal Processing (ICASSP), 2014 IEEE Interna-
IEEE, 2004, p. 149b.
tional Conference on. IEEE, 2014, pp. 3734–3738.
[22] M.J.Beauchamp, S.Hauck,K.D.Underwood, andK.S.Hem-
[7] H. Erdogan, J. R. Hershey, S. Watanabe, and J. Le Roux,
mert, “Embedded floating-point units in fpgas,” in Proceed-
“Phase-sensitive and recognition-boosted speech separation
ingsofthe2006ACM/SIGDA14thinternationalsymposium on
using deep recurrent neural networks,” in Acoustics, Speech
Field programmable gate arrays. ACM, 2006, pp. 12–20.
and Signal Processing (ICASSP), 2015 IEEE International
Conference on. IEEE, 2015, pp. 708–712. [23] K.Hwang and W. Sung, “Fixed-point feedforward deep neural
network design using weights+ 1, 0, and- 1,” in Signal Pro-
[8] F.Weninger, H.Erdogan, S.Watanabe, E. Vincent,J. LeRoux,
cessing Systems (SiPS), 2014 IEEE Workshop on. IEEE, 2014,
J. R. Hershey, and B. Schuller, “Speech enhancement with
pp. 1–6.
LSTM recurrent neural networks and its application to noise-
robust ASR,” in International Conference on Latent Variable [24] M. Courbariaux, Y. Bengio, and J. P. David, “Training deep
Analysis and Signal Separation. Springer, 2015, pp. 91–99. neural networks with low precision multiplications,” arXiv
preprint arXiv:1412.7024, 2014.
[9] F. Weninger, J. R. Hershey, J. Le Roux, and B. Schuller,
“Discriminatively trained recurrent neural networks for single- [25] C. Zhu, S. Han, H. Mao, and W. J. Dally, “Trained ternary
channel speech separation,” in Proceedings 2nd IEEE Global quantization,” arXiv preprint arXiv:1612.01064, 2016.
Conference on Signal and Information Processing, GlobalSIP,
[26] M.Kimand P.Smaragdis, “Bitwiseneural networks,” inInter-
Machine Learning Applications in Speech Processing Sympo-
national Conference on Machine Learning (ICML) Workshop
sium, Atlanta, GA, USA, 2014.
on Resource-Efficient Machine Learning, Jul 2015.
[10] Y. Isik, J. Le Roux, Z. Chen, S. Watanabe, and J. R. Hershey,
[27] S.Lloyd, “Least squares quantization inPCM,” IEEEtransac-
“Single-channel multi-speaker separation using deep cluster-
tions on information theory, vol. 28, no. 2, pp. 129–137, 1982.
ing,” arXiv preprint arXiv:1607.02173, 2016.
[28] Z. Duan, G. J. Mysore, and P. Smaragdis, “Online PLCA for
[11] Z.Chen, S.Watanabe, H. Erdogan, and J.R.Hershey, “Speech
real-time semi-supervised source separation,” in International
enhancement andrecognition usingmulti-tasklearning of long
ConferenceonLatentVariableAnalysisandSignalSeparation.
short-term memory recurrent neural networks,” in Sixteenth
Springer, 2012, pp. 34–41.
Annual Conference of the International Speech Communica-
tion Association, 2015. [29] E. Vincent, R. Gribonval, and C. Fe´votte, “Performance mea-
surement in blind audio source separation,” IEEE transactions
[12] Y. Bengio, P. Simard, and P. Frasconi, “Learning long-term
on audio, speech, and language processing, vol. 14, no. 4, pp.
dependencies with gradient descent is difficult,” IEEE trans-
1462–1469, 2006.
actions on neural networks, vol. 5, no. 2, pp. 157–166, 1994.
[30] C. H. Taal, R. C. Hendriks, R. Heusdens, and J. Jensen, “A
[13] R. J. Williams and J. Peng, “An efficient gradient-based al-
short-time objective intelligibility measure for time-frequency
gorithm for on-line training of recurrent network trajectories,”
weighted noisy speech,” in Acoustics Speech and Signal Pro-
Neural computation, vol. 2, no. 4, pp. 490–501, 1990.
cessing (ICASSP), 2010 IEEE International Conference on.
[14] I. Sutskever, Training recurrent neural networks, University
IEEE, 2010, pp. 4214–4217.
of Toronto Toronto, Ontario, Canada, 2013.