Reconstruction in the Labeled Stochastic Block Model
Marc Lelarge Laurent Massouli´e Jiaming Xu ∗
February 12, 2015
5102 beF 11  ]LM.tats[  1v56330.2051:viXra
Abstract
The labeled stochastic block model is a random graph model representing networks with
community structure and interactions of multiple types. In its simplest form, it consists of two
communities of approximately equal size, and the edges are drawn and labeled at random with
probability depending on whether their two endpoints belong to the same community or not.
It has been conjectured in [16] that correlated reconstruction (i.e. identification of a partition
correlated with the true partition into the underlying communities) would be feasible if and
only if a model parameter exceeds a threshold. We prove one half of this conjecture, i.e.,
reconstruction is impossible when below the threshold. In the positive direction, we introduce
a weighted graph to exploit the label information. With a suitable choice of weight function,
we show that when above the threshold by a specific constant, reconstruction is achieved by
(1) minimum bisection, (2) a semidefinite relaxation of minimum bisection, and (3) a spectral
method combined with removal of edges incident to vertices of high degree. Furthermore,
we show that hypothesis testing between the labeled stochastic block model and the labeled
Erd˝os-R´enyi random graph model exhibits a phase transition at the conjectured reconstruction
threshold.
1 Introduction
1.1 Motivation
Community detection aims to identify underlying communities of similar characteristics in an overall
population from the observation of pairwise interactions between individuals [12, 24, 23]. The
stochastic block model, also known as planted partition model, is a popular random graph model
for analyzing the community detection problem [25, 28, 2, 27, 9], in which pairwise interactions
are binary: an edge is either present or absent between two individuals. In its simplest form,
the stochastic block model consists of two communities of approximately equal size, where the
within-community edge is present at random with probability p; while the across-community edge
is present with probability q. If p > q, it corresponds to assortative communities where interactions
are more likely within rather than across communities; while p < q corresponds to disassortative
communities.
In practice, interactions can be of various types and these types reveal more information on the
underlying communities than the mere existence of the interaction itself. For example, in recom-
mender systems, interactions between users and items come with user ratings. Such ratings contain
∗M. Lelarge is with INRIA-ENS, marc.Lelarge@ens.fr. L. Massouli´e is with the Microsoft Research-INRIA Joint
Centre, laurent.massoulie@inria.fr. J. Xu is with with the Department of ECE, University of Illinois at Urbana-
Champaign, Urbana, IL, jxu18@illinois.edu. Part of work is performed while J. Xu was an intern with Technicolor.
A preliminary version of this paper appeared in the Proceedings of the 2013 Information Theory Workshop.
1

far more information than the interaction itself to characterize the user and item types. Similarly,
protein-protein chemical interactions in biological networks can be exothermic and endothermic;
email exchanges in a club may be formal or informal; friendship in social networks may be strong or
weak. The labeled stochastic block model was recently proposed in [16] to capture rich interaction
types. In this model interaction types are described by labels drawn from an arbitrary collection.
In particular, for the simple two communities case, the within-community edge is labeled at ran-
dom with distribution µ; while the across-community edge is labeled with a different distribution
ν. In this context an important question is how to leverage the labeling information for detecting
underlying communities.
1.2 Information-Scarce Regime
In this paper, we focus on the sparse labeled stochastic block model in which every vertex has a
limited average degree, i.e., p,q = O(1/n), where n is the number of vertices. It corresponds to the
information-scarce regime where only O(n) edges and labels are observed in total1. This regime is
of practical interest, arising in several contexts. For example, in recommender systems, users only
give ratings to few items; in biological networks, only few protein-protein interactions are observed
due to cost constraints; in social networks, a person only has a limited number of friends.
For the stochastic block model in this information-scarce regime, there are Θ(n) isolated ver-
tices, as in Erd˝os-R´enyi random graphs with bounded average degree. For isolated vertices, it is
impossible to determine their community membership and thus exact reconstruction of communi-
ties is impossible. Therefore, we resort to finding a partition into communities positively correlated
to the true community partition (see Definition 1 below).
1.3 Main Results
Focusing on the two communities scenario, we show that a positively correlated reconstruction is
fundamentally impossible when below a threshold. This establishes one half of the conjecture in
[16]. In the positive direction, we establish the following results. We introduce a graph weighted
by a suitable function of observed labels, on which we show that:
(1) Minimum bisection gives a positively correlated partition when above the threshold by a
factor of 64ln2.
(2) A semidefinite relaxation of minimum bisection gives a positively correlated partition when
above the threshold by a factor of 217 ln2.
(3) A spectral method combined with removal of edges incident to vertices of high degree gives
a positively correlated partition when above the threshold by a constant factor.
Furthermore, we show that the labeled stochastic block model is contiguous to a labeled Erd˝os-
R´enyi random graph when below the reconstruction threshold and orthogonal to it when above
the threshold. It implies that for the hypothesis testing problem between the labeled stochastic
block model and the labeled Erd˝os-R´enyi random graph model, the correct identification of the
underlying distribution is feasible if and only if above the reconstruction threshold. It also implies
that there is no consistent estimator for model parameters when below the reconstruction threshold.
1.4 Related Work
For the stochastic block model, most previous work focuses on the “dense” regime with an average
degree diverging as the size of the graph n grows, (see, e.g., [4, 5] and the references therein).
1We also provide results for p,q = O(polylog(n)/n) in Theorem 4.
2

For the “sparse” regime with bounded average degrees, a sharp phase transition threshold for
reconstruction was conjectured in [9] by analyzing the belief propagation algorithm. The converse
part of the conjecture was rigorously proved in [22]. The achievability part is proved independently
in [21, 19]. In addition, it is shown in [6] that a variant of spectral method gives a positively
correlated partition when above the threshold by an unknown constant factor. More recently, it is
shown in [15] that a semidefinite program finds a correlated partition when above the threshold by
some large constant factor.
The labeled stochastic block was first proposed and studied in [16] and a new reconstruction
threshold that incorporates the extra labeling information was conjectured. Simulations further
indicate that the belief propagation algorithm works when above the threshold, but reconstruction
algorithms that provably work are still unknown.
Finally, we recently became aware of the work [1] that studies the problem of decoding binary
node labels from noisy edge measurements. In the case where the background graph is Erd˝os-
R´enyi random graph and each node label is independently and uniformly chosen from {±1}, the
model in [1] can be viewed as a special case of the labeled stochastic block model with p = q,
µ = (1 − (cid:15))δ + (cid:15)δ and ν = (cid:15)δ + (1 − (cid:15))δ , where δ denotes the probability measure
+1 −1 +1 −1 x
concentrated on point x (See Section 2 for the formal model description). When p = q = alogn/n
for some constant a and (cid:15) → 1/2, it is shown in [1] that exact recovery of node labels is possible if
and only if a(1 − 2(cid:15))2 > 2. In contrast, our results show that when p = q = a/n for some constant
a, correlated recovery of node labels is impossible if a(1 − 2(cid:15))2 < 1 for any 0 ≤ (cid:15) ≤ 1. Moreover,
we show that distinguishing hypothesis (cid:15) = (cid:15) and hypothesis (cid:15) = 1/2 is possible if and only if
0
a(1 − 2(cid:15) )2 > 1.
0
1.5 Outline
Section 2 introduces the precise definition of the labeled stochastic block model to be studied and
the key notations. The main theorems are introduced and briefly discussed in Section 3. The
detailed proofs are presented in Section 4. Section 5 ends the paper with concluding remarks.
Miscellaneous details and proofs are in the Appendix.
2 Model and Notation
This section formally defines the labeled stochastic block model with two symmetric communities
and introduces the key notations and definitions used in the paper. Let L denote a finite set.
The labeled stochastic block model G(n,p,q,µ,ν) is a random graph with n vertices of {±1} types
indexed by [n] and {(cid:96) ∈ L}-labeled edges. To generate a particular realization (G,L,σ), first
assign type σ ∈ {±1} to each vertex u uniformly and independently at random. Then, for every
u
vertex pair (u,v), independently of everything else, draw an edge between u and v with probability
p if σ = σ and with probability q otherwise. Finally, every edge e = (u,v) is labeled with (cid:96)
u v
independently at random with probability µ((cid:96)) if σ = σ and with probability ν((cid:96)) otherwise.
u v
Equivalently, we can specify G(n,p,q,µ,ν) by its probability distribution. Let

pµ(L ) if σ = σ ,(u,v) ∈ E(G),
 uv u v

 qν(L ) if σ (cid:54)= σ ,(u,v) ∈ E(G),
uv u v
φ (G,L,σ) =
uv
1 − p if σ = σ ,(u,v) ∈/ E(G),
 u v


1 − q if σ (cid:54)= σ ,(u,v) ∈/ E(G),
u v
3

where E(G) is the set of edges of G and L is the label on the edge (u,v). Then,
uv
(cid:89)
P −n
(G,L,σ) = 2 φ (G,L,σ). (1)
n uv
(u,v):u<v
When µ = ν, it reduces to the classical stochastic block model without labels. This paper focuses
on the sparse case where p = a/n and q = b/n for two fixed constants a and b, and the goal is
to reconstruct the true underlying types of vertices σ by observing the graph structure G and the
labels on edges L.
It is known that in the sparse graph, there are Θ(n) isolated vertices whose types clearly cannot
be recovered accurately. Therefore, our goal is to reconstruct a type assignment which is positively
correlated to the true type assignment. More formally, we adopt the following definition.
Definition 1. A type assignment σ is said to be positively correlated with the true type assignment
(cid:98)
σ if a.a.s.
1 1
Q(σ,σ) := − min{d(σ,σ),d(σ,−σ)} > 0, (2)
(cid:98) (cid:98) (cid:98)
2 n
where d is the Hamming distance, and Q is called the Overlap.
The shorthand a.a.s. denotes asymptotically almost surely. A sequence of events A holds a.a.s.
n
if the probability of A converges to 1 as n → ∞. Define τ as
n
(cid:18) (cid:19)2
a + b (cid:88) aµ((cid:96)) + bν((cid:96)) aµ((cid:96)) − bν((cid:96))
τ = . (3)
2 a + b aµ((cid:96)) + bν((cid:96))
(cid:96)∈L
It was conjectured in [16] that τ is the threshold for positively correlated reconstruction.
Conjecture 1. (i) If τ > 1, then it is possible to find a type assignment correlated with the
true assignement a.a.s.
(ii) If τ < 1, then it is impossible to find a type assignment correlated with the true assignement
a.a.s.
In this paper, we prove (ii) and propose three different algorithms able to find a type assignment
correlated with the true assignment for τ big enough.
Notation Let A denote the adjacency matrix of the graph G, I denote the identity matrix, and
J denote the all-one matrix. We write X (cid:23) 0 if X is positive semidefinite and X ≥ 0 if all the
entries of X are non-negative. For any matrix Y , let (cid:107)Y (cid:107) denote its spectral norm. For any positive
integer n, let [n] = {1,...,n}. For any set T ⊂ [n], let |T| denote its cardinality and Tc denote its
complement. We use standard big O notations, e.g., for any sequences {a } and {b }, a = Θ(b )
n n n n
or a (cid:16) b if there is an absolute constant c > 0 such that 1/c ≤ a /b ≤ c. Let Bern(p) denote
n n n n
the Bernoulli distribution with mean p and Binom(N,p) denote the binomial distribution with N
trials and success probability p. All logarithms are natural and we use the convention 0log0 = 0.
Rn,
For a vector x ∈ sign(x) gives the sign of x componentwise, and (cid:107)x(cid:107) denotes the L norm. For
2
a graph G, let V (G) denote its vertex set and E(G) denote its edge set.
4

3 Main Theorems
3.1 Minimum Bisection
To recover the community partition, one approach is via the maximum likelihood estimation. In
view of (1), the log-likelihood function can be written as:
(cid:20) (cid:18) (cid:19)(cid:21)
1 (cid:88) aµ(L ) ab
P uv
log (G,L|σ) = log σ σ + log µ(L )ν(L )
u v uv uv
2 bν(L ) n2
uv
(u,v)∈E(G)
(cid:20) (cid:18) (cid:19) (cid:21)
1 (cid:88) 1 − a/n
+ log σ σ + log((1 − a/n)(1 − b/n)) .
u v
2 1 − b/n
(u,v)∈/E(G)
(cid:80)
Under the constraint σ = 0, the maximum likelihood estimation is equivalent to
u u
(cid:20) (cid:21)
(cid:88) a(1 − b/n)µ(L )
uv
max log A σ σ
uv u v
σ b(1 − a/n)ν(L )
uv
(u,v)∈E(G)
(cid:88)
n
s.t. σ = 0, σ ∈ {±1} .
u
u
This is equivalent to the minimum bisection on the weighted graph with a specific weight function
a(1−b/n)µ((cid:96))
w((cid:96)) = log . For a general weighing function w : L → [−1,1], the minimum bisection
b(1−a/n)ν((cid:96))
finds a balanced bipartite subgraph in G with the minimum weighted cut, i.e.,
(cid:88)
min W
uv
σ
(u,v):σ (cid:54)=σ
u v
(cid:88)
s.t. σ = 0, σ ∈ {±1}, (4)
u u
u
where W = A w(L ) and A is the adjacency matrix of G.
uv uv uv
(cid:80) (cid:80)
Theorem 1. Assume the technical condition: aµ((cid:96))w2((cid:96)), bν((cid:96))w2((cid:96)) > 8ln2. Then if
(cid:96) (cid:96)
(cid:80) √
(aµ((cid:96)) − bν((cid:96)))w((cid:96))
(cid:96) > 128ln2, (5)
(cid:112)(cid:80)
(aµ((cid:96)) + bν((cid:96)))w2((cid:96))
(cid:96)
a.a.s. solutions of the minimum bisection (4) are positively correlated to the true type assignment
σ∗. Moreover, the left hand side of (5) is maximized when w((cid:96)) = aµ((cid:96))−bν((cid:96)) , in which case (5)
aµ((cid:96))+bν((cid:96))
reduces to τ > 64ln2.
3.2 Semidefinite relaxation method
The minimum bisection is known to be NP-hard in the worst case [14, Theorem 1.3]. In this section,
we present a semidefinite relaxation of the minimum bisection (4) which is solvable in polynomial
time, and show it finds an assignment correlated with the true assignment provided τ is large
(cid:80)
enough. Let Y = σσ(cid:62). Then σ = ±1 is equivalent to Y = 1, and σ = 0 if and only if
u uu u u
(cid:104)Y,J(cid:105) = 0. Therefore, (4) can be recast as
max (cid:104)W,Y (cid:105)
Y,σ
(cid:62)
s.t. Y = σσ
Y = 1, u ∈ [n]
uu
(cid:104)J,Y (cid:105) = 0. (6)
5

Notice that the matrix Y = σσ(cid:62) is a rank-one positive semidefinite matrix. If we relax this
condition by dropping the rank-one restriction, we obtain the following semidefinite relaxation of
(6):
Y(cid:98) = argmax (cid:104)W,Y (cid:105)
SDP
Y
s.t. Y (cid:23) 0
Y = 1, u ∈ [n]
uu
(cid:104)J,Y (cid:105) = 0. (7)
To get an estimator of the type assignment from Y(cid:98) , let y denote an eigenvector of Y(cid:98) corre-
√ SDP SDP
(cid:44)
sponding to the largest eigenvalue and (cid:107)y(cid:107) = n. The following result shows that σ sign(y)
(cid:98)SDP
is positively correlated with the true type assignment.
(cid:80)
Theorem 2. Assume the technical condition: w2((cid:96))(aµ((cid:96)) + bν((cid:96))) > 8ln2. If
(cid:96)
(cid:80) √
(aµ((cid:96)) − bν((cid:96)))w((cid:96))
(cid:96) > 512 ln2, (8)
(cid:112)(cid:80)
(aµ((cid:96)) + bν((cid:96)))w2((cid:96))
(cid:96)
then a.a.s. σ is positively correlated to the true type assignment σ∗. Moreover, the left hand
(cid:98)SDP
side of (8) is maximized when w((cid:96)) = aµ((cid:96))−bν((cid:96)) , in which case (8) reduces to τ > 217 ln2.
aµ((cid:96))+bν((cid:96))
In the stochastic block model without labels, i.e., µ = ν, condition (8) reduces to (a − b)2 >
218 ln2(a + b); similar conditions with a different constant have been proved in [15, Theorem 1.1]
using the Grothendieck’s inequality. Our proof builds upon the analysis in [15].
3.3 Spectral Method
In this section, we present a polynomial-time spectral algorithm based on the weighted adjacency
matrix W and show that this algorithm allows us to find an assignment correlated with the true
assignment provided τ is large enough.
Note that E [W|σ] = αJ + β σσ(cid:62) − α+β I with
n n n
1 (cid:88)
α = w((cid:96))(aµ((cid:96)) + bν((cid:96))),
2
(cid:96)
1 (cid:88)
β = w((cid:96))(aµ((cid:96)) − bν((cid:96))). (9)
2
(cid:96)
α+β
The term I is irrelevant to the main results (thanks to Weyl’s perturbation theorem) and
n
neglected for simplicity. Let D = W−αJ and then E [D|σ] = β σσ(cid:62) has rank one with singular value
n n
β. Hence, it makes sense to define D(cid:98) as the best rank-1 approximation of the matrix D. In other
(cid:80)
words, if D = v x x(cid:62) is the eigenvalue decomposition of D with eigenvalues |v | ≥ |v | ≥ ...,
i i i i 1 2
we define D(cid:98) = v x x(cid:62). Then if the matrix D is close to its mean E [D|σ] in the spectral norm,
1 1 1
we expect v to be close to β, and sign(x ) to be correlated with σ. Unfortunately, in the sparse
1 1
logn
regime, there are vertices of degree Ω( ) and thus the largest singular value of W could reach
loglogn
(cid:113)
logn
Ω( ) which is much higher than β. In order to take care of the issue, we begin with a
loglogn
preliminary step to clean the spectrum of W: we remove all edges incident to vertices in the graph
with degree larger than 3 a+b. To summarize, for a given weight function w((cid:96)), our algorithm
2 2
Spectral − Reconstruction has the following structure:
6

1. Remove edges incident to vertices with degree larger than 3 a+b and let G(cid:48) denote the resulting
2 2
graph. Define W(cid:48) to be the weighted adjacency matrix of G(cid:48).
2. Let x be the left-singular vector associated with the largest singular value of D(cid:48) = W(cid:48) − αJ,
(cid:98) n
i.e.,
(cid:62) (cid:48)
x = argmax{|x D x|, (cid:107)x(cid:107) = 1}. (10)
(cid:98)
Output sign(x) for the types of the vertices.
(cid:98)
Observe that (10) can be seen as a (non-convex) relaxation of the minimum bisection (4) by
(cid:80)
replacing the integer constraint with the unit-norm constraint and relaxing the constraint σ =
u u
0 to be a regularized term αx(cid:62)Jx in the objective function. Spectral − Reconstruction needs
n
estimates of α and a + b, which can be well approximated by 11(cid:62)W1 and 21(cid:62)A1, respectively.
n n
To simplify the analysis, we will assume that the exact values of α and a + b are known.
Theorem 3. Assume a > b > C for some sufficiently large constant C . There exists a universal
0 0
constant C (i.e. not depending on a, b, µ or ν) such that if β2 > C(a + b), where β is defined
in (9), then a.a.s. Spectral − Reconstruction outputs a type assignment correlated with the true
assignment. In the particular case, where w((cid:96)) = aµ((cid:96))−bν((cid:96)) , the condition β2 > C(a + b) reduces
aµ((cid:96))+bν((cid:96))
(cid:112)
to τ > C(a + b).
In the stochastic block model without labels, letting w((cid:96)) = 1, condition β2 > C(a+b) reduces
to (a−b)2 > 4C(a+b); the sharp condition (a−b)2 > 2(a+b) has been proved recently in [21, 19].
Compared to point (i) in the Conjecture 1, our result does not give the right order of magnitude
when a and b are large. Indeed, we are able to improve it if we allow a and b to grow with n.
6
Theorem 4. Assume that min(a,b) = Ω(log n). If
(cid:80)
[ (aµ((cid:96)) − bν((cid:96)))w((cid:96))]2
(cid:96) > 256, (11)
(cid:80)
(aµ((cid:96)) + bν((cid:96)))w2((cid:96))
(cid:96)
then Spectral − Reconstruction outputs a type assignment correlated with the true assignment a.a.s.
aµ((cid:96))−bν((cid:96))
Moreover, the left hand side of (11) is maximized when w((cid:96)) = , in which case (11) reduces
aµ((cid:96))+bν((cid:96))
to τ > 128. With this choice of w((cid:96)), as soon as τ → ∞, Spectral − Reconstruction outputs the
true assignment for all vertices except o(n) a.a.s.
6
Note that in the regime min(a,b) = Ω(log n), the degrees are very concentrated and step
1) of the algorithm can be removed without harm. The simulation results, depicted in Fig. 1,
further indicate that Spectral − Reconstruction leaving out step 1) outputs a positively correlated
assignment when above the threshold. In the simulation, we assume for simplicity only two labels:
r and b, and define µ(r) = 0.5 + (cid:15) and ν(r) = 0.5 − (cid:15). We generate the graph from the labeled
stochastic block model with n = 1000 vertices for various a,b,(cid:15). Fix a,b, we plot the overlap Q
against (cid:15) and indicate the threshold τ = 1 as a vertical dash line. All plotted values are averages
over 100 trials.
Note that our algorithm is most efficient when the parameters (a, b, µ and ν) of the model are
known as the optimal weight function depends on these parameters. In the case where the labels
are uninformative, i.e. µ = ν, our algorithm is very simple, does not require to know the values a
and b, and in the range of Theorem 4, has the best known performance guarantee (see [4, Table I]).
7

n=1000
1  
a=12,b=8
0.9
a=6,b=4
a=3,b=1
0.8
0.7
0.6
palrevO
0.5
0.4
0.3
0.2
0.1
0 
0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
ε
Figure 1: The overlap Q against (cid:15) from 0.05 to 0.5.
3.4 Converse Result
This section proves part (ii) of Conjecture 1. In particular, we show that when τ < 1, asymptotically
it is impossible to tell whether any two vertices are more likely to belong to the same community.
It further implies that reconstructing a positively correlated type assignment is fundamentally
impossible.
Theorem 5. If τ < 1, then for any fixed vertices ρ and v,
P
(σ = +1|G,L,σ = +1) → 1/2 a.a.s. (12)
n ρ v
Remark 1. Reconstructing a positively correlated type assignment is harder than telling whether
any two vertices are more likely to belong to the same community. In particular, given a positively
correlated type assignment σ, for two vertices randomly chosen, they are more likely to belong to
(cid:98)
the same community if they have the same type in σ.
(cid:98)
Theorem 5 is related to the Ising spin model in the statistical physics [10, 20], and it essentially
says that there is no long range correlation in the type assignment when τ < 1. The main idea in
the proof of Theorem 5 is borrowed from [22] and works as follows: (1) pick any two fixed vertices
ρ,v and consider the local neighborhood of ρ up to distance O(log(n)). The vertex v lies outside
of the local neighborhood of ρ a.a.s.. (2) conditional on the type assignment at the boundary of
the local neighborhood, σ is asymptotically independent with σ . (3) the local neighborhood of
ρ v
ρ looks like a Markov process on a labeled Galton-Watson tree rooted at ρ. (4) For the Markov
process on the labeled Galton-Watson tree, the types of leaves provide no information about the
type of the root ρ when the depth of tree goes to infinity.
3.5 Hypothesis Testing
Consider a labeled Erd˝os-R´enyi random graph G(n, a+b), where independently at random, each
2
pair of two vertices is connected with probability a+b, and every edge is labeled with (cid:96) ∈ L with
2
aµ((cid:96))+bν((cid:96)) P(cid:48)
probability . Let denote the distribution of the labeled Erd˝os-R´enyi random graph.
a+b n
8

| 0   | 1   | 2       |
|:----|:----|:--------|
| a   | a   | =12,b=8 |
| a   | a   | =6,b=4  |
| a   | a   | =3,b=1  |

| None   |    |    |
|:-------|:---|:---|
|        |    |    |

P P(cid:48)
Given a graph (G,L) which was drawn from either or , an interesting hypothesis testing
n n
problem is to decide which one is the underlying distribution of (G,L)? It turns out that when
τ > 1, the correct identification of the underlying distribution is feasible a.a.s.; however, when
τ < 1, one is bound to make error with non-vanishing probability.
P P(cid:48)
Theorem 6. If τ > 1, then and are asymptotically orthogonal, i.e., there exists event A
n n n
P P(cid:48)
such that (A ) → 1 and (A ) → 0.
n n n n
P P(cid:48)
If τ < 1, then and are contiguous, i.e., for every sequence of event A ,
n n n
P P(cid:48)
lim (A ) = 0 ⇔ lim (A ) = 0.
n n n n
n→∞ n→∞
Theorem 6 further implies the following corollary regarding the model parameter estimation.
Corollary 1. If τ < 1, then there is no consistent estimator for parameters a,b,µ,ν.
a b a b
Proof. The second part of Theorem 6 implies that G(n, 1, 1,µ ,ν ) and G(n, 2, 2,µ ,ν ) are
1 1 2 2
n n n n
contiguous as long as a µ ((cid:96)) + b ν ((cid:96)) = a µ ((cid:96)) + b ν ((cid:96)) and
1 1 1 1 2 2 2 2
(cid:88) (a µ ((cid:96)) − b ν ((cid:96)))2
i i i i
< 1,
2(a µ ((cid:96)) + b ν ((cid:96)))
i i i i
(cid:96)
a b a b
for i = 1,2. Therefore, one cannot distinguish between G(n, 1, 1,µ ,ν ) and G(n, 2, 2,µ ,ν )
1 1 2 2
n n n n
with the success probability converging to 1, and thus there is no consistent estimator for parameters
a,b,µ,ν.
In the special case where µ = ν, i.e., no labeling information is available, Theorem 6 reduces to
Theorem 2.4 in [22]. The positive part of Theorem 6 is proved by counting the number of labeled
short cycles and the second moment method. The negative part of Theorem 6 is proved using
the small subgraph conditioning method as introduced in [22]. The small subgraph conditioning
method was originally developed to show that random d-regular graphs are Hamiltonian a.s.s.
[26, 17].
4 Proofs
4.1 Proof of Theorem 1
Recall that σ∗ denotes the true type assignment. Since |{u : σ∗ = 1}| ∼ Binom(n,1/2), by Chernoff
u
bound, a.a.s.,
(cid:104) (cid:105)
(cid:112) (cid:112)
∗
|{u : σ = 1}| ∈ n/2 − nlogn,n/2 + nlogn . (13)
u
For ease of presentation, assume |{u : σ∗ = 1}| = n/2. Let m(σ) (cid:44) |{u : σ = +1,σ∗ = −1}|
u u u
and (cid:15) > 0 be an arbitrarily small constant. To prove the theorem, by the definition of positively
correlated reconstruction, it suffices to show that for all σ with n(1 − (cid:15)) ≤ m(σ) ≤ n,
4 4
(cid:88) (cid:88)
W − W := Y (σ) − Y (σ) > 0.
uv uv 1 2
(u,v):σ (cid:54)=σ , (u,v):σ =σ ,
u v u v
σ∗=σ∗ σ∗(cid:54)=σ∗
u v u v
9

To ease the notation, we suppress the argument σ. Observe that Y is a sum of 2m(n/2 − m) i.i.d.
1
random variables whose value is w((cid:96)) with probability aµ((cid:96)); Y is a sum of 2m(n/2 − m) i.i.d.
2
n
random variables whose value is w((cid:96)) with probability bν((cid:96)). Thus,
n
(cid:88)
E
y := [Y ] = 2m(n/2 − m)(a/n) µ((cid:96))w((cid:96)),
1 1
(cid:96)
(cid:88)
E
y := [Y ] = 2m(n/2 − m)(b/n) ν((cid:96))w((cid:96)).
2 2
(cid:96)
Define
(cid:88)
2
z := 2m(n/2 − m)(a/n) µ((cid:96))w ((cid:96)),
1
(cid:96)
(cid:88)
2
z := 2m(n/2 − m)(b/n) ν((cid:96))w ((cid:96)).
2
(cid:96)
Then, for 0 < λ ≤ 1,
2
(cid:34) (cid:35)2m(n/2−m)
a (cid:88)
E −λw((cid:96))
[exp(−λY )] = 1 + (e − 1)µ((cid:96))
1
n
(cid:96)
(cid:34) (cid:35)
a (cid:88)
−λw((cid:96))
≤ exp 2m(n/2 − m) (e − 1)µ((cid:96))
n
(cid:96)
(cid:34) (cid:35)
a (cid:88)(cid:0) (cid:1)
2 2
≤ exp 2m(n/2 − m) −λw((cid:96)) + 2λ w ((cid:96)) µ((cid:96))
n
(cid:96)
2
= exp(−λy + 2λ z ),
1 1
where the first inequality follows from the fact that 1+x ≤ ex and the second one follows from the
fact that ex ≤ 1 + x + 2x2 for |x| ≤ 1/2. The Chernoff bound gives that for 0 < λ ≤ 1,
2
P E
(Y ≤ (1 − t)y ) ≤ [exp(−λY )]exp((1 − t)λy )
1 1 1 1
2
≤ exp(−tλy + 2λ z ). (14)
1 1
We define E [W ] (cid:44) (cid:80) µ((cid:96))w((cid:96)) and E [W2] (cid:44) (cid:80) µ((cid:96))w2((cid:96)). Let t2 = (64ln2)1+(cid:15) 1 E[W µ2] and
µ (cid:96) µ (cid:96) 1 1−(cid:15) a (E[W ])2
µ
t y
λ = 1 1. We first check that with these values, we have λ ≤ 1/2:
4z
1
2E [W2]
1
µ
λ ≤ ⇔ t ≤
1 E
2 [W ]
µ
1 + (cid:15) 8ln2
E 2
⇔ ≤ [W ].
µ
1 − (cid:15) a
Thanks to the assumption made in Theorem 1, we can find (cid:15) sufficiently small such that this last
inequlity is valid. Notice that t2 1y 12 ≥ (1 + (cid:15))2nln2. It follows from (14) that
8z
1
(cid:18) t2y2(cid:19)
P 1 1 −n(1+(cid:15))
(Y ≤ (1 − t )y ) = exp − ≤ 2 .
1 1 1
8z
1
10

(cid:0)n/2(cid:1)(cid:0)n/2(cid:1)
Since there are ≤ 2n different σ with m(σ) = m, a simple union bound yields that as
m m
n → ∞,
P
(∃σ : (1 − (cid:15))n/4 ≤ m(σ) ≤ n/4,Y ≤ (1 − t )y ) → 0.
1 1 1
Similarly, let t2 = (64ln2)1+(cid:15) 1 E[W ν2] with E [W ] (cid:44) (cid:80) ν((cid:96))w((cid:96)) and E [W2] (cid:44) (cid:80) ν((cid:96))w2((cid:96)).
2 1−(cid:15) b (E[W ])2 ν (cid:96) ν (cid:96)
ν
Then
P
(∃σ : (1 − (cid:15))n/4 ≤ m(σ) ≤ n/4,Y ≥ (1 + t )y ) → 0.
2 2 2
With (cid:15) sufficiently small, a.a.s.
Y − Y ≥ (1 − t )y − (1 + t )y
1 2 1 1 2 2
(cid:114)
2m 1 + (cid:15) (cid:16)(cid:113) (cid:112) (cid:17)
= y − y − (n/2 − m) (64ln2) aE [W2] + bE [W2]
1 2 µ ν
n 1 − (cid:15)
(cid:32) (cid:114) (cid:33)
2m 1 + (cid:15) (cid:113)
(cid:0) (cid:1)
≥ (n/2 − m) aE [W ] − bE [W ] − (128ln2) aE [W2] + bE [W2]
µ ν µ ν
n 1 − (cid:15)
which is larger than zero as soon as (cid:15) is sufficiently small and (5) is satisfied.
By Cauchy-Schwartz inequality,
(cid:32) (cid:33)2
(cid:88) (cid:88)
2
(aµ((cid:96)) − bν((cid:96)))w((cid:96)) ≤ 2τ (aµ((cid:96)) + bν((cid:96)))w ((cid:96))
(cid:96) (cid:96)
aµ((cid:96))−bν((cid:96))
with equality achieved when w((cid:96)) = . This completes the proof.
aµ((cid:96))+bν((cid:96))
4.2 Proof of Theorem 2
Without loss of generality, assume (13) holds for σ∗. Let Y ∗ = σ∗(σ∗)(cid:62). By the optimality of Y(cid:98) ,
SDP
∗ E ∗ E ∗
0 ≤ (cid:104)W,Y(cid:98) (cid:105) − (cid:104)W,Y (cid:105) = (cid:104) [W],Y(cid:98) − Y (cid:105) + (cid:104)W − [W],Y(cid:98) − Y (cid:105). (15)
SDP SDP SDP
Since E [W] = αJ + β Y ∗ − α+β I with α,β defined in (9), and Y(cid:98) is a feasible solution to (6),
SDP
n n n
β α β
E ∗ ∗ ∗ ∗ ∗ ∗
(cid:104) [W],Y(cid:98) − Y (cid:105) = (cid:104)Y ,Y(cid:98) − Y (cid:105) − (cid:104)J,Y (cid:105) ≤ (cid:104)Y ,Y(cid:98) − Y (cid:105),
SDP SDP SDP
n n n
where the last inequality holds because (cid:104)J,Y ∗(cid:105) ≥ 0. In view of (15), it follows that
β
∗ ∗ E ∗
(cid:104)Y ,Y − Y(cid:98) (cid:105) ≤ (cid:104)W − [W],Y(cid:98) − Y (cid:105). (16)
SDP SDP
n
Notice that
(cid:16) (cid:17)
∗ 2 ∗ 2 2 ∗ 2 ∗ ∗ ∗
(cid:107)Y − Y(cid:98) (cid:107) = (cid:107)Y (cid:107) + (cid:107)Y(cid:98) (cid:107) − 2(cid:104)Y ,Y(cid:98) (cid:105) ≤ 2 n − (cid:104)Y ,Y(cid:98) (cid:105) = 2(cid:104)Y ,Y − Y(cid:98) (cid:105).
SDP F F SDP F SDP SDP SDP
It follows from (16) that
β
∗ 2 E ∗ E E ∗
(cid:107)Y − Y(cid:98) (cid:107) ≤ (cid:104)W − [W],Y(cid:98) − Y (cid:105) ≤ |(cid:104)W − [W],Y(cid:98) (cid:105)| + |(cid:104)W − [W],Y (cid:105)|. (17)
SDP F SDP SDP
2n
11

To upper bound |(cid:104)W − E [W],Y ∗(cid:105)|, Notice that
(cid:88)
E ∗ ∗ E
(cid:104)W − [W],Y (cid:105) = 2 Y (W − [W ]).
ij ij ij
i<j
Let σ2 = (cid:80) var[W ] = (1 + o(1))n (cid:80) w2((cid:96))(aµ((cid:96)) + bν((cid:96))). By the Bernstein inequality given
i<j ij 2 (cid:96)
in Theorem 8, for any t > 0,
 
(cid:12) (cid:12) √
 (cid:12)(cid:88) (cid:12) 2 
P (cid:12) Y ∗ (W − E [W ])(cid:12) ≥ 2σ2t + t ≤ 2e−t .
ij ij ij
(cid:12) (cid:12) 3
 
i<j
Letting t = logn = o(σ2), it follows that with probability at least 1 − 2n−1,
(cid:115)
(cid:12)(cid:88) (cid:12) (cid:88)
(cid:12) Y ∗ (W − E [W ])(cid:12) ≤ (1 + o(1)) nlogn w2((cid:96))(aµ((cid:96)) + bν((cid:96))),
ij ij ij
i<j (cid:96)
and thus |(cid:104)W − E [W],Y ∗(cid:105)| ≤ (2 + o(1))(cid:112) nlogn(cid:80) w2((cid:96))(aµ((cid:96)) + bν((cid:96))) with probability at least
(cid:96)
1 − 2n−1.
E
We bound |(cid:104)W − [W],Y(cid:98) (cid:105)| next. It follows from Grothendieck’s inequality [15, Theorem
SDP
3.4] that
E E E
|(cid:104)W − [W],Y(cid:98) (cid:105)| ≤ sup |(cid:104)W − [W],Y (cid:105)| ≤ K (cid:107)W − [W](cid:107) ,
SDP G ∞→1
Y(cid:23)0,diag{Y}=I
where K is an absolute constant known as Grothendieck constant and it is known that K <
G G
π √ ≤ 1.783. Moreover,
2ln(1+ 2)
E (cid:44) E (cid:62) E
(cid:107)W − [W](cid:107) sup (cid:107)(W − [W])x(cid:107) = sup x (W − [W])y
∞→1 1
x:(cid:107)x(cid:107) ≤1 x,y∈{±1}n
∞
(cid:88)
E
= sup (W − [W ])(x y + x y ).
ij ij i j j i
x,y∈{±1}n
i<j
For any fixed x,y ∈ {±1}n, using the Bernstein inequality, we have for any t > 0,
 
√
 (cid:88) 4 
P (W − E [W ])(x y + x y ) ≥ 8σ2t + t ≤ e−t .
ij ij i j j i
3
 
i<j
Hence, for arbitrarily small constant (cid:15) > 0, with probability at least 2−2(1+(cid:15))n,
 
(cid:115)
(cid:88) (cid:88) 8ln2(1 + (cid:15))
(W − E [W ])(x y + x y ) ≤ n 8ln2(1 + (cid:15)) w2((cid:96))(aµ((cid:96)) + bν((cid:96))) +
ij ij i j j i  
3
i<j (cid:96)
(cid:115)
(a) 4n (cid:88)
≤ 8ln2(1 + (cid:15)) w2((cid:96))(aµ((cid:96)) + bν((cid:96))),
3
(cid:96)
(cid:80)
where (a) follows from the technical assumption w2((cid:96))(aµ((cid:96)) + bν((cid:96))) > 8ln2. It follows from
(cid:96)
the union bound that with probability at least 1 − 4−(cid:15)n,
(cid:115)
4n (cid:88)
(cid:107)W − E [W](cid:107) ≤ 8ln2(1 + (cid:15)) w2((cid:96))(aµ((cid:96)) + bν((cid:96))).
∞→1
3
(cid:96)
12

In view of (17), with probability at least 1 − 4−(cid:15)n − 2n−1,
(cid:115)
1 8K (cid:88)
(cid:107)Y ∗ − Y(cid:98) (cid:107)2 ≤ (1 + o(1)) G 8ln2(1 + (cid:15)) w2((cid:96))(aµ((cid:96)) + bν((cid:96)))
n2 SDP F 3β
(cid:96)
(cid:112)(cid:80)
(a) w2((cid:96))(aµ((cid:96)) + bν((cid:96)))
(cid:112)
(cid:96)
≤ (1 + o(1))32 ln2(1 + (cid:15))
(cid:80)
w((cid:96))(aµ((cid:96)) − bν((cid:96))
(cid:96)
(b) 1
≤ (1 − (cid:15)) , (18)
16
√
where (a) follows by 2K ≤ 3 and the definition of β given in (9); (b) holds by invoking (8) and
G
letting (cid:15) be sufficiently small.
√
Recall that y is an eigenvector of Y(cid:98) corresponding to the largest eigenvalue and (cid:107)y(cid:107) = n.
SDP
By Davis-Kahan sinθ theorem stated in Lemma 5,
√ √
1 2 2(cid:107)Y(cid:98) − Y ∗(cid:107) 2 2(cid:107)Y(cid:98) − Y ∗(cid:107)
√ min{(cid:107)σ∗ − y(cid:107),(cid:107)σ∗ + y(cid:107)} ≤ SDP ≤ SDP F .
n n n
Note that for any x ∈ Rn, Hamming distance d(σ∗,sign(x)) ≤ (cid:107)σ∗ − x(cid:107)2. It follows that
1 8(cid:107)Y(cid:98) − Y ∗(cid:107)
∗ ∗ SDP F
min{d(σ ,sign(y)),d(σ ,sign(−y))} ≤ ,
n n2
and the theorem holds in view of (18).
4.3 Proof of Theorem 3
Recall that W(cid:48) is the weighted adjacency matrix after removal of edges incident to vertices with
high degrees and D(cid:48) = W(cid:48) − αJ. Define D(cid:99)(cid:48) as the best rank-1 approximation of D(cid:48) such that
n
D(cid:99)(cid:48) = v xx(cid:62) with (cid:107)x(cid:107) = 1. Recall that E [D|σ] = β σσ(cid:62). Applying Davis-Kahan sinθ theorem
1
n
restated in Lemma 5 with D(cid:48) and E [D|σ] gives:
√
σ σ 2 2
min{(cid:107)√ − x(cid:107),(cid:107)√ + x(cid:107)} ≤ (cid:107)D(cid:48) − E [D|σ](cid:107).
n n |β|
√
Since Hamming distance d(σ,sign(x) ≤ (cid:107)σ − nx(cid:107)2, it follows that
1 8 8
(cid:48) E 2 (cid:48) E 2
min{d(σ,sign(x),d(σ,−sign(x))} ≤ (cid:107)D − [D|σ](cid:107) = (cid:107)W − [W|σ](cid:107) . (19)
n β2 β2
√
Lemma 6 implies that a.a.s. (cid:107)W(cid:48) − E [W|σ](cid:107) ≤ C a + b for some universal positive constant C.
Hence, in view of (19), we get
1 a + b
2
min{d(σ,sign(x),d(σ,−sign(x))} ≤ 8C ,
(cid:98) (cid:98)
n β2
and the theorem follows.
13

4.4 Proof of Theorem 4
The proof follows the same steps as for Theorem 3, except that we are able to strengthen Lemma
6 thanks to a result of Vu [30]. Note that the variance of the elements of W is upper bounded by
1 (cid:80) w2((cid:96))(aµ((cid:96)) + bν((cid:96))) so that by Theorem 1.4 in [30], we get
n (cid:96)
Lemma 1. Under the conditions of Theorem 4, we have
(cid:115)
(cid:88)
(cid:107)W − E [W|σ](cid:107) ≤ 2 w2((cid:96))(aµ((cid:96)) + bν((cid:96))) a.a.s.
(cid:96)
4.5 Proof of Theorem 5
Consider a Galton-Watson tree T with Poisson offspring distribution with mean a+b. The type of
2
the root ρ is chosen from {±1} uniformly at random. Each child has the same type as its parent
with probability a and a different type with probability b . Every edge (u,v) is labeled at
a+b a+b
random with distribution µ if σ = σ and ν otherwise. Let T denote the Galton-Watson tree T
u v R
up to depth R and ∂T denote the set of leaves of T . Let G denote the subgraph of G induced
R R R
by vertices up to distance R from ρ and ∂G be the set of vertices at distance R from ρ.
R
The following lemma similar to Proposition 4.2 in [22] establishes a coupling between the local
neighborhood of ρ and the labeled Galton-Watson tree rooted at ρ.
logn
Lemma 2. Let R = R(n) = (cid:98) (cid:99), then there exists a coupling such that a.a.s.
10log(2(a+b))
(G ,L ,σ ) = (T ,L ,σ ),
R G G R T T
R R R R
where L and σ denote the labels and types on the subgraph G , respectively.
G G R
R R
Proof. See proof in Section C.
To ease notation, we omit the shorthand a.a.s. in the sequel. To prove Theorem 5, it suffices
to show that Var(σ |G,L,σ ) → 1. By the law of total variance,
ρ v
E E
Var(σ |G,L,σ ) = [Var(σ |G,L,σ ,σ )] + Var [ [σ |G,L,σ ,σ ]].
ρ v σ ρ v ∂G σ ρ v ∂G
∂G R R ∂G R R
Hence, it further reduces to show that Var(σ |G,L,σ ,σ ) → 1.
√ρ v ∂G
R
Let R be as in Lemma 2, then G = o( n) and thus v ∈/ G . Lemma 4.7 in [22] shows that σ
R R ρ
is asymptotically independent with σ conditionally on σ . Hence,
v ∂G
R
Var(σ |G,L,σ ,σ ) → Var(σ |G,L,σ ).
ρ v ∂G ρ ∂G
R R
Let Gc denote the subgraph of G induced by edges not in G , and L denote the set of labels
R R Gc
R
on Gc . Recall that V (G ) and V (Gc ) denote the set of vertices in G and Gc , respectively.
R R−1 R R−1 R
Let S (cid:44) V (G ) \ {ρ} and T (cid:44) V (Gc ) \ ∂G . Then {ρ} ∪ ∂G ∪ S ∪ T = V (G). Notice that
R−1 R R R
14

conditional on (G ,L ,σ ), σ is independent of (Gc ,L ). In particular,
R G R ∂G R ρ R Gc R
P
{σ |G ,L ,σ }
ρ R G ∂G
R R
(cid:80) P
{σ ,G,L,σ }
Gc R,L Gc ρ ∂G R
= R
(cid:80) P
{G,L,σ }
Gc R,L Gc ∂G R
R
(cid:16) (cid:17)(cid:16) (cid:17)
(cid:80) (cid:80) (cid:81) (cid:80) (cid:81) (cid:81)
φ φ φ
Gc R,L Gc σ S u,v∈V(G R):u<v uv σ T u,v∈T:u<v uv u∈∂G R,v∈T uv
= R
(cid:16) (cid:17)(cid:16) (cid:17)
(cid:80) (cid:80) (cid:80) (cid:81) (cid:80) (cid:81) (cid:81)
φ φ φ
Gc R,L Gc σ ρ σ S u,v∈V(G R):u<v uv σ T u,v∈T:u<v uv u∈∂G R,v∈T uv
R
(cid:16) (cid:17) (cid:16) (cid:17)
(cid:80) (cid:81) (cid:80) (cid:80) (cid:81) (cid:81)
φ φ φ
(a) σ S u,v∈V(G R):u<v uv Gc R,L Gc σ T u,v∈T:u<v uv u∈∂G R,v∈T uv
= R
(cid:16) (cid:17) (cid:16) (cid:17)
(cid:80) (cid:80) (cid:81) (cid:80) (cid:80) (cid:81) (cid:81)
φ φ φ
σ ρ σ S u,v∈V(G R):u<v uv Gc R,L Gc σ T u,v∈T:u<v uv u∈∂G R,v∈T uv
R
(cid:80) (cid:81)
φ
σ (u,v)∈V(G ):u<v uv
= S R
(cid:80) (cid:80) (cid:81)
φ
σ σ (u,v)∈G :u<v uv
ρ S R
(cid:16) (cid:17)(cid:16) (cid:17)
(cid:80) (cid:81) (cid:80) (cid:81) (cid:81)
φ φ φ
σ (u,v)∈V(G ):u<v uv σ u,v∈T:u<v uv u∈∂G ,v∈T uv
S R T R
=
(cid:16) (cid:17)(cid:16) (cid:17)
(cid:80) (cid:80) (cid:81) (cid:80) (cid:81) (cid:81)
φ φ φ
σ σ (u,v)∈V(G ):u<v uv σ u,v∈T:u<v uv u∈∂G ,v∈T uv
ρ S R T R
P
{σ ,G,L,σ }
= ρ ∂G R = P {σ |G,L,σ },
P {G,L,σ } ρ ∂G R
∂G
R
(cid:80) (cid:81)
where (a) holds because φ does not depend on Gc and L . It follows that
σ (u,v)∈V(G ):u<v uv R Gc
S R R
Var(σ |G,L,σ ) = Var(σ |G ,L ,σ ).
ρ ∂G ρ R G ∂G
R R R
Lemma 2 implies that
Var(σ |G ,L ,σ ) → Var(σ |T ,L ,σ ).
ρ R G ∂G ρ R T ∂T
R R R R
For the labeled Galton-Watson tree, it was shown in [16] that if τ < 1, the types of the leaves
provide no information about the type of the root when the depth R → ∞, i.e.,
1
P
(σ = +1|T,L,σ ) → .
ρ ∂T
R 2
Hence, Var(σ |T ,L ,σ ) → 1 and the theorem follows.
ρ R T ∂T
R R
4.6 Proof of Theorem 6
We introduce some necessary notations. For a graph G with n vertices and labeled edges, denote a
k-sequence of labels by [(cid:96)] = ((cid:96) ,(cid:96) ,...,(cid:96) ) ∈ Lk. A cycle in G is called a k-cycle with labels [(cid:96)] ,
k 1 2 k k
if starting from the vertex with the minimum index and ending at its neighbor with the smaller
index among its two neighbors, the sequence of labels on edges is given by [(cid:96)] . Let X ([(cid:96)] ) denote
k n k
the number of k-cycles with labels [(cid:96)] in G. Let (X) = X(X − 1)···(X − j + 1) for integers X
k j
and 1 ≤ j ≤ X. Then (X ([(cid:96)] )) is the number of ordered j-tuples of k-cycles with labels [(cid:96)] in
n k j k
(cid:81)
G. The product is assumed to taken over all possible sequences of labels with length k. The
[(cid:96)]
k
following lemma gives the asymptotic distribution of the number of k-cycles with labels [(cid:96)] .
k
15

Lemma 3. For any fixed integer m ≥ 3, {X ([(cid:96)] ) : [(cid:96)] ∈ Lk}m jointly converge to independent
n k k k=3
P(cid:48)
Poisson random variables with mean λ([(cid:96)] ) under graph distribution , and ξ([(cid:96)] ) under graph
k n k
P
distribution , where
n
k
1 (cid:89)
λ([(cid:96)] ) = (aµ((cid:96) ) + bν((cid:96) )),
k i i
2k+1k
i=1
(cid:32) (cid:33)
k k
1 (cid:89) (cid:89)
ξ([(cid:96)] ) = (aµ((cid:96) ) + bν((cid:96) )) + (aµ((cid:96) ) − bν((cid:96) )) .
k i i i i
2k+1k
i=1 i=1
We are ready to prove Theorem 6. The first part of Theorem 6 is proved using Lemma 3 and
(cid:80)
Chebyshev inequality. Define η([(cid:96)] ) = ξ([(cid:96)] )/λ([(cid:96)] ) − 1 and X = X([(cid:96)] )η([(cid:96)] ). Then, by
k k k k [(cid:96)] k k
k
Lemma 3, as n → ∞,
(cid:88)
E
[X ] = λ([(cid:96)] )η([(cid:96)] ),
P(cid:48) k k k
[(cid:96)]
k
(cid:88)
E
[X ] = λ([(cid:96)] )η([(cid:96)] )(1 + η([(cid:96)] )).
P k k k k
[(cid:96)]
k
Note that
(cid:32) (cid:33)k
k
(cid:88) (cid:88) (cid:89) (aµ((cid:96) ) − bν((cid:96) ))2 (cid:88) (aµ((cid:96)) − bν((cid:96)))2
2 s s k
2k λ([(cid:96)] )η ([(cid:96)] ) = = = τ . (20)
k k
2(aµ((cid:96) ) + bν((cid:96) )) 2(aµ((cid:96)) + bν((cid:96)))
s s
[(cid:96)] [(cid:96)] s=1 (cid:96)∈L
k k
Therefore,
(cid:88)
E E 2 k
[X ] − [X ] = λ([(cid:96)] )η ([(cid:96)] ) = τ /(2k),
P k P(cid:48) k k k
[(cid:96)]
k
and
(cid:88)
2 k
Var [X ] = λ([(cid:96)] )η ([(cid:96)] ) = τ /(2k),
P(cid:48) k k k
[(cid:96)]
k
(cid:88)
2 k
Var [X ] = ξ([(cid:96)] )η ([(cid:96)] ) ≤ τ /k.
P k k k
[(cid:96)]
k
Choose ρ = τk/(6k). By Chebyshev’s inequality,
Var [X ] 18k
P(cid:48) E P(cid:48) k
{X > [X ] + ρ} ≤ = .
k P(cid:48) k
ρ2 τk
E P(cid:48)-a.a.s..
Let k increases with n sufficiently slowly. Then since τ > 1, X ≤ [X ] + ρ Similarly,
k P(cid:48) k
E P E E E
X ≥ [X ]−ρ -a.a.s.. By definition of ρ, [X ]−ρ > [X ]+ρ. Set A = {X ≤ [X ]+ρ},
k P k P k P(cid:48) k n k P(cid:48) k
P(cid:48)(A P
then ) → 1 and (A ) → 0.
n n
The second part of Theorem 6 is proved using the following small subgraph conditioning theo-
rem, which is adapted from [17, Theorem 9.12].
P P P(cid:48)
Theorem 7. Let Y = n. If and are absolutely contiguous for any fixed n, and
n P(cid:48) n n
n
1. For each fixed m ≥ 3, {X ([(cid:96)] )}m converge jointly to independent Poisson variables with
n k k=3
P(cid:48) P
means λ([(cid:96)] ) > 0 under distribution , and ξ([(cid:96)] ) under distribution ;
k n k n
16

(cid:80) (cid:80)
2. λ([(cid:96)] )η([(cid:96)] )2 < ∞;
k≥3 [(cid:96)] k k
k
3. E [Y 2] → exp((cid:80) (cid:80) λ([(cid:96)] )η2([(cid:96)] )) as n → ∞,
P(cid:48) n k≥3 [(cid:96)] k k
n k
P P(cid:48)
Then, and are contiguous.
n n
P P(cid:48)
In this paper, and are discrete distributions on the space of labeled graphs, and for any
n n
P P(cid:48)
fixed n, and are absolutely continuous. Condition 1) is verified by Lemma 3. Condition 2)
n n
holds because in view of (20),
(cid:88)(cid:88) (cid:88) τk log(1 − τ) + τ + τ2/2
2
λ([(cid:96)] )η ([(cid:96)] ) = = − < ∞. (21)
k k
2k 2
k≥3 [(cid:96)] k≥3
k
We are left to verify condition 3). By definition,
(cid:88) (cid:89)
−n
Y (G,L) = 2 W (G,L,σ),
n u,v
σ∈{±1}n (u,v):u<v
where

2aµ(l)
if σ = σ ,(u,v) ∈ E(G),L = (cid:96),
  aµ((cid:96))+bν((cid:96)) u v uv

 2bν(l)
 if σ (cid:54)= σ ,(u,v) ∈ E(G),L = (cid:96),
u v uv
aµ((cid:96))+bν((cid:96))
W (G,L,σ) =
uv 1−a/n
if σ = σ ,(u,v) ∈/ E(G),
 u v
 1−(a+b)/(2n)

  1−b/n if σ (cid:54)= σ ,(u,v) ∈/ E(G),
u v
1−(a+b)/(2n)
Then,
(cid:88) (cid:89)
2 −2n
Y = 2 W (G,L,σ)W (G,L,δ). (22)
n u,v u,v
σ,δ∈{±1}n (u,v):u<v
Lemma 4. For any fixed σ,δ ∈ {±1}n, if σ σ = δ δ , then
u v u v
E 2 2 −3
[W (G,L,σ)W (G,L,δ)] = 1 + τ/n + (a − b) /(4n ) + O(n ).
P(cid:48) u,v u,v
n
Otherwise,
E 2 2 −3
[W (G,L,σ)W (G,L,δ)] = 1 − τ/n − (a − b) /(4n ) + O(n ).
P(cid:48) u,v u,v
n
Proof. Suppose σ σ = δ δ = 1. Then,
u v u v
E
[W (G,L,σ)W (G,L,δ)]
P(cid:48) u,v u,v
n
(cid:18) (cid:19)2 (cid:18) (cid:19)2 (cid:18) (cid:19)
(cid:88) 2aµ((cid:96)) aµ((cid:96)) + bν((cid:96)) 1 − a/n a + b
= + 1 −
aµ((cid:96)) + bν((cid:96)) 2n 1 − (a + b)/(2n) 2n
(cid:96)
1 (cid:88) 2a2µ2((cid:96)) (cid:16) a(cid:17)2 (cid:18) a + b (a + b)2 (cid:19)
−3
= + 1 − 1 + + + O(n )
n aµ((cid:96)) + bν((cid:96)) n 2n 4n2
(cid:96)
1 (cid:88)(cid:18) 2a2µ2((cid:96)) bν((cid:96)) − 3aµ((cid:96))(cid:19) (a − b)2
−3
= 1 + + + + O(n )
n aµ((cid:96)) + bν((cid:96)) 2 4n2
(cid:96)
1 (cid:88) (aµ((cid:96)) − bν((cid:96)))2 (a − b)2
−3
= 1 + + + O(n )
n 2(aµ((cid:96)) + bν((cid:96))) 4n2
(cid:96)
2 2 −3
= 1 + τ/n + (a − b) /(4n ) + O(n ). (23)
17

By symmetry, (23) holds for σ σ = δ δ = −1. Suppose σ = σ and δ (cid:54)= δ . Then,
u v u v u v u v
E
[W (G,L,σ)W (G,L,δ)]
P(cid:48) u,v u,v
n
(cid:18) (cid:19)
(cid:88) 4abµ((cid:96))ν((cid:96)) aµ((cid:96)) + bν((cid:96)) (1 − a/n)(1 − b/n) a + b
= + 1 −
(aµ((cid:96)) + bν((cid:96)))2 2n (1 − (a + b)/(2n))2 2n
(cid:96)
1 (cid:88) (aµ((cid:96)) − bν((cid:96)))2 (a − b)2
−3
= 1 − − + O(n )
n 2(aµ((cid:96)) + bν((cid:96))) 4n2
(cid:96)
2 2 −3
= 1 − τ/n − (a − b) /(4n ) + O(n ).
(cid:44) (cid:44)
In view of Lemma 4, letting S(σ,δ) {(u,v) : u < v,σ σ = δ δ } and T(σ,δ) {(u,v) : u <
u v u v
v,σ σ (cid:54)= δ δ }, and γ (cid:44) τ/n + (a − b)2/(4n2) + O(n−3), it follows from (22) that
u v u v n
E (cid:2) 2(cid:3) −2n (cid:88) |S(σ,δ)| |T(σ,δ)|
Y = 2 (1 + γ ) (1 − γ ) . (24)
P(cid:48) n n n
n
σ,δ∈{±1}n
Define ρ(σ,δ) = (cid:104)σ,δ(cid:105) and then |S(σ,δ)| = (n2 +ρ2)/4−n/2 and |T(σ,δ)| = (n2 −ρ2)/4. It follows
from (24) that
E (cid:2) 2(cid:3) n2/4−n/2 n2/4 −2n (cid:88) ρ2/4 −ρ2/4
Y = (1 + γ ) (1 − γ ) 2 (1 + γ ) (1 − γ ) . (25)
P(cid:48) n n n n n
n
σ,δ∈{±1}n
Taylor expansion yields
n2/4−n/2 n2/4 (cid:0) −1 (cid:1) (cid:2) 2 (cid:3)
(1 + γ ) (1 − γ ) = 1 + O(n ) exp −τ /4 − τ/2 ,
n n
(cid:20) ρ2 (cid:21)
ρ2/4 −ρ2/4 −1
(1 + γ ) (1 − γ ) = exp (τ/2 + O(n )) . (26)
n n
n
Combing (25) and (26), we get that
(cid:104) (cid:105)
E (cid:2) 2(cid:3) (cid:0) −1 (cid:1) (cid:2) 2 (cid:3) E Z2(τ/2+O(n−1))
P(cid:48) Y = 1 + O(n ) exp −τ /4 − τ/2 e n , (27)
n
n
where Z = √1 (cid:104)σ,δ(cid:105) and σ,δ are independently and uniformly distributed over {±1}n. Let Z
n
n
denote a standard Gaussian random variable. Then central limit theorem implies that Z converges
n
to Z in distribution. Since x → exp(x2τ/2) is a continuous mapping, exp(Z2τ/2) converges to
n
exp(Z2τ/2) in distribution. Moreover, {exp(Z2τ/2)} are uniformly bounded in L norm for
n 1+(cid:15)
some (cid:15) > 0 and thus uniformly integrable. In particular,
(cid:40) (cid:115) (cid:41)
(cid:90) ∞ (cid:90) ∞ 2lnt
(cid:2) (cid:3) (cid:8) (cid:9)
E 2 P 2 P
exp((1 + (cid:15))Z τ/2) = exp((1 + (cid:15))Z τ/2) > t dt = Z > dt
n n n
(1 + (cid:15))τ
0 0
(cid:90) ∞
(a) − 1 (b)
= t (1+(cid:15))τ dt < ∞,
0
where (a) follows from the Hoeffding’s inequality P {Z ≥ t} ≤ exp(−t2/2); (b) holds by choosing (cid:15)
n
sufficiently small such that (1+(cid:15))τ < 1. Hence, E [exp(Z2τ/2)] converges to E [exp(Z2τ/2)] = √ 1 .
n
1−τ
It follows from (27) that when τ < 1, as n → ∞,
exp−τ/2−τ2/4
(cid:2) (cid:3)
E Y 2 → √ .
P(cid:48)
n
n 1 − τ
Hence, in view of (21), condition 3) of Theorem 7 holds and the second part of Theorem 6 follows
from Theorem 7.
18

5 Conclusion
Our results show that when τ < 1 it is fundamentally impossible to give a positively correlated
reconstruction; when τ is large enough, the labeling information can be effectively exploited through
the suitably weighted graph. An interesting future work is to prove the positive part of Conjecture
1.
6 Acknowledgement
J. X. would like to thank Yudong Chen and Bruce Hajek for helpful conversations related to
this project. M. L. acknowledges the support of the French Agence Nationale de la Recherche
(ANR) under reference ANR-11-JS02-005-01 (GAP project). J. X. acknowledges the support of
the National Science Foundation under Grant ECCS 10-28464.
References
[1] E. Abbe, A. Bandeira, A. Bracher, and A. Singer. Decoding binary node labels from censored
edge measurements: Phase transition and efficient recovery. IEEE Transactions on Network
Science and Engineering, 1(1):10–22, Nov. 2014.
[2] P. J. Bickel and A. Chen. A nonparametric view of network models and newmangirvan and
other modularities. Proceedings of the National Academy of Sciences, 2009.
[3] B. Bollobas. Random Graphs. Cambridge University Press, 2001.
[4] Y. Chen, S. Sanghavi, and H. Xu. Clustering sparse graphs. Oct. 2012, available at:
http://arxiv.org/abs/1210.3335.
[5] Y. Chen and J. Xu. Finding a growing number of planted clusters and submatrices: funda-
mental limits and statistical-computational tradeoffs. arXiv:1402.1267, submitted to Journal
of Machine Learning Research. Short version appeard in Proceedings of The 31st International
Conference on Machine Learning (ICML), 2014.
[6] A. Coja-oghlan. Graph partitioning via adaptive spectral techniques. Comb. Probab. Comput.,
19(2):227–284.
[7] A. Coja-Oghlan, A. Goerdt, A. Lanka, and F. Sch¨adlich. Techniques from combinatorial
approximation algorithms yield efficient algorithms for random 2k-SAT. Theoret. Comput.
Sci., 329(1-3):1–45, 2004.
[8] C. Davis and W. M. Kahan. The rotation of eigenvectors by a perturbation. III. SIAM Journal
on Numerical Analysis, 7(1):pp. 1–46, 1970.
[9] A. Decelle, F. Krzakala, C. Moore, and L. Zdeborova. Asymptotic analysis of the stochas-
tic block model for modular networks and its algorithmic applications. Physics Review E,
84:066106, 2011.
[10] W. Evans, C. Kenyon, Y. Peres, and L. J. Schulman. Broadcasting on trees and the ising
model. The Annals of Applied Probability, 10(2):410–433, 2000.
19

[11] U. Feige and E. Ofek. Spectral techniques applied to sparse random graphs. Random Struct.
Algorithms, 27(2):251–275, Sept. 2005.
[12] S. Fortunato. Community detection in graphs. Jan. 2010, available at:
http://arxiv.org/abs/0906.0612.
[13] J. Friedman, J. Kahn, and E. Szemer´edi. On the second eigenvalue of random regular graphs.
In Proceedings of the twenty-first annual ACM symposium on Theory of computing, STOC ’89,
pages 587–598, New York, NY, USA, 1989. ACM.
[14] M. R. Garey, D. S. Johnson, and L. Stockmeyer. Some simplified NP-complete graph problems.
Theoret. Comput. Sci., 1(3):237–267, 1976.
[15] O. Gu´edon and R. Vershynin. Community detection in sparse networks via Grothendieck’s
inequality. arXiv:1411.4686,2014.
[16] S. Heimlicher, M. Lelarge, and L. li´e. Community detection in the labelled stochastic block
model. Nov. 2012, avaiable at: http://arxiv.org/abs/1209.2910.
[17] S. Janson, T. Luczak, and A. Rucinski. Random Graphs. Wiley Series in Discrete Mathematics
and Optimization. Wiley, 2011.
[18] R. Keshavan, A. Montanari, and S. Oh. Matrix completion from a few entries. IEEE Trans-
actions on Information Theory, 56(6):2980 –2998, June 2010.
[19] L. Massouli´e. Community detection thresholds and the weak Ramanujan property. In STOC
2014: 46th Annual Symposium on the Theory of Computing, pages 1–10, New York, United
States, June 2014.
[20] E. Mossel. Survey - information flows on trees. DIMACS series in discrete mathematics and
theoretical computer science, pages 155–170, 2004.
[21] E. Mossel, J. Neeman, and A. Sly. A proof of the block model threshold conjecture.
arXiv:1311.4115, 2013.
[22] E. Mossel, J. Neeman, and A. Sly. Stochastic block models and reconstruction. Feb. 2012,
available at: http://arxiv.org/abs/1202.1499.
[23] M. E. J. Newman. Modularity and community structure in networks. Proceedings of the
National Academy of Sciences, 103(23):8577–8582, 2006.
[24] M. E. J. Newman and M. Girvan. Finding and evaluating community structure in networks.
Phys. Rev. E, 69:026113, Feb 2004.
[25] S. L. Paul W. Holland, Kathryn Blackmond Laskey. Stochastic blockmodels: First steps.
Social Networks, 5(2):109–137, 1983.
[26] R. W. Robinson and N. C. Wormald. Almost all regular graphs are hamiltonian. Random
Strucr. Algorithms, 5(2):363–374, 1994.
[27] K. Rohe, S. Chatterjee, and B. Yu. Spectral clustering and the high-dimensional stochastic
blockmodel. The Annals of Statistics, 39(4):1878–1915, 2011.
20

[28] T. A. Snijders and K. Nowicki. Estimation and prediction for stochastic blockmodels for graphs
with latent block structure. Journal of Classification, 14(1):75–100, 1997.
[29] R. Vershynin. Introduction to the non-asymptotic analysis of random matrices.
arXiv:1011.3027, 2010.
[30] V. H. Vu. Spectral norm of random matrices. Combinatorica, 27(6):721–736, 2007.
A Special case of Davis-Kahan sin θ Theorem
The following lemma is Davis-Kahan sin θ theorem [8] specialized to the rank-1 setting. For
completeness, we restate the theorem and provide a proof.
Lemma 5. Let M = αxx(cid:62) and M(cid:48) = βyy(cid:62), with α,β ∈ R , (cid:107)x(cid:107) = (cid:107)y(cid:107) = 1 and x(cid:62)y ≥ 0. Then
√
2
(cid:48)
(cid:107)x − y(cid:107) ≤ (cid:107)M − M (cid:107).
max{|α|,|β|}
Furthermore, if M(cid:48) is the best rank-1 approximation of M(cid:102), then
√
2 2
(cid:107)x − y(cid:107) ≤ (cid:107)M − M(cid:102)(cid:107).
max{|α|,|β|}
Proof. First define θ ∈ [0,π/2] as x(cid:62)y = cosθ ≥ 0. Hence we have (cid:107)x − y(cid:107) = 2sin θ. Moreover
2
a simple calculation shows that min (cid:107)x − γy(cid:107) = sinθ and moreover for θ ∈ [0,π/2], we have
γ∈R
√ √
2sin θ ≤ sinθ. Hence we get (cid:107)x − y(cid:107) ≤ 2min (cid:107)x − γy(cid:107). Taking γ = β y(cid:62)x, then gives
γ
2 α
√ √
√
β 2 2
(cid:62) (cid:48) (cid:48)
(cid:107)x − y(cid:107) ≤ 2(cid:107)x − yy x(cid:107) = (cid:107)(M − M )x(cid:107) ≤ (cid:107)M − M (cid:107).
α |α| |α|
By symmetry, the first part of the lemma is proved. The second part of the lemma follows from
the fact that
(cid:48) (cid:48)
(cid:107)M − M (cid:107) ≤ (cid:107)M − M(cid:102)(cid:107) + (cid:107)M(cid:102)− M (cid:107) ≤ 2(cid:107)M(cid:102)− M(cid:107),
where the last inequality holds because M is of rank 1 and M(cid:48) is the best rank-1 approximation of
M(cid:102).
B Spectrum of Sparse Labeled Stochastic Block Model
Lemma 6. Assume a ≥ b > C for some sufficiently large constant C . There exists some absolute
0 0
constant C such that conditional on σ,
√
(cid:48) E
(cid:107)W − [W|σ](cid:107) ≤ C a + b, a.a.s.
For the special case of Erd˝os-R´enyi random graph, i.e., w((cid:96)) = 1 for all (cid:96) and a = b, Lemma 6
is proved in [11]. Our analysis is very similar to that given in [11] with small technical differences
due to the edge weights. We provide a formal proof below for completeness.
21

Proof. Define V be the (random) set of vertices remained and Vc denote the set of vertices removed.
(cid:0) (cid:1)
For every vertex, its degree is distributed as Binom n − 1, a+b . It is shown by [7][Lemma 39] that
2n
there exists a constant C > 0 such that a.a.s. |Vc| ≤ nexp(−C (a + b)). To prove the lemma, it
1 √ 1
suffices to show |x(cid:62)(W(cid:48) − E [W|σ])x| = O( a + b) for all x such that (cid:107)x(cid:107) = 1. The proof ideas
2
borrow from [13, 11, 18] and consists of three steps:
1. Reduce the problem by proving the same bound for x belonging to a discrete grid.
2. For the discrete grid, bound the contribution of light pairs (defined below) by applying a
union bound and a large deviation estimate.
3. Bound the contribution of heavy pairs using the bounded degree and the discrepancy prop-
erties (defined below) .
B.1 Reduction to a discrete grid
For any 0 < (cid:15) < 1, define a grid T which approximates the unit sphere Sn−1 = {x : lx(cid:107) = 1}:
(cid:15)
(cid:26) (cid:18) (cid:19)n (cid:27)
(cid:15)
T = x ∈ √ Z : (cid:107)x(cid:107) ≤ 1 .
(cid:15)
n
For every point x ∈ Sn−1, there exists some point y ∈ T such that (cid:107)x − y(cid:107) ≤ (cid:15). Therefore, T is
(cid:15) √ (cid:15)
an (cid:15)-net of Sn−1. Moreover, the hypercubes of side length (cid:15)/ n centered at the points in T are
(cid:15)
disjoint. On the other hand, all such hypercubes lie in the ball of radius (1 + (cid:15)/2) centered at the
1 √+o(1) (cid:0) 2π(cid:1)n/2
origin. Since the volume of a unit ball is ,
nπ n
√
(cid:18) (cid:19)n/2 (cid:18) (cid:19)n (cid:18) (cid:20) (cid:18) (cid:19) (cid:21)(cid:19)
1 + o(1) 2π (cid:16) (cid:15)(cid:17)n n 1 1 1
|T | ≤ √ 1 + = exp n log + + log(2π) + o(1) . (28)
(cid:15)
nπ n 2 (cid:15) 2 (cid:15) 2
Lemma 5.4 in [29] implies that
(cid:48) E (cid:62) (cid:48) E −1 (cid:62) (cid:48) E
(cid:107)W − [W|σ](cid:107) = sup |x (W − [W|σ])x| ≤ (1 − 2(cid:15)) sup |x (W − [W|σ])x|.
x∈Sn−1 x∈T
(cid:15)
Choosing (cid:15) = 1, we have (cid:107)W(cid:48) − E [W|σ](cid:107) ≤ 2sup |x(cid:62)(W(cid:48) − E [W|σ])x|. Hence, it suffices to
4 2 x∈T
1/4
bound sup |x(cid:62)(W(cid:48) − E [W])x|.
x∈T
1/4
B.2 Bounding the contribution of light pairs
Given an x ∈ T , directly applying the concentration inequality to x(cid:62)(W(cid:48) − E [W|σ])x, such as
1/4
(cid:44)
Bernstein’s inequality, does not give the desired result. Define the set of light pairs L {(u,v) :
x
√
a+b (cid:44)
u < v,|x x | < } and the set of heavy pairs H {(u,v) : u < v} \ L . Observe that
u v x x
n
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:88) (cid:12) (cid:12) (cid:88) (cid:12)
(cid:62) (cid:48) E (cid:48) (cid:62)E (cid:48)
sup |x (W − [W|σ])x| ≤ sup (cid:12) x W x − x [W|σ]x(cid:12) + sup (cid:12) x W x (cid:12).
u uv v u uv v
(cid:12) (cid:12) (cid:12) (cid:12)
x∈T x∈T x∈T
1/4 1/4 (u,v)∈L 1/4 (u,v)∈H
x x
We bound the contribution of heavy pairs separately in the next subsection. Recall that V denote
the set of vertices remained. Given V = V , define WV by setting to zero the rows and columns of
W corresponding to vertices removed, and define the event
 
(cid:12) (cid:12)
 √ 
(cid:12) (cid:88) (cid:12)
V (cid:62)E
E(V ) = sup (cid:12) x W x − x [W|σ]x(cid:12) > C a + b .
u uv v
(cid:12) (cid:12)
x∈T 
1/4 (u,v)∈L
x
22

Then
 
(cid:12) (cid:12)
 √ 
(cid:12) (cid:88) (cid:12)
P (cid:48) (cid:62)E P n P
sup (cid:12) x W x − x [W|σ]x(cid:12) > C a + b = {E(V)} ≤ 2 max {E(V )}. (29)
u uv v
x∈T (cid:12) (cid:12)  V
1/4 (u,v)∈L
x
Lemma 7 below, together with a union bound over all possible points x ∈ T and (28), implies
1/4
that for any positive constant C(cid:48), there exists a constant C > 0 such that P {E(V )} ≤ exp(−C(cid:48)n).
2 2
P
In view of (29) and a union bound, we conclude that {E(V)} is exponentially small by choosing
C large enough.
Lemma 7. Fix x ∈ T and V to be the set of vertices remaind. Define WV by setting to zero
1/4
(cid:80)
the rows and columns of W corresponding to vertices removed. Let X = x WV x −
(u,v)∈L u √uv v
x
x(cid:62)E |E
[W|σ]x. Assume a > b > C for some sufficiently large constant C . Then [X]| ≤ 2 a + b
0 0
and for any constant C > 0, there exists some constant C > 0 such that
2 3
(cid:110) √ (cid:111)
P E
|X − [X]| > C a + b ≤ exp(−C n).
3 2
Proof. Note that
(cid:88) α + βσ σ
E u vI (cid:62)E
[X] = x x − x [W|σ]y,
{u,v∈V} u v
n
(u,v)∈L
x
(cid:88) α + βσ σ (cid:88) α + βσ σ
u v u v I
= − x x − (1 − )x x .
u v {u,v∈V} u v
n n
(u,v)∈H (u,v)∈L
x x
Since α,β ≤ a + b, it follows that
a + b (cid:88) a + b (cid:88) (cid:88) (cid:88) (cid:88)
E
| [X]| ≤ |x y | + ( |x | |x | + |x | |x |). (30)
u v u v u v
n n
(u,v)∈H u∈/V v u v∈/V
x
Notice that |H |a+b ≤ (cid:80) x2x2 ≤ 1. Thus |H | ≤ n2 and by Cauchy-Schwartz inequality,
x n2 (i,j)∈H i j x a+b
x
 1/2
(cid:88) (cid:88) n
|x x | ≤ |H |1/2 x2 x2 ≤ √ .
u v x  u v
a + b
(u,v)∈H (u,v)∈H
x x
Again by Cauchy-Schwartz inequality,
 
(cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
c 1/2 2 2 c 1/2 −C (a+b)/2
|x u| |x v| + |x u| |x v| ≤ 2(n|V |)  x ux v ≤ 2(n|V |) ≤ 2ne 1 ,
u∈/V v u v∈/V u∈[n],v∈/V
where the last inequality follows because a.a.s. |Vc| ≤ nexp(−C (a + b)). It follows from (30) that
1
√ √
E −C (a+b)/2
| [X]| ≤ a + b + 2(a + b)e 1 ≤ 2 a + b,
where the last inequality holds when a ≥ C for a sufficiently large constant C .
0 0
E
Below we bound |X − [X]| using the Bernstein inequality. Define
I I
X = W x x .
uv uv u v {(u,v)∈L } {u,v∈V}
x
23

√
E (cid:80) E E a+b
Then X − [X] = 2 (X − [X ]). Note that |X − [X ]| ≤ and var(X ) ≤
u<v uv uv uv uv n uv
x2x2a+b. Therefore, var(X) ≤ a+b (cid:80) x2x2 = a+b. It follows from the Bernstein inequality that
u v n n u,v u v n
for any positive universal constant C > 0,
2
P(cid:26) (cid:12) (cid:12) (cid:112) 4C 2√ (cid:27)
E −C n
(cid:12)X − [X](cid:12) ≤ 2C (a + b) + a + b ≤ e 2 .
2
3
B.3 Bounding the contribution of heavy pairs
For the set of heavy pairs, since w((cid:96)) ∈ [−1,1], it follows that
(cid:88) (cid:88)
(cid:48) (cid:48)
sup | x W x | ≤ sup |x y |A , (31)
u uv v u v uv
x∈T x∈T
1/4 (u,v)∈H 1/4 (u,v)∈H
x
where A(cid:48) is defined by setting to zero the rows and columns of A corresponding to vertices removed.
We upper bound (31) by showing that the graph G(cid:48) with the adjacency matrix given by A(cid:48) satisfy
the following two properties.
Definition 2 (Bounded degree of order (d,c )). A graph is said to have bounded degree
4
property of order (d,c ) if every vertex has a degree bounded by c d for some universal constant
4 4
c > 1.
4
Definition 3 (Discrepancy of order (d,c ,c )). A graph is said to have discrepancy property
5 6
of order (d,c ,c ) if for every S,T ⊂ [n] with |T| > |S|, one of the following holds:
5 6
1. e(S,T) ≤ c ed|S||T|.
5
n
(cid:16) (cid:17)
2. e(S,T)log e(S,T)n ≤ c |S|log n ,
6
d|S||T| |T|
where e(S,T) denotes the set of edges between vertices in S and vertices in T.
Thanks to removal of edges incident to vertices with degree larger than 3 a+b, G(cid:48) satisfy the
2 2
(cid:0) (cid:1)
bound degree property of order a+b, 3 . In the case with |T| ≥ n, then
2 2 e
3(a + b) 3e(a + b)
e(S,T) ≤ |S| ≤ |S||T|,
4 4n
where the first inequality follows from the bounded degree property. Therefore, G(cid:48) satisfy the
discrepancy property with d = a + b and c = 3/4.
5
In the case with |T| < n, let G(cid:101) denote an Erd˝os-R´enyi random graph with n vertices and edge
e
probability (a + b)/n; there exists a coupling such that if (u,v) ∈ E(G), then (u,v) ∈ E(G(cid:101)). It
is shown in [11, Section 2.2.5] that with probability at least 1 − 1/n, G(cid:101) satisfies the discrepancy
property of order (a+b,c ,c ) for some constants c and c . Since removal of edges only decreases
5 6 5 6
e(S,T), G(cid:48) also satisfies the discrepancy property of order (a + b,c ,c ) with probability at least
5 6
1 − 1/n.
Applying [Corollary 2.11][11], we conclude that there exists some constant C such that
 
 (cid:88) √  1
P (cid:48)
sup |x x |A ≤ C a + b ≥ 1 −
i j uv
n
x∈T 
1/4 (u,v)∈H
x
The conclusion follows in view of (29) and (31).
24

C Proof of Lemma 2
We introduce some necessary notations for the labeled tree T. For a vertex v ∈ T, let Y denote
v
the number of children of v. Let Y = denote the number of children of v with the same type as v
v
and Y (cid:54)= = Y − Y =. By Poisson splitting property, Y = and Y (cid:54)= are independent Poisson random
v v v v v
variables with mean a/2 and b/2, respectively. Let Y (cid:96) denote the number of children of v with the
v
=,(cid:96)
edge connected to v being labeled with (cid:96). Let Y denote the number of children of v with the same
v
type as v and the edge connected to v being labeled with (cid:96) and Y (cid:54)=,(cid:96) = Y (cid:96) − Y =,(cid:96) . Then Y =,(cid:96) and
v v v v
(cid:54)=,(cid:96)
Y are independent Poisson random variables with mean (aµ((cid:96))/2) and (bν((cid:96))/2), respectively.
v
Similarly introduce the corresponding notations for G . Let V (G ) denote the set of vertices
R R
+1
of G and V = V \ V (G ). Let V denote the vertices of type +1 in V and similarly for
R R R R R
V −1 . For a vertex v ∈ ∂G , let X denote the number of children of v in V and X= denote
R R v R v
the number of children of v in V with the same type as v. Let X(cid:54)= = X − X=. Then, X= ∼
R v v v v
Binom(|V σ v|,a/n) and X(cid:54)= ∼ Binom(|V −σ v|,b/n). Let X(cid:96) denote the number of children of v in V
R v R v R
=,(cid:96)
with edge connected to v being labeled with (cid:96). Let X denote the number of children of v in V
v R
with the same type as v and the edge connected to v being labeled with (cid:96) and X(cid:54)=,(cid:96) = X(cid:96) − X=,(cid:96) .
v v v
=,(cid:96) σ (cid:54)=,(cid:96) −σ
Then, X ∼ Binom(|V v|,aµ((cid:96))/n) and X ∼ Binom(|V v|,bν((cid:96))/n). Note that it is possible
v v
R R
to have u,v ∈ ∂G which share the same child in V and thus G may not be a tree. The goal is
R R R
to show that such events are rare.
In particular, for any integer 1 ≤ r ≤ R, let A denote the event that no vertex in V has more
r r
than one parent in G . Let B denote the event that there are no edges within ∂G . Define an
r r r
event C as
r
s s
C = {|∂G | ≤ 2 (a + b) logn for all 0 ≤ s ≤ r},
r s
which is useful to establish that V is large enough so that the binomial distribution is close to
r
Poisson distribution. Lemma 4.4 and 4.5 in [22] show that for any r ≤ R,
P −3/4
(A |C ,σ) ≥ 1 − O(n ),
n r r
P −3/4
(B |C ,σ) ≥ 1 − O(n ),
n r r
P −log(4/e)
(C |C ,σ) ≥ 1 − n , (32)
n r+1 r
and |G | = O(n1/8) on C .
r r
We are ready to prove the proposition. Let V +1 and V −1 denote the set of vertices in V
with type +1 and −1, respectively. Then a.a.s. ||V +1| − |V −1|| ≤ n3/4 in view of (13). Suppose
that (G ,L ,σ ) = (T ,L ,σ ) and C holds. By (32), the event A , B and C hold
r G G r T T r r r r+1
r r r r
simultaneously with probability at least 1 − O(n−1/8) and |G | = O(n1/8). Note that if further
r
=,(cid:96) =,(cid:96) (cid:54)=,(cid:96) (cid:54)=,(cid:96)
X = Y and X = Y for every v ∈ ∂G and every (cid:96) ∈ L, then (G ,L ,σ ) =
v v v v r r+1 G G
r+1 r+1
(T ,L ,σ ).
r+1 T T
r+1 r+1
For each v ∈ ∂G , X=,l ∼ Binom(|V σ v|,aµ(l)/n), and
r v r
3/4 σ σ σ 3/4 1/8
n/2 + n ≥ |V v| ≥ |V v| ≥ |V v| − |G | ≥ n/2 − n − O(n ).
r r
Lemma 4.6 in [22] bounds total variation distance between binomial and Poisson random variables
as
(cid:18) (cid:19)
(cid:16) c (cid:17) max{1,|m − n|}
(cid:107)Binom m, − Pois(c)(cid:107) = O .
TV
n n
25

=,(cid:96) =,(cid:96) P =,(cid:96)
Therefore, for any fixed v ∈ ∂G and (cid:96) ∈ L, X can be coupled with Y such that {X (cid:54)=
r v v v
Y =,(cid:96) } = O(n−1/4) and similarly for X(cid:54)=,(cid:96) . Since |∂G | = O(n1/8) and L is a finite set, the union
v v r
=,(cid:96) =,(cid:96) (cid:54)=,(cid:96) (cid:54)=,(cid:96)
bound concludes that X = Y and X = Y for every v ∈ ∂G and every (cid:96) ∈ L with
v v v v r
probability at least 1 − O(n−1/8), Therefore
P(cid:8) (cid:12) (cid:9) 1
(G r+1,L ,σ ) = (T r+1,L ,σ ),C r+1(cid:12)(G r,L ,σ ) = (T r,L ,σ ),C ≥ 1 − O(n 8).
G G T T G G T T r
r+1 r+1 r+1 r+1 r r r r
By definition of condition probability,
P
{(G ,L ,σ ) = (T ,L ,σ ),C }
r+1 G G r+1 T T r+1
r+1 r+1 r+1 r+1
(cid:16) (cid:17)
−1/8 P
≥ 1 − O(n ) {(G ,L ,σ ) = (T ,L ,σ ),C }. (33)
r G G r T T r
r r r r
P
Since (C ) = 1, and G and T starts at the same root ρ, the proposition follows by recursively
0 R R
applying (33).
D Proof of Lemma 3
P(cid:48)
First consider the graph distribution . By the method of moments (see Theorem 6.10 [17]), it
n
P(cid:48)
suffices to show that under ,
n
 
m m
(cid:89) (cid:89) (cid:89) (cid:89)
E j([(cid:96)] )
 (X n([(cid:96)] k)) ) → (λ([(cid:96)] k)) k , (34)
j([(cid:96)]
k
k=1 [(cid:96)] k=1 [(cid:96)]
k k
E
for all possible non-negative integers {j([(cid:96)] )}. We first show that for any fixed [(cid:96)] , [X ([(cid:96)] )] →
k k n k
λ([(cid:96)] ).
k
Let v ,...,v be k distinct vertices among n vertices. Let I be the indicator that v ,...,v
0 k−1 0 k−1
is a k-cycle with labels [(cid:96)] . Then,
k
k
(cid:89) aµ((cid:96) ) + bν((cid:96) )
E s s
[I] = . (35)
2n
s=1
By the linearity of expectation,
(cid:18) (cid:19) (cid:18) (cid:19) k
(a) n (k − 1)! n (k − 1)! (cid:89) aµ((cid:96) s) + bν((cid:96) s)
E E
[X ([(cid:96)] )] = [I] = ,
n k
k 2 k 2 2n
s=1
(cid:0)n(cid:1)
where (a) holds because there are different choices of v ,...,v and k! different permutations
k 0 k−1
E
of them; each cycle corresponds to 2k different permutations. Therefore, [X ([(cid:96)] )] → λ([(cid:96)] ) as
√ n k k
n → ∞ as long as k = o( n).
Then, we argue that E [(X ([(cid:96)] )) ] → (λ([(cid:96)] ))j. Note that (X ([(cid:96)] )) is the number of ordered
n k j k n k j
j-tuples of k-cycles with labels [(cid:96)] in G. Divide these j-tuples into two sets: A is the set of j-tuples
k
for which all of the k-cycles are disjoint, and B is the set of the rest of the j-tuples.
Take (C ,C ,...,C ) ∈ A. Since C ’s are disjoint, they appear independently. By the previous
1 2 j i
argument, it follows that the cycles C ,...,C are all present in G with probability
1 j
j k
(cid:89) (cid:89) aµ((cid:96) ) + bν((cid:96) )
s s
.
2n
i=1s=1
26

(cid:0)n(cid:1) (kj)!
Since there are elements in A, the expected number of vertex-disjoint j-tuples of k-cycles
kj (2k)j
with [(cid:96)] is
k
(cid:18) (cid:19) j k
n (kj)! (cid:89) (cid:89) aµ((cid:96) ) + bν((cid:96) )
s s j
→ (λ([(cid:96)] )) .
k
kj (2k)j 2n
i=1s=1
˜ ˜
Let I be the number of non-vertex-disjoint j-tuples. Then the distribution of I is stochastically
˜ max{a,b}
dominated by the distribution of I under an Erd˝os-R´enyi random graph G(n, ). It is shown
n
1/4 E ˜
by [3][Corollary 4.4] that if k = O(log n), then [I] → 0 for any G(n,c/n) with constant c .
E [I˜ P(cid:48)
Hence, ] → 0 under .
n
Finally, note that the same argument applies to any joint factorial moment corresponding to
cycles with different lengths and labels. Thus equation (34) follows.
P P
Next consider the graph distribution . It suffices to show that under
n n
 
m m
(cid:89) (cid:89) (cid:89) (cid:89)
E j([(cid:96)] )
 (X n([(cid:96)] k)) ) → (ξ([(cid:96)] k)) k . (36)
j([(cid:96)]
k
k=1 [(cid:96)] k=1 [(cid:96)]
k k
E
We claim that for any fixed [(cid:96)] , [X ([l] )] → ξ([(cid:96)] ). Let v ,...,v be k distinct vertices among
k n k k 0 k−1
n vertices. Let I be the indicator that v ,...,v is a k-cycle with labels [(cid:96)] and v being the
0 k−1 k 0
(cid:0)n(cid:1)(k−1)!E
E
vertex with the minimum index. Let v = v , then [X ([(cid:96)] )] = [I] and
k 0 n k k 2
(cid:89) (cid:89)
E −k
[I|σ ,...,σ ] = n aµ((cid:96) ) bν((cid:96) ).
v v i i
0 k−1
1≤i≤k 1≤i≤k
σ vi−1=σ vi σ vi−1(cid:54)=σ vi
Notice that there are always even number of i such that σ (cid:54)= σ . Thus,
v v
i−1 i
(cid:32) (cid:33)
k k
(cid:89) (cid:89)
E E E −k
[I] = [ [I|σ]] = (2n) (aµ((cid:96) ) + bν((cid:96) )) + (aµ((cid:96) ) − bν((cid:96) )) .
σ i i i i
i=1 i=1
E
Therefore, [X ([(cid:96)] )] → ξ([(cid:96)] ) and by the same argument as before, equation (36) holds.
n k k
E Bernstein Inequality
Theorem 8. Let X ,...,X be independent random variables such that |X | ≤ M almost surely.
1 n i
(cid:80)n
Let σ2 = var(X ) and σ2 = σ2, then
i i i=1 i
(cid:40) (cid:41) (cid:32) (cid:33)
n
−t2
(cid:88)
P
X ≥ t ≤ exp .
i
2σ2 + 2Mt
i=1 3
It follows then
(cid:40) (cid:41)
n √
(cid:88) 2Mu
P X ≥ 2σ2u + ≤ e−u .
i
3
i=1
27