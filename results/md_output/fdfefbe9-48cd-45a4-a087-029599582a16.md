Text Classification based on Word Subspace
with Term-Frequency
Erica K. Shimomoto∗, Lincon S. Souza∗, Bernardo B. Gatto†, Kazuhiro Fukui∗
∗School of Systems and Information Engineering, University of Tsukuba, Japan
{erica,lincons}@cvlab.cs.tsukuba.ac.jp, kfukui@cs.tsukuba.ac.jp
†Center for Artificial Intelligence Research (C-Air), University of Tsukuba, Japan
bernard.gatto@gmail.com
Abstract—Text classification has become indispensable due to has limitations: first, feature vectors are commonly very high-
8102 nuJ 8  ]LM.tats[  1v52130.6081:viXra
the rapid increase of text in digital form. Over the past three dimensional, resulting in sparse document representations,
decades, efforts have been made to approach this task using
which are hard to model due to space and time complexity.
various learning algorithms and statistical models based on bag-
Second, BOW does not consider the proximity of words and
of-words (BOW) features. Despite its simple implementation,
their position in the text and consequently cannot encode the
BOW features lack of semantic meaning representation. To solve
this problem, neural networks started to be employed to learn words semantic meanings.
word vectors, such as the word2vec. Word2vec embeds word To solve these problems, neural networks have been em-
semantic structure into vectors, where the angle between vectors
ployed to learn vector representations of words [5]–[8]. In par-
indicates the meaningful similarity between words. To measure
ticular, the word2vec representation [9] has gained attention.
the similarity between texts, we propose the novel concept of
Given a training corpus, word2vec can generate a vector for
word subspace, which can represent the intrinsic variability of
features in a set of word vectors. Through this concept, it is each word in the corpus that encodes its semantic information.
possible to model text from word vectors while holding semantic These word vectors are distributed in such a way that words
information. To incorporate the word frequency directly in the
from similar contexts are represented by word vectors with
subspace model, we further extend the word subspace to the
high correlation, while words from different contexts are
term-frequency (TF) weighted word subspace. Based on these
represented by word vectors with low correlation.
new concepts, text classification can be performed under the
mutual subspace method (MSM) framework. The validity of One crucial aspect of the word2vec representation is that
our modeling is shown through experiments on the Reuters arithmetic and distance calculation between two word vectors
text database, comparing the results to various state-of-art
can be performed, giving information about their semantic
algorithms.
relationship. However, rather than looking at pairs of word
Index Terms—Text classification, Word subspace, Term-
vectors, we are interested in studying the relationship between
frequency, Subspace based methods
sets of vectors as a whole and, therefore, it is desirable to have
a text representation based on a set of these word vectors.
I. INTRODUCTION
To tackle this problem, we introduce the novel concept
Text classification has become an indispensable task due of word subspace. It is mathematically defined as a low
to the rapid growth in the number of texts in digital form dimensional linear subspace in a word vector space with high
available online. It aims to classify different texts, also called dimensionality. Given that words from texts of the same class
documents, into a fixed number of predefined categories, belong to the same context, it is possible to model word vectors
helping to organize data, and making easier for users to find of each class as word subspaces and efficiently compare them
the desired information. Over the past three decades, many in terms of similarity by using canonical angles between
methods based on machine learning and statistical models have the word subspaces. Through this representation, most of
been applied to perform this task, such as latent semantic anal- the variability of the class is retained. Consequently, a word
ysis (LSA), support vector machines (SVM), and multinomial subspace can effectively and compactly represent the context
naive Bayes (MNB). of the corresponding text. We achieve this framework through
The first step in utilizing such methods to categorize textual the mutual subspace method (MSM) [10].
data is to convert the texts into a vector representation. One The word subspace of each text class is modeled by ap-
of the most popular text representation models is the bag- plying PCA without data centering to the set of word vectors
of-words model [1], which represents each document in a of the class. When modeling the word subspaces, we assume
collection as a vector in a vector space. Each dimension of the only one occurrence of each word inside the class.
vectors represents a term (e.g., a word, a sequence of words), However, as seen in the BOW approach, the frequency of
and its value encodes a weight, which can be how many times words inside a text is an informative feature that should be
the term occurs in the document. considered. In order to introduce this feature in the word
Despite showing positive results in tasks such as language subspace modeling and enhance its performance, we further
modeling and classification [2]–[4], the BOW representation extend the concept of word subspace to the term-frequency

(TF) weighted word subspace. term can be a single word, constituting the conventional bag-
In this extension, we consider a set of weights, which en- of-words, or combinations of N words, constituting the bag-
codes the words frequencies, when performing the PCA. Text of-N-grams. If a term occurs in the document, its position in
classification with TF weighted word subspace can also be the vector will have a non-zero value, also known as term
performed under the framework of MSM. We show the validity weight. Two documents in the VSM can be compared to each
of our modeling through experiments on the Reuters1 database, other by taking the cosine distance between them [2].
an established database for natural language processing tasks. There are several ways to compute the term weights. Among
We demonstrate the effectiveness of the word subspace formu- them, we can highlight some: Binary weights, term-frequency
lation and its extension, comparing our methods’ performance (TF) weights, and term-frequency inverse document-frequency
to various state-of-art methods. (TF-IDF) weights.
|D|
The main contributions of our work are: Consider a corpus with documents D = {d } and a
i i=1
|V|
The introduction of the concept of word subspace, which vocabulary with all terms in the corpus V = {w } . The
• i i=1
is efficient to represent a text based on the word2vec term weights can be defined as:
representation. Binary weight: If a term occurs in the document, its
•
• An extension of word subspace to the term-frequency weight is 1. Otherwise, it is zero.
weighted word subspace, which is capable of incorporat- Term-frequency weight (TF): The weight of a term w is
•
ing word frequency information directly in the subspace defined by the number of times it occurs in the document
model. d.
• A comprehensive evaluation of the word subspace con- TF(w,d) = nw (1)
d
cept and its extension, verifying its effectiveness in
Inverse document-frequency: The weight of a term w,
representing sets of word vectors obtained through the •
given the corpus D, is defined as the total number of
word2vec.
documents |D| divided by the number of documents that
The remainder of this paper is organized as follows. In
have the term w, |Dw|.
Section II, we describe the main works related to text clas-
sification. In Section III, we present the formulation of our |D|
IDF(w|D) = (2)
proposed word subspace. In Section IV, we explain how text |Dw|
classification with word subspaces is performed under the
Term-frequency inverse document-frequency (TF-IDF):
•
MSM framework. Then, we present the TF weighted word
The weight of a term w is defined by the multiplication
subspace extension in Section V. Evaluation experiments and
of its term-frequency and its inverse document-frequency.
their results are described in Section VI. Further discussion
When considering only the TF weights, all terms have
is then presented in Section VII, and our conclusions are
the same importance among the corpus. By using the
described in Section VIII.
IDF weight, words that are more common across all
documents in D receive a smaller weight, giving more
II. RELATED WORK
importance to rare terms in the corpus.
In this section, we outline relevant work towards text
classification. We start by describing how text data is conven- TFIDF(w,d|D) = TF × IDF (3)
tionally represented using the bag-of-words model and then
In very large corpus, it is common to consider the
follow to describe the conventional methods utilized in text
logarithm of the IDF in order to dampen its effect.
classification.
TFIDF(w,d|D) = TF × log (IDF) (4)
10
A. Text Representation with bag-of-words
B. Conventional text classification methods
The bag-of-words representation comes from the hypothesis
that frequencies of words in a document can indicate the 1) Multi-variate Bernoulli and multinomial naive Bayes:
relevance of the document to a query [1], that is, if documents Multi-variate Bernoulli (MVB) and multinomial naive Bayes
and a query have similar frequencies for the same words, they (MNB) are two generative models based on the naive Bayes
might have a similar meaning. This representation is based assumption. In other words, they assume that all attributes
on the vector space model (VSM), that was developed for the (e.g., the frequency of each word, the presence or absence of
SMART information retrieval system [11]. In the VSM, the a word) of each text are independent of each other given the
main idea is that documents in a collection can be represented context of the class [12].
as a vector in a vector space, where vectors close to each other In the MVB model, a document is represented by a vector
represent semantically similar documents. generated by a bag-of-words with binary weights. In this case,
More formally, a document d can be represented by a vector a document can be considered an event, and the presence or
in Rn, where each dimension represents a different term. A the absence of the words to be the attributes of the event. On
the other hand, the MNB model represents each document as
1http://www.daviddlewis.com/resources/testcollections/reuters21578 a vector generated by a bag-of-words with TF weights. Here,

the individual word occurrences are considered as events and 2) Latent Semantic Analysis: Latent semantic analysis
the document is a collection of word events. (LSA), or latent semantic indexing (LSI), was proposed
Both these models use the Bayes rule to classify a docu- in [13], and it extends the vector space model by using singular
ment. Consider that each document should be classified into value decomposition (SVD) to find a set of underlying latent
one of the classes in C = {c }|C| . The probability of each variables which spans the meaning of texts.
j j=1
class given the document is defined as: It is built from a term-document matrix, in which each row
represents a term, and each column represents a document.
This matrix can be built by concatenating the vectors of
P(d |c )P(c )
i j j
P(c |d ) = . (5)
j i all documents in a corpus, obtained using the bag-of-words
P(d )
i
model, that is, X = [v ,v ,...,v ], where v is the vector
1 2 |D| i
representation obtained using the bag-of-words model.
The prior P(d ) is the same for all classes, so to determine
i
In this method, the term-document matrix is decomposed
the class to which d belongs to, the following equation can
i
using the singular value decomposition,
be used:
X = UΣV (cid:62), (11)
prediction(d ) = argmax P(d |c )P(c ) (6)
i c i j j
j where U and V are orthogonal matrices and correspond to
the left singular vectors and right singular vectors of X,
The prior P(c ) can be obtained by the following equation:
j
respectively. Σ is a diagonal matrix, and it contains the square
roots of the eigenvalues of XTX and XXT. LSA finds a
1 + |D |
j
P(c j) = , (7) low-rank approximation of X by selecting only the k largest
|C| + |D|
singular values and its respective singular vectors,
where |D | is the number of documents in class c .
j j
X = U Σ V (cid:62). (12)
As for the posterior P(d |c ), different calculations are k k k k
i j
performed for each model. For MVB, it is defined as:
To compare two documents, we project both of them into
this lower dimension space and calculate the cosine distance
|V|
(cid:89) )tk ))1−tk between them. The projection dˆ of document d is obtained by
P(d |c ) = P(w |c i (1 − P(w |c i , (8)
i j k j k j
the following equation:
k=1
dˆ = Σ−1U(cid:62)d. (13)
where w is the k-th word in the vocabulary V , and tk is the k k
k i
value (0 or 1) of the k-th element of the vector of document Despite its extensive application on text classification [14]–
d . [16], this method was initially proposed for document indexing
i
For the MNB, it is defined as: and, therefore, does not encode any class information when
modeling the low-rank approximation. To perform classifica-
(cid:89)|V| P(w k|c j)nk tion, 1-nearest neighbor is usually performed, placing a query
i
P(d |c ) = P(|d |)|d |! , (9)
i j i i document into the class of the nearest training document.
nk!
i
k=1
3) Support Vector Machine: The support vector machine
where |d | is the number of words in document d and nk is (SVM) was first presented in [17] and performs the separation
i i i
between samples of two different classes by projecting them
the k-th element of the vector of document d and it represents
i
onto a higher dimensionality space. It was first applied in text
how many times word w occurs in d .
k i
classification by [18] and have since been successfully applied
Finally, the posterior P(w |c ) can be obtained by the
k j
in many tasks related to natural language processing [19], [20].
following equation:
Consider a training data set D, with n samples
1 + |Dk|
P(w k|c j) = j , (10) D = {(x i,c i)|x i ∈ Rp,c i ∈ {−1,1}}n i=1, (14)
|C| + |D|
where c represents the class to which x belongs to. Each x is
i i i
where |Dk| is the number of documents in class c that contain a p-dimensional vector. The goal is to find the hyperplane that
j j
the word w . divides the points from c = 1 from the points from c = −1.
k i i
In general, MVB tends to perform better than MNB at This hyperplane can be written as a set of points x satisfying:
small vocabulary sizes whereas MNB is more efficient on large
vocabularies. w · x − b = 0, (15)
Despite being robust tools for text classification, both these
where · denotes the dot product. The vector w is perpendicular
models depend directly on the bag-of-words features and do
to the hyperplane. The parameter b determines the offset
not naturally work with representations such as word2vec. (cid:107)w(cid:107)
of the hyperplane from the origin along the normal vector w.

We wish to choose w and b, so they maximize the distance Text classification based on word subspace can be per-
between the parallel hyperplanes that are as far apart as formed under the framework of mutual subspace method
possible, while still separating the data. (MSM). This task involves two different stages: A learning
If the training data is linearly separable, we can select two stage, where the word subspace for each class is modeled, and
hyperplanes in a way that there are no points between them and a classification stage, where the word subspace for a query is
then try to maximize the distance. In other words, minimize modeled and compared to the word subspaces of the classes.
(cid:107)w(cid:107) subject to c (w · x − b) ≥ 1,i = {1,2,...,n}. If the In the learning stage, it is assumed that all documents of
i u
training data is not linearly separable, the kernel trick can be the same class belong to the same context, resulting in a set
{wk}N
applied, where every dot product is replaced by a non-linear of words W = c . This set assumes that each word
c c k=1
kernel function. appears only once in each class. Each set {W }|C| is then
c c=1
modeled into a word subspace Y , as explained in Section III.
III. WORD SUBSPACE c
As the number of words in each class may vary largely, the
All methods mentioned above utilize the BOW features to
dimension m of each class word subspace is not set to the
c
represent a document. Although this representation is simple
same value.
and powerful, its main problem lies on disregarding the word
In the classification stage, for a query document d , it is
q
semantics within a document, where the context and meaning
also assumed that each word occurs only once, generating a
could offer many benefits to the model such as identification
subspace Y .
q
of synonyms.
To measure the similarity between a class word subspace
In our formulation, words are represented as vectors in a
Y and a query word subspace Y , the canonical angles
real-valued feature vector space Rp, by using word2vec [9]. c q
between the two word subspaces are used [22]. There are
Through this representation, it is possible to calculate the
several methods for calculating canonical angles [23], [24],
distance between two words, where words from similar con-
and [25], among which the simplest and most practical is the
texts are represented by vectors close to each other, while
singular value decomposition (SVD). Consider, for example,
words from different contexts are represented as far apart
two subspaces, one from the training data and another from the
vectors. Also, this representation brings the new concept
query, represented as matrices of bases, Y = [Φ ...Φ ] ∈
c 1 m
of arithmetic operations between words, where operations c
Rp×m c and Y = [Ψ ...Ψ ] ∈ Rp×m q, where Φ are
q 1 m i
such as addition and subtraction carry meaning (eg., “king”- q
the bases for Y and Ψ are the bases for Y . Let the
c i q
“man”+“woman”=“queen”) [21].
SVD of Y (cid:62)Y ∈ Rm c×m q be Y (cid:62)Y = UΣV (cid:62), where
c q c q
Consider a set of documents which belong to the same m
Σ = diag(κ ,...,κ ), {κ } q represents the set of sin-
context D = {d }|D c| . Each document d is represented by 1 m c i i=1 m
c i i=1 i gular values. The canonical angles {θ } q can be obtained
N i i=1
a set of N words, d = {w } i . By considering that all
i i k k=1 as {cos−1(κ ),...,cos−1(κ )} (κ ≥ ... ≥ κ ). The
1 m 1 m
words from documents of the same context belong to the same q q
similarity between the two subspaces is measured by t angles
N
distribution, a set of words W = {w } c with the words in
c k k=1 as follows:
the context c is obtained.
We then translate these words into word vectors using
t
word2vec, resulting in a set of word vectors X = {xk}N c ∈ 1 (cid:88)
c c k=1 S [t] = cos2 θ , 1 ≤ t ≤ m , m ≤ m . (17)
Rp. (Y c,Y q) i q q c
This set of word vectors is modeled into a word subspace, t
i=1
which is a compact, scalable and meaningful representation of
Fig. 1 shows the modeling and comparison of sets of words
the whole set. Such a word subspace is generated by applying
by MSM. This method can compare sets of different sizes, and
PCA to the set of word vectors.
naturally encodes proximity between sets with related words.
First, we compute an autocorrelation matrix, R :
c
Finally, the class with the highest similarity with d is
q
N
1 (cid:88)c assigned as the class of d :
xixi(cid:62) q
R = . (16)
c N c c
c i=1 prediction(d ) = argmax (S ). (18)
q c (Y ,Y )
c q
The orthonormal basis vectors of m -dimensional subspace
c
V. TF WEIGHTED WORD SUBSPACE
Y are obtained as the eigenvectors with the m largest
c c
eigenvalues of the matrix R . We represent a subspace Y The word subspace formulation presented in Section III
c c
by the matrix Y ∈ Rp×m c, which has the corresponding is a practical and compact way to represent sets of word
c
vectors, retaining most of the variability of features. However,
orthonormal basis vectors as its column vectors.
as seen in the BOW features, the frequency of words is relevant
IV. TEXT CLASSIFICATION BASED ON WORD SUBSPACE information that can improve the characterization of a text. To
We formulate our problem as a single label classification incorporate this information into the word subspace modeling,
problem. Given a set of training documents, which we will re- we propose an extension of the word subspace, called the term-
|D| |C|
fer as corpus, D = {d } , with known classes C = {c } , frequency (TF) weighted word subspace.
i i=1 j j=1
we wish to classify a query document d into one of the classes Like the word subspace, the TF weighted word subspace is
q
in C. mathematically defined as a low-dimensional linear subspace

TABLE I
plants DOCUMENT DISTRIBUTION OVER THE CLASSES OF THE REUTERS-8
garden rose Class Number of samples
animals
acq 2292
flower tree
crude 374
zoo
earn 3923
park
lion grain 51
interest 271
money-fx 293
ship 144
trade 326
𝐶
1 𝑄
VI. EXPERIMENTAL EVALUATION
𝐶
2
In this section we describe the experiments performed to
demonstrate the validity of our proposed method and its
extension. We used the Reuters-8 dataset without stop words
from [28] aiming at single-label classification, which is a
𝜃 ,𝜃 ,…
1 2 preprocessed format of the Reuters-215782. Words in the
texts were considered as they appeared, without performing
stemming or typo correction. This database has eight different
Fig. 1. Comparison of sets of word vectors by the mutual subspace method.
classes with the number of samples varying from 51 to over
3000 documents, as can be seen in Table I.
in a word vector space with high dimensionality. However, a To obtain the vector representation of words, we used
weighted version of the PCA [26], [27] is utilized to incorpo- a freely available word2vec model3, trained by [9], on ap-
rate the information given by the frequencies of words (term- proximately 100 billion words, which encodes the vector
R300
frequencies). This TF weighted word subspace is equivalent representation in of over 3 million words from several
to the word subspace if we consider all occurrences of the different languages. Since we decided to focus on English
words. words only, we filtered these vectors to about 800 thousand
Consider the set of word vectors {xk}N c ∈ Rp, which words, excluding all words with non-roman characters.
c k=1
To show the validity of our word subspace representation for
represents each word in the context c, and the set of weights
N text classification and the proposed extension, we divided our
{ω } c , which represent the frequencies of the words in the
i i=1
experiment section into two parts: The first one aims to verify
context c.
if sets of word vectors are suitable for subspace representation,
We incorporate these frequencies into the subspace calcu-
and the second one puts our methods in practice in a text
lation by weighting the data matrix X as follows:
classification test, comparing our results with the conventional
methods described in Section II.
X(cid:102)= XΩ1/2, (19)
A. Evaluation of the word subspace representation
where X ∈ Rp×N c is a matrix containing the word vectors In this experiment, we modeled the word vectors from each
{xk}N
c and Ω is a diagonal matrix containing the weights class in the Reuters-8 database into a word subspace. The
c k=1
N
{ω i} i=c 1. primary goal is to visualize how much of the text data can be
We then perform PCA by solving the SVD of the matrix represented by a lower dimensional subspace.
X(cid:102): Subspace representations are very efficient in compactly
represent data that is close to a normal distribution. This
X(cid:102)= AMB(cid:62), (20)
characteristic is due to the application of the PCA, that is
optimal to find the direction with the highest variation within
where the columns of the orthogonal matrices A and B are,
the data.
respectively, the left-singular vectors and right-singular vectors
In PCA, the principal components give the directions of
of the matrix X(cid:102), and the diagonal matrix M contains singular
maximum variance, while their corresponding eigenvalues
values of X(cid:102).
give the variance of the data in each of them. Therefore,
Finally, the orthonormal basis vectors of the m -
c
by observing the distribution of the eigenvalues computed
dimensional TF weighted subspace W are the column vectors
when performing PCA in the modeling of the subspaces,
in A corresponding to the m largest singular values in M.
c
we can suggest if the data is suitable or not for subspace
Text classification with TF weighted word subspace can also
representation.
be performed under the framework of MSM. In this paper, we
will refer to MSM with TF weighted word subspace as TF-
2http://www.daviddlewis.com/resources/testcollections/reuters21578
MSM. 3https://code.google.com/archive/p/word2vec/

| plants   |
|:---------|
| garden   |
| flower   |

| rose   |
|:-------|
| tree   |
| park   |

| animals   |
|:----------|
| zoo       |
| lion      |

|    |
|:---|
|    |

TABLE II
Eigenvalues distribution
1 RESULTS FOR THE TEXT CLASSIFICATION EXPERIMENT ON REUTERS-8
DATABASE
Method Feature Accuracy (%) Std. Deviation
0.8 SA w2v 78.73 1.56
MSM w2v 90.62 0.42
s TF-MSM w2v 92.01 0.30
e
u
la MVB binBOW 62.70 0.69
v n e 0.6 MNB tfBOW 91.47 0.37
g
ie w2v 34.58 0.40
 d
e z binBOW 86.92 0.74
ila LSA
tfBOW 86.23 0.96
m 0.4
r o tfidfBOW 86.35 1.03
N
w2v 26.61 0.30
binBOW 89.23 0.24
SVM
0.2 tfBOW 89.10 0.29
tfidfBOW 88.78 0.40
0
20 40 60 80 100 120 140
using word2vec features, to which we refer as w2v. For
Eigenvalues order
MVB, due to its nature, only bag-of-words features with
binary weights were used (binBOW). For the same reason, we
Fig. 2. Eigenvalue distribution on word subspaces.
only used bag-of-words features with term-frequency weights
(tfBOW) with MNB. Classification with LSA and SVM is
For each class, we normalized the eigenvalues by the largest usually performed using bag-of-words features and, therefore,
one of the class. Fig. 2 shows the mean of the eigenvalues we tested with binBOW, tfBOW, and with the term-frequency
and the standard deviation among classes. It is possible to see inverse document-frequency weight, tfidfBOW. We also tested
that the first largest eigenvalues retain larger variance than them using word2vec vectors. In this case, we considered each
the smallest ones. In fact, looking at the first 150 largest word vector from all documents in each class to be a single
eigenvalues, we can see that they retain, on average, 86.37% of sample.
the data variance. Also, by observing the standard deviation,
To determine the dimensions of the class subspaces and
we can understand that the eigenvalues distribution among
query subspace of MSM and TF-MSM, and the dimension of
classes follows the same pattern, that is, most of the variance
the approximation performed by LSA, we performed a 10-fold
is in the first dimensions. This plot indicates that text data
cross validation, wherein each fold, the data were randomly
represented by vectors generated with word2vec is suitable
divided into train (60%), validation (20%) and test set (20%).
for subspace representation.
The results can be seen in Table II. The simplest baseline,
SA with w2v, achieved an accuracy rate of 78.73%. This result
B. Text classification experiment
is important because it shows the validity of the word2vec
In this experiment, we performed text classification among
representation, performing better than more elaborate methods
the classes in the Reuters-8 database. We compared the classi-
based on BOW, such as MVB with binBOW.
fication using the word subspace, and its weighted extension,
LSA with BOW features was almost 10% more accurate
based on MSM (to which we will refer as MSM and TF-MSM,
than SA, where the best results with binary weights were
respectively) with the baselines presented in Section II: MVB,
achieved with an approximation with 130 dimensions, with
MNB, LSA, and SVM. Since none of the baseline methods
TF weights were achieved with 50 dimensions, and with TF-
work with vector set classification, we also compared to a
IDF weights were achieved with 30 dimensions. SVM with
simple baseline for comparing sets of vectors, defined as the
BOW features was about 3% more accurate than LSA, with
average of similarities between all vector pair combinations of
binary weights leading to a higher accuracy rate.
two given sets. For two matrices A and B, containing the sets
It is interesting to note that despite the reasonably high
of vectors {xi }N A and {xi}N B, respectively, where N and
a i=1 b i=1 A accuracy rates achieved using LSA and SVM with BOW
N are the number of main words in each set, the similarity
B features, they poorly performed when using w2v features.
is defined as:
Among the baselines, the best method was MNB with
N N
1 (cid:88)A (cid:88)B tfBOW features, with an accuracy of 91.47%, being the
Sim = xi (cid:62) xj . (21)
(A,B) N N a b only conventional method to outperform MSM. MSM with
A B
i j
w2v had an accuracy rate of 90.62%, with the best results
We refer to this baseline as similarity average (SA). For this achieved with word subspace dimensions for the training
method, we only considered one occurrence of each word in classes ranging from 150 to 181, and for the query ranging
each set. from 3 to 217. Incorporating the frequency information in
Different features were used, depending on the method. the subspace modeling resulted in higher accuracy, with TF-
Classification with SA, MSM, and TF-MSM was performed MSM achieving 92.01%, with dimensions of word subspaces

| 0.8   |
|:------|
|       |
| 0.6   |
|       |
| 0.4   |
|       |
| 0.2   |

for training classes ranging from 150 to 172, and for the However, bag-of-words are commonly high dimensional mod-
query, ranging from 2 to 109. To confirm that TF-MSM is els, with a sparse representation, which is computationally
significantly more accurate than MNB, we performed a t-test heavy to model. Also, bag-of-words fail to convey the semantic
to compare their results. It resulted in a p-value of 0.031, meaning of words inside a text. Due to these problems, neural
which shows that at a 95% significance level, TF-MSM has networks started to be applied to generate a vector representa-
produced better results. tion of words. Despite the fact that these representations can
encode the semantic meaning of words, conventional methods
VII. DISCUSSION
do not work well when considering word vectors separately.
Given the observation of the eigenvalues distribution of In our work, we focused on the word2vec representation,
word vectors, we could see that word vectors that belong to which can embed the semantic structure of words, rendering
the same context, i.e., same class, are suitable for subspace vector angles as a useful metric to show meaningful similar-
representation. Our analysis showed that half of the word ities between words. Our experiments showed that our word
vector space dimensions suffice to represent most of the subspace modeling along with the MSM outperforms most
variability of the data in each class of the Reuters-8 database. of the conventional methods. Ultimately, our TF weighted
The results from the text classification experiment showed subspace formulation resulted in significantly higher accuracy
that subspace-based methods performed better than the text when compared to all conventional text classification methods
classification methods discussed in this work. Ultimately, our discussed in this work.
proposed TF weighted word subspace with MSM surpassed It is important to note that our method does not consider
all the other methods. word2vec features are reliable tools to the order of the words in a text, resulting in a loss of context
represent the semantic meaning of the words and when treated information. As a future work, we wish to extend our word
as sets of word vectors, they are capable of representing the subspace concept further in mainly two directions. First, we
content of texts. However, despite the fact that word vectors seek to encode word order, which may enrich the repre-
can be treated separately, conventional methods such as SVM sentation of context information. Second, we wish to model
and LSA may not be suitable for text classification using word dynamic context change, enabling analysis of large documents,
vectors. by having a long-short memory to interpret information using
Among the conventional methods, LSA and SVM achieved cues from different parts of a text.
about 86% and 89%, respectively, when using bag-of-words
features. Interestingly, both methods had better performance ACKNOWLEDGMENT
when using binary weights. For LSA, we can see that despite
This work is supported by JSPS KAKENHI Grant Number
the slight differences in the performance, tfidfBOW required
JP16H02842 and the Japanese Ministry of Education, Culture,
approximations with smaller dimensions. SVM had the lowest
Sports, Science, and Technology (MEXT) scholarship.
accuracy rate when using the tfidfBOW features. One possible
explanation for this is that TF-IDF weights are useful when
REFERENCES
rare words and very frequent words exist in the corpus, giving
higher weights for rare words and lower weights for common [1] G. Salton, A. Wong, and C. S. Yang, “A vector space model for
automatic indexing,” Communications of the ACM, vol. 18, no. 11, pp.
words. Since we removed the stop words, the most frequent
613–620, 1975.
words among the training documents were not considered and,
[2] P. D. Turney and P. Pantel, “From frequency to meaning: Vector space
therefore, using TF-IDF weights did not improve the results. models of semantics,” Journal of artificial intelligence research, vol. 37,
pp. 141–188, 2010.
Only MNB with tfBOW performed better than MSM. This
[3] M. Baroni and A. Lenci, “Distributional memory: A general framework
result may be because tfBOW features encode the word
for corpus-based semantics,” Computational Linguistics, vol. 36, no. 4,
frequencies, while MSM only considers a single occurrence pp. 673–721, 2010.
[4] S. Pado´ and M. Lapata, “Dependency-based construction of semantic
of words. When incorporating the word frequencies with our
space models,” Computational Linguistics, vol. 33, no. 2, pp. 161–199,
TF weighted word subspace, we achieved a higher accuracy of
2007.
92.01%, performing better than MNB at a significance level [5] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A neural proba-
bilistic language model,” Journal of machine learning research, vol. 3,
of 95%.
no. Feb, pp. 1137–1155, 2003.
[6] R. Collobert and J. Weston, “A unified architecture for natural language
VIII. CONCLUSIONS AND FUTURE WORK
processing: Deep neural networks with multitask learning,” in Proceed-
ings of the 25th international conference on Machine learning. ACM,
In this paper, we proposed a new method for text classi-
2008, pp. 160–167.
fication, based on the novel concept of word subspace under
[7] A. Mnih and G. E. Hinton, “A scalable hierarchical distributed language
the MSM framework. We also proposed the term-frequency model,” in Advances in neural information processing systems, 2009, pp.
weighted word subspace which can incorporate the frequency 1081–1088.
[8] J. Turian, L. Ratinov, and Y. Bengio, “Word representations: a simple
of words directly in the modeling of the subspace by using a
and general method for semi-supervised learning,” in Proceedings of the
weighted version of PCA. 48th annual meeting of the association for computational linguistics.
Most of the conventional text classification methods are Association for Computational Linguistics, 2010, pp. 384–394.
[9] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient estimation of
based on the bag-of-words features, which are very simple
word representations in vector space,” arXiv preprint arXiv:1301.3781,
to compute and had been proved to produce positive results. 2013.

[10] K. Fukui and A. Maki, “Difference subspace and its generalization for [19] S. Wang and C. D. Manning, “Baselines and bigrams: Simple, good
subspace-based methods,” IEEE Transactions on Pattern Analysis and sentiment and topic classification,” in Proceedings of the 50th Annual
Machine Intelligence, vol. 37, no. 11, pp. 2164–2177, 2015. Meeting of the Association for Computational Linguistics: Short Papers-
[11] G. Salton, “The smart retrieval system: experiments in automatic docu- Volume 2. Association for Computational Linguistics, 2012, pp. 90–94.
ment processing,” 1971. [20] E. Leopold and J. Kindermann, “Text categorization with support vector
[12] A. McCallum, K. Nigam et al., “A comparison of event models for machines. how to represent texts in input space?” Machine Learning,
naive bayes text classification,” in AAAI-98 workshop on learning for vol. 46, no. 1-3, pp. 423–444, 2002.
text categorization, vol. 752. Madison, WI, 1998, pp. 41–48. [21] T. Mikolov, W.-t. Yih, and G. Zweig, “Linguistic regularities in con-
[13] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and tinuous space word representations.” in hlt-Naacl, vol. 13, 2013, pp.
R. Harshman, “Indexing by latent semantic analysis,” Journal of the 746–751.
American society for information science, vol. 41, no. 6, p. 391, 1990. [22] F. Chatelin, Eigenvalues of Matrices: Revised Edition. SIAM, 2012.
[14] N. Ishii, T. Murai, T. Yamada, and Y. Bao, “Text classification by com- [23] H. Hotelling, “Relations between two sets of variates,” Biometrika,
bining grouping, lsa and knn,” in Computer and Information Science, vol. 28, no. 3,4, pp. 321–377, 1936.
2006 and 2006 1st IEEE/ACIS International Workshop on Component- [24] S. N. Afriat, “Orthogonal and oblique projectors and the characteristics
Based Software Engineering, Software Architecture and Reuse. ICIS- of pairs of vector spaces,” in Mathematical Proceedings of the Cam-
COMSAR 2006. 5th IEEE/ACIS International Conference on. IEEE, bridge Philosophical Society, vol. 53, no. 04. Cambridge Univ Press,
2006, pp. 148–154. 1957, pp. 800–816.
[15] G. Kou and Y. Peng, “An application of latent semantic analysis for text [25] K. Fukui and O. Yamaguchi, “Face recognition using multi-viewpoint
categorization,” International Journal of Computers Communications & patterns for robot vision,” Robotics Research, The Eleventh International
Control, vol. 10, no. 3, pp. 357–369, 2015. Symposium, ISRR, pp. 192–201, 2003.
[16] T. Cvitanic, B. Lee, H. I. Song, K. Fu, and D. Rosen, “Lda v. lsa: A [26] M. J. Greenacre, Theory and applications of correspondence analysis.
comparison of two computational text analysis tools for the functional London (UK) Academic Press, 1984.
categorization of patents.” in ICCBR Workshops, 2016, pp. 41–50. [27] I. Jolliffe, Principal Component Analysis. Springer Science & Business
[17] C. Cortes and V. Vapnik, “Support-vector networks,” Machine learning, Media, 2006.
vol. 20, no. 3, pp. 273–297, 1995. [28] A. Cardoso-Cachopo, “Improving Methods for Single-label Text Cate-
[18] T. Joachims, “Text categorization with support vector machines: Learn- gorization,” Ph.D. dissertation, Instituto Superior Tecnico, Universidade
ing with many relevant features,” Machine learning: ECML-98, pp. 137– Tecnica de Lisboa, 2007.
142, 1998.