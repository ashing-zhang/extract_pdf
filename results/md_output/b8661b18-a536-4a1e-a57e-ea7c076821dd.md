Fast clustering for scalable statistical analysis on structured images
Bertrand Thirion BERTRAND.THIRION@INRIA.FR
Parietal team, INRIA, Saclay and CEA, Neurospin France
Andre´s Hoyos-Idrobo
Parietal team, INRIA, Saclay and CEA, Neurospin France
5102 voN 61  ]LM.tats[  1v89840.1151:viXra
Jonas Kahn
Laboratoire Paul Painleve´ (UMR 8524), Universite´ de Lille 1, CNRS Cite´ Scientifique–Baˆt. M2, 59655 Villeneuve dAscq
Cedex France
Gae¨l Varoquaux
Parietal team, INRIA, Saclay and CEA, Neurospin France
Abstract moving high-frequency noise, actually improv-
ing subsequent estimations steps. As a conse-
The use of brain images as markers for dis-
quence, the proposed approach yields very ac-
eases or behavioral differences is challenged by
curate models on several large-scale problems
the small effects size and the ensuing lack of
yet with impressive gains in computational ef-
power, an issue that has incited researchers to
ficiency, making it possible to analyze large
rely more systematically on large cohorts. Cou-
datasets.
pled with resolution increases, this leads to very
large datasets. A striking example in the case
of brain imaging is that of the Human Connec- 1. Introduction
tome Project: 20 Terabytes of data and grow-
ing. The resulting data deluge poses severe chal- Big data in brain imaging. Medical images are increas-
lenges regarding the tractability of some pro- ingly used as markers to predict some diagnostic or be-
cessing steps (discriminant analysis, multivari- havioral outcome. As the corresponding biomarkers can
ate models) due to the memory demands posed be tenuous, researchers have come to rely more systemati-
by these data. In this work, we revisit dimen- cally on larger cohorts to increase the power and reliability
sion reduction approaches, such as random pro- of group studies (see e.g. (Button et al., 2013) in the case
jections, withtheaimofreplacingcostlyfunction of neuroimaging). In addition, the typical resolution of im-
evaluations by cheaper ones while decreasing the ages is steadily increasing, so that datasets become larger
memory requirements. Specifically, we investi- both in the feature and the sample dimensions. A strik-
gate the use of alternate schemes, based on fast ing example in the brain imaging case is that of the Human
clustering, that are well suited for signals exhibit- ConnectomeProject(HCP):20Terabytesofdataandgrow-
ing a strong spatial structure, such as anatomi- ing. The whole field is thus presently in the situation where
cal and functional brain images. Our contribu- very large datasets are assembled.
tion is two-fold: i) we propose a linear-time clus-
tering scheme that bypasses the percolation is- Computational bottlenecks. This data deluge poses se-
sues inherent in these algorithms and thus pro- vere challenges regarding the tractability of statistical pro-
vides compressions nearly as good as traditional cessing steps (components extraction, discriminant anal-
quadratic-complexity variance-minimizing clus- ysis, multivariate models) due to the memory demands
tering schemes; ii) we show that cluster-based posed by the data representations involved. For instance,
compression can have the virtuous effect of re- given a problem with n samples and p dimensions the
most classical linear algorithms (such as Principal compo-
Proceedings of the 32nd International Conference on Machine nents analysis) have complexity O(min(p2n,n2p), which
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copy- becomes exorbitant when both p and n large. In med-
right 2015 by the author(s).
ical imaging, p would be the number of voxels (shared

Fast clustering for statistics
across images, assuming that a prior alignment has been • Showing that, unlike random projections, this ap-
performed) and n the number of samples: while p is e.g. of proach actually has a denoising effect, that can be in-
the order of 105 − 106 for brain images at the 1 − 2mm terpreted as anisotropic smoothing of the data.
resolution, n is now becoming larger (106 in the case of the
HCP dataset). The impact on computational cost is actually
2. Theory
worse than a simple linear effect: as datasets no longer fit
in cache, the power of standard computational architectures Accuracy of random projections. An important charac-
can no longer be leveraged, resulting in an extremely inef- teristic of random projections is the existence of theorems
ficient use of resources. As a result, practitioners are left that guarantee the accuracy of the projection, in particu-
with the alternative of simplifying their analysis framework lar the Johnson-Lindenstrauss lemma (Johnson & Linden-
or working on sub-samples of the data (see e.g. (Zalesky strauss, 1984) and its variants:
et al., 2014)).
Given 0 < ε < 1, a set X of n points in Rp, and a number
k > 8log(n) , there is a linear map f : Rp −→ Rk such that
Lossy compression via random projections and cluster- ε2
ing. Part of the solution to this issue is to reduce the di-
(1−ε)(cid:107)x −x (cid:107)2 ≤ (cid:107)f(x )−f(x )(cid:107)2 ≤ (1+ε)(cid:107)x −x (cid:107)2
1 2 1 2 1 2
mensionality of the data. Principal components analysis,
(1)
or even its randomized counterpart (Halko et al., 2009), is
for all (x ,x ) ∈ X × X. The map f is simply taken
1 2
no longer an option, because these procedures become in-
as the projection to a random k-dimensional subspace with
efficient due to cache size effects. Non-linear data repre-
rescaling. The interpretation is that, given a large enough
sentations (multi-dimensional scaling, Isomap, Laplacian
number of random projections of a given dataset, one can
eigenmaps...) suffer from the same issue. However, more
obtain a faithful representation with explicit control on the
aggressive reductions can be obtained with random projec-
error. This accurate representation (in the sense of the (cid:96)
2
tions, i.e. the construction of random representations of the
norm) can then be used for further analyses, such as kernel
dataset in a k−dimensional space, k (cid:28) p. An essential
methods that consider between-sample similarities (see e.g.
virtue of random projections is that they come with some
(Rahimi &Recht, 2007)). In addition, the numberof neces-
guarantees on the reconstruction accuracy (see next sec-
sary projections can be lowered if the data are actually sam-
tion). An important drawback is that the projected data
Rp
pled from a sub-manifold of (Baraniuk & Wakin, 2009).
can no longer be embedded back in the native observa-
In practice, sparse random projections are used to reduce
tion space. Moreover, random projections are a generic
the memory requirements and increase their efficiency (Li
approach that does not take into account any relevant infor-
et al., 2006).
mationontheproblemathand: forinstance, theyignorethe
spatially continuous structure of the signals in medical im- There are two important limitations to this approach: i) the
Rp Rk
ages. By contrast, spatially- and contrast-aware compres- random mapping from to cannot be inverted in gen-
sionschemesareprobablybettersuitedformedicalimages. eral, due to its high dimensionality; this means that the en-
We propose here to investigate adapted clustering proce- suing inference steps cannot be made explicit in original
dures that respect the anatomical outline of image struc- data space; ii) this approach ignores the structure of the
tures. In practice, however, standard data-based clustering data, such as the spatial continuity (or dominance of low
(k-means, agglomerative) yield computationally expensive frequencies) in medical images.
estimation procedures. Alternatively, fast clustering proce-
dures suffer from percolation (where a huge cluster groups Signal versus noise. By contrast, clustering techniques
mostofthevoxels, whilemanysmallclustersareobtained). have been used quite frequently in medical imaging as a
means to compress information, with empirical success yet
Our contribution Here we propose a novel approach for in the absence of formal guarantees, as in super-voxel ap-
fast image compression, based on spatial clustering of the proaches (Heinrich et al., 2013). The explanation is that
data. This approach is designed to solve percolation issues medical images are typically composed of signal and noise,
encountered in these settings, in order to guarantee a good such that the high-frequency noise is reduced by within-
enough clustering quality. Our contributions are: cluster averaging, while the low-frequency signal of inter-
est is preserved. If we denote an image, the associated
• Designing a novel fast (linear-time) clustering algo- signal and noise by x, s and n, and by (u ) a set of
i i∈[k]
rithm on a 3D lattice (image grid) that avoids percola- projectors to k clusters:
tion.
x = s + n =⇒ (cid:104)x,u (cid:105) = (cid:104)s,u (cid:105) + (cid:104)n,u (cid:105) ∀i ∈ [k]
i i i
• Showing that, used as a data-reduction strategy, it ef-
fectively reduces the computational cost of kernel- While (cid:104)s,u (cid:105) represents a local signal average, (cid:104)n,u (cid:105) is
i i
based estimators without losing accuracy. reduced by averaging. ((cid:104)x, u i (cid:105)) form thus a com-
(cid:107)u (cid:107)2 i∈[k]
i

Fast clustering for statistics
pressed representation of x. The problem boils down to Algorithm 1 Fast clustering by recursive nearest neighbor
defining a suitable partition of the image volume, or equiv- agglomeration
alently of the associated projectors (u i) i∈[k], where k is Require: Input image X with shape (p,n), associated
large. Data-unaware clustering partitions are obviously topological graph T , nearest neighbor extraction func-
sub-optimal, as they do not respect the underlying struc- tion nn, connected components extraction function cc,
tures and lead to signal loss. Data-driven clustering can be desired number k of clusters.
performed through various approaches, such as k-means or Ensure: Clustering of the voxels l : [p] → [k]
agglomerative clustering, but they tend to be expensive: k- 1: G = (δ(T )(cid:107)x − x (cid:107)),(i,j) ∈ [p] × [p] {Create
ij i j
means has a complexity O(npk); agglomerative clustering weighted graph}
(based on average or complete linkage heuristics or Ward’s 2: l = cc(nn(G)) {connected components of nearest-
strategy (Ward, 1963)) is also expensive (O(np2)). Single neighbor graph}
linkage clustering is fast but suffers from percolation is- 3: q = #(l) {number of connected components}
sues. Percolation is a major issue, because decompositions 4: U = (δ(j == l(i)),(i,j) ∈ [p] × [q] {assignment
withonegiantclusterandsingletonsorquasi-singletonsare matrix}
obviously suboptimal to represent the input signals. 5: while q > k do
6: X ← (UTU)−1UTX {reduced data matrix}
3. Fast clustering 7: T ← UTT U {reduced topological model}
8: G = (δ(T )(cid:107)x −x (cid:107)),(i,j) ∈ [q]×[q] {weighted
ij i j
Percolation on lattices. Voxel clustering should take into graph}
account the 3D lattice structure of medical images and be 9: λ = cc(nn(G),k) {cc extracts at most k compo-
based on local image statistics (e.g. local contrasts instead nents}
of cluster-level statistical summaries) in order to obtain 10: U ← (δ(j == λ(i)),(i,j) ∈ [q] × [#λ]
linear-time algorithms. A given dataset X is thus repre- {assignment matrix}
sented by a graph G with 3D lattice topology, where edges 11: q ← #λ {number of connected components}
between neighboring voxels indexed by i and j are associ- 12: l ← λ ◦ l {update the voxel labeling}
ated with a distance (cid:107)x i −x j(cid:107) that measures the similarity 13: end while
between their features. A common observation is that ran-
dom graphs on lattices display percolation as soon as the
edge density reaches a critical density (≈.2488 on a regu- Since all the operations involved are linear in the number
lar 3D lattice), meaning that a huge cluster will group most of vertices, the procedure is actually linear in p. As pre-
of the voxels, leaving only small islands apart (Stauffer & dicted by theory (Teng & Yao, 2007) –namely the fact that
Aharony, 1971). While single linkage clustering suffers a one-nearest neighbor graph on any set of point (whether
from percolation, a simple variant alleviates this problem: on a regular lattice or not) does not exhibit percolation–
the cluster sizes are very even. This procedure yields more
1. Generate the minimum spanning tree M of G even cluster sizes than agglomerative procedures, and per-
forms about as well as k-means for this purpose (see e.g.
2. Deleterandomly(k−1)edgesfromMwhileavoiding
Fig. 2). We call it fast clustering henceforth.
to create singletons (by a test on each incident node’s
degree).
4. Experiments
This strategy is called rand single linkage or, more sim- We compare the performance of various compression
ply rand single, in this paper. Sophisticated strategies have schemes: single, average and complete linkage, Ward,
been proposed in the framework of computer vision (e.g. fast clustering and sparse random projections in a se-
(Felzenszwalb & Huttenlocher, 2004)), but they have not ries of tasks involving public neuroimaging datasets (ei-
been designed to avoid percolation and do not make it pos- ther anatomical or functional). We do not further study k-
sible to control the number k of clusters. means, as the estimation is overly expensive in the large k
regime of interest.
In order to obtain better clustering, we have designed the
linear-time clustering algorithm described in Alg. 1 and il-
Accuracy of the compressed representation First, we
lustrated on a 2D brain image in Fig. 1. This algorithm
study the accuracy of the isometry in Eq. 1, which we
is a recursive nearest-neighbor agglomeration, that merges
simply check empirically by evaluating the ratio η =
clusters that are nearest neighbor of each other at each step.
(cid:107)f(x )−f(x )(cid:107)2
Since the number of vertices is divided by at least 2 at each 1 2 for pairs (x , x ) of samples on simulated
(cid:107)x −x (cid:107)2 1 2
1 2
step, the number of iterations is at most O(log(p/k)), i.e. and real data. Random projections come with precise guar-
5 or less in practice, as we use typically p/k ≈ 10 or 20. antees on the variance of η as a function of k, but no

Fast clustering for statistics
sidered a set of activation maps. Specifically, we relied on
the motor activation dataset taken from 67 subjects of the
HCP dataset (Barch et al., 2013), from which we consid-
ered the activation maps related to five different contrasts:
(moving the) left hand versus average (activation), right
hand vs. average, left foot vs. average, right foot vs. av-
erage and tongue vs average. These activation maps have
Figure 1. Principleofthefastclusteringprocedureillustratedina
been obtained by general linear model application upon the
real2Dbrainimage: the(non-percolating)nearestneighborgraph
preprocessed data resampled at 2mm resolution in MNI
iscomputedfromtheorigindataandsoonrecursively. Atthelast
space. From these sets of maps, in each voxel we com-
iteration,onlytheclosestneighborsareassociatedtoyieldexactly
the desired number k of components. puted the ratio of the between-condition variance (aver-
aged across subjects) to the between-subject variance (av-
Histogram of cluster sizes for 20000 clusters
6
10 eraged across conditions). Then we did the same on the
kmeans
5 fast cluster-based representation. The quotient of these
10 fast
rand single two values is equal to 1 whenever the signals are identi-
4
10
ward
cal. Values greater than 1 indicate a denoising effect, as the
3 average
10 between-condition variance reflects the signal of interest
complete
102 while between-subject variance is expected to reflect noise
plus between-subject variability. We simply consider the
1
10
boxplot of the log of this ratio, as a function of the number
0
10
0 1 2 3 k of components.
10 10 10 10
Cluster size
Fast logistic regression We performed a discriminative
Figure 2. Percolation behavior of various clustering methods ob-
analysis on the OASIS dataset (Marcus et al., 2007): We
served through the cluster size histogram for a fixed number
used n = 403 anatomical images and processed them with
k = 20,000 of clusters, obtained by averaging across 10 sub-
the SPM8 software to obtain modulated grey matter den-
jects of the HCP dataset. K-means and fast clustering best avoid
percolation, as they display neither singletons nor very large clus- sity maps sampled in the MNI space at 2mm resolution.
ters. Traditional agglomerative clustering methods, on the other We used these maps to predict the gender of the subject.
hand, exhibit both giant and small components. Similar results To achieve this, the images were masked to an approximate
are obtained for other values of k and datasets. average mask of the grey matter, leaving p = 140,398 vox-
els. The voxel density values were then analyzed with an
(cid:96) -logistic classifier, the regularization parameter of which
2
such result exists for cluster-based representations. The was setby cross-validation. Thiswas performed forthe fol-
simulated data is simply a cube of shape 50 × 50 × 50, lowing methods: non-reduced data, fast clustering, Ward
that contains a signal consisting of smooth random sig- and random projection reduction to either k = 4,000 or
nal (FWHM=8mm), with additional white noise; n = 100 k = 20,000 components. The accuracy of the procedure
samples are drawn. The experimental data are a sample was assessed in a 10-fold cross validation loop. We mea-
of 10 individuals taken from NYU test-retest resting-state sured the computation time taken to reach a stable solution
functional Magnetic Resonance Imaging (fMRI) dataset by varying the convergence control parameter.
(Shehzad et al., 2009), after preprocessing with a standard
Note that the estimation problem is rotationally invariant –
SPM8 pipeline, sampled at 3mm resolution in the MNI
i.e. the objective function is unchanged by a rotation of the
space (n = 197 images per subject, p = 43878 voxels).
feature space– which makes it well suited for projection-
To avoid the bias inherent to learning the clusters and mea- based dimension reductions. Indeed, these can be inter-
suring the accuracy of the representation on the same data, preted as a kernel.
we perform a cross-validation loop: the clusters a learned
on a training dataset, while the accuracy is measured on an
Fast Independent Components Analysis We performed
independent dataset. Importantly, it can be observed that
an Independent Components Analysis (ICA) on resting
clustering is actually systematically compressive. Hence,
state fMRI from the HCP dataset, as this is a task per-
we base our conclusions on the variance of η across pairs
formed routinely on this dataset. Specifically, ICA is used
of samples, i.e. the stability of the ratio between distances.
to separate functional connectivity signal from noise and
obtain a spatial model of the functional connectome (Smith
Noise reduction To assess the differential effect of the et al., 2013). In the present experiment we analyzed inde-
spatial compressions on the signal and the noise, we con- pendently data from 93 subjects. These data consist of two

|    |    |    |    |    |    |    |      |          |   None | ward       | None   | None   | None   |
|    |    |    |    |    |    |    |      |          |        | average    |        |        |        |
|    |    |    |    |    |    |    |      |          |        | complete   |        |        |        |
|:---|:---|:---|:---|:---|:---|:---|:-----|:---------|-------:|:-----------|:-------|:-------|:-------|
|    | 0  |    |    |    |    |    | 101  | 1        |     02 |            |        |        |        |
|    |    |    |    |    |    |    | Clus | ter size |        |            |        |        |        |
|    | u  | r  | e  | 2  |    |    |      |          |        |            | e      | et     |        |
|    | v  | e  | d  | t  |    |    |      |          |        |            | o      | d      |        |
|    | =  | o  | o  | 20 |    |    |      |          |        |            | n      | ss     |        |
|    | s  | ,  | l  | f  |    |    |      |          |        |            | i      | b      |        |
|    | c  | u  | T  | at |    |    |      |          |        |            | o      | la     |        |
|    | .  | (  | b  | r  |    |    |      |          |        |            | n      | la     |        |
|    | d  | p  | l  | e  |    |    |      |          |        |            | ;      | n      |        |
|    | o  | 0  | c  | ta |    |    |      |          |        |            | s      | 5      |        |
|    | h  | t  | F  | re |    |    |      |          |        |            | I      | d      |        |
|    | t  | e  | l  | at |    |    |      |          |        |            | a      | n      |        |
|    | m  | v  | i  | o  |    |    |      |          |        |            | o      | a      |        |
|    | 1  | n  | h  | W  |    |    |      |          |        |            | s      | ti     |        |
|    | c  | p  | 8  | e  |    |    |      |          |        |            | u      | )      |        |
|    | M  | p  | e  | i  |    |    |      |          |        |            | e      | s      |        |
|    | c  | t  | g  | o  |    |    |      |          |        |            | e      | t      |        |
|    | a  | b  | e  | z  |    |    |      |          |        |            | r      | x      |        |
|    | i  | a  | t  | (  |    |    |      |          |        |            | d      | a      |        |
|    | a  | s  | e  | oi |    |    |      |          |        |            | e      | a      |        |
|    | e  | i  | a  | t  |    |    |      |          |        |            | e,     | a      |        |
|    | s  |    | m  | r  |    |    |      |          |        |            |        | r      |        |
|    | s  |    | e  | ra |    |    |      |          |        |            |        | r      |        |
|    | i  |    | a  | e  |    |    |      |          |        |            |        | .      |        |
|    | t  |    |    | ri |    |    |      |          |        |            |        | o      |        |
|    |    |    |    | s  |    |    |      |          |        |            |        | i      |        |
|    |    |    |    | l  |    |    |      |          |        |            |        | c      |        |

Fast clustering for statistics
resting-state sessions of 1200 scans. We relied on the pre- ments, we only consider Ward and fast clustering.
processed data, resampled at 2mm resolution in the MNI
space. Each image represents about 1GB of data, that is Noise reduction The differential effect of the spatial
converted to a data matrix with (p ≈ 220,000, n = 1,200). compression on the signal and the noise is displayed in
Fig. 5. This shows that, in spite of large between-voxel
We performed an ICA analysis of each dataset in three set-
variability, there is a clear trend toward a higher signal-to-
tings: i) on the raw data, ii) on the data compressed by fast
noise ratio for lower values of k. This means that spatial
clustering (k = 20,000) and iii) on the data compressed
compressions like clustering impose a low-pass filtering of
by sparse random projections (k = 20,000). We extracted
the data that better preserves important discriminative fea-
q = 40 independent components as it is a standard number
tures than variability components, part of which is simply
in the literature. Based on these analyses we investigated i)
noise.
whether the components obtained from each dataset were
similar or not before and after clustering; ii) How similar
Fast logistic regression. The results of the application
the components of session 1 and session 2 were after each
of logistic regression to the OASIS dataset are displayed
type of processing. This was done by matching the compo-
in Fig. 6: this shows that the compressed datasets (with
nents across sessions with the Hungarian algorithm, using
either fast clustering, Ward or random projections) can
the absolute value of the pairwise correlation as a between-
achieve at least the same level of accuracy as the uncom-
componentssimilarity; iii)thecomputationtimeoftheICA
pressed version, with drastic time savings. This result is a
decomposition.
straightforward consequence of the approximate isometry
property of the compressed representations. The accuracy
Implementation aspects The data that we used are
achieved is actually higher for cluster-based compressions
the publicly available NYU test-retest, OASIS and HCP
than with the original data or random projections: this il-
datasets, for which we used the data with the preprocess-
lustrates again the denoising effect of spatial compression.
ing steps provided in the release 500-subjects release. We
As a side note, achieving full convergence did not improve
relied on the Scikit-learn library (Pedregosa et al., 2011)
the classifier performance. Qualitatively similar results are
(v0.15) for machine learning tasks (ICA, logistic regres-
obtained with other rotationally invariant methods (e.g., (cid:96) -
2
sion) and for Ward clustering. We relied on the Scipy li-
SVMs, ridge regression). They should carry out to any ker-
brary for the agglomerative clustering methods and the use
nel machine.
of sparse matrices.
Fast Independent Components Analysis The results of
5. Results
the ICA experiment are summarized in Fig. 7: We found
that the q = 40 first components were highly similar before
Computational cost. The computational cost of the dif-
and after fast clustering: the average absolute correlation
ferent compression schemes is displayed in Fig. 3. While
between the components was about 0.75, while random
sparse random projections are obviously faster, as they do
projections do not recover the components (average cor-
not require any training, fast clustering outperforms by far
relation < 0.4). Across two sessions, the components ob-
Ward clustering, which is much faster than average or com-
tained by clustering are actually more similar after cluster-
plete linkage procedures. The clustering of a relatively
ing than before, showing again the denoising effect of clus-
largeimagecanbeobtainedinasecond, thiscostisactually
tering. This effect was observed in all 93 subjects, hence
much smaller than standard linear algebra computations on
is extremely significant (p < 10−10, paired Wilcoxon rank
the same dataset (blas level 3 operations). Furthermore,
test). On the opposite, random projections yielded a degra-
this cost is reduced by learning the clustering on a subset
dation of the similarity: this is because random projec-
of the images (e.g. from 2.3s to 0.6s if one uses 10 images
tions perturb the statistical structure of the data, in particu-
of OASIS instead of 100).
lar the deviations from normality, which are used by ICA.
As a consequence, ICA cannot recover the sources derived
Accuracy of the compressed representation The qual- from the original data. By contrast, the statistical structure
ity of distance preservation is summarized in Fig. 4. The is mostly preserved after clustering. Finally, the compu-
p
random projections accuracy improves with k, as predicted tation time is reduced by a factor of 20, while ≈ 12
k
by theory. Among the clustering algorithms, Ward cluster- thus improving drastically the tractability of the procedure.
ing performs best in terms of distance preservation. Fast Faster convergence is obtained by fast-clustering than with
clustering performs slightly worse, though better than ran- random projections. In summary, fast clustering not only
dom projections. On the other hand, average and complete helped to make ICA faster, it also improved the stability of
linkageperformpoorly onthistask–whichis expected, due the results. Random projections cannot be used for such a
to their tendency toward percolation. In the next experi- purpose.

Fast clustering for statistics
ward
rand single linkage
rp sparse
fast cluster
complete linkage
average linkage
-1 0 1 2 3
10 10 10 10 10
Time (s)
Figure 3. Evaluation of the computation time of the clustering algorithms (to obtain k = 10,000 clusters) tested on n = 100 images
taken from the OASIS dataset. The proposed fast clustering outperforms by far all alternatives, except random projections.
rorre noitcejorp etulosba eht fo ecnairaV 10-6 Simulated data rorre noitcejorp etulosba eht fo ecnairaV 102 VBM data
average linkage
complete linkage
fast cluster
10-7 101
rp sparse
rand single linkage
ward
-8 0
10 10
4 6 8 10 12 14 16 18 20 22 2 4 6 8 10 12 14 16
Compression ratio % Compression ratio %
Figure 4. Evaluation of the metric accuracy of the compressed representations obtained through various compression techniques, for
different numbers of components. These experiments are based on simulated (left) and the OASIS dataset (right) respectively. The
compression ratio is k and error bars are across 10 datasets;
p
log(between condition variance / between subject variance) 
 relative to voxel-based model
0.5
0.4
0.3
0.2
0.1
0.0
0.1
0.2
0.3
500 1k 2k 3k 4k 5k 7k 10k 15k 20k 30k 50k
number of clusters
Figure 5. Denoising effect of cluster-based compression: the ratio of between-contrasts (of interest) to between subject variance (of no
interest) is increased when a lower number of regions is used in the data compression scheme. This is based on five motor contrasts of
the HCP fMRI dataset and the fast clustering procedure.

|    |
|:---|
|    |

Fast clustering for statistics
0.8
0.7
all 140398
erocs noitciderP 0.6
fast_cluster 4000
0.5
fast_cluster 20000
0.4 rp_sparse 4000
rp_sparse 20000
0.3
ward 4000
0.2
ward 20000
0.1
0.0
-2 -1 0 1
10 10 10 10
Time (s)
Figure 6. Quality of the fit of a logistic regression of the OASIS dataset as a function of computation time. The cluster-based methods
obtain significantly higher scores than regression on the whole dataset with a much smaller computation time (by 1.5 orders of magni-
tude). Note that the time displayed does not include cluster computation, which is costly in the case of Ward clustering (≈ 10 seconds,
see Fig. 3).
consistency with raw between-session similarity Computation time(s)
4
0.9 0.50 10
0.45
0.8
0.40
3
0.7 10
0.35
0.6
0.30
2
0.5 10
0.25
0.4
0.20
1
0.3 0.15 10
fast clustering RP raw fast clustering RP raw fast clustering RP
Figure 7. Results of the ICA experiments (left) the accuracy of the fast clustering with respect to the non-compressed components is
high. (Middle) across two sessions, fast clustering yields components more consistent than raw data, while random projections fail to
do so; (right) Regarding computation time, fast clustering yields a gain factor of ≈ 20, actually larger than p/k. The boxplots represent
distributions across 93 subjects.
6. Discussion lation or super-voxels. The difference lies in the interpreta-
tion: we do not view spatial compression as a meaningful
Our experiments have shown that on moderate size
model per se, but as a way to reduce data dimensionality
datasets, a fast clustering technique can yield impressive
without losing too much information. We will typically set
gains in computation speed for a minimal overhead to
k = p/10 and this number is necessarily a trade-off be-
build the spatially- and contrast-aware data compression
tween computational efficiency and data fidelity. Note that
schemes. More importantly, the gain is found to be more
in this regime, Ward clustering is slightly more powerful
than linear in various applications. This comes with two
in terms of representation accuracy, but it is much slower
other good news: even in the absence of theoretical re-
hence cannot be considered as a practical solution.
sult, we found that spatial compression schemes perform
as well as the state-of-the-art approach in data compression As shown by the ICA experiment, clustering-based com-
for machine learning, namely random projections. This pression can be used even in tasks in which the (cid:96) norm
2
holds thanks to the structure of medical images, where the preservation alone does not guarantee a good representa-
noise is often observed in higher frequency components tion. The combination of clustering, randomization and
than the relevant information. Finally, we found that the sparsity has also proved to be an extremely effective tool
spatial compression schemes presented here actually have in ill-posed multivariate estimation problems (Varoquaux
a denoising effect, yielding possibly more accurate predic- et al., 2012; Bu¨hlmann et al., 2012), hence fast clustering
tors than uncompressed version, or random projections. seems particularly well-suited for these problems.
Conceptually, it is tempting to compare the spatial model In conclusion, we have shown that a procedure using our
obtained with fast clustering with traditional brain parcel- fast clustering method as a data reduction yields a speed

Fast clustering for statistics
up of 1.5 order of magnitude on two real-world multivari- image representations for deformable registration of
ate statistic problems. Moreover, on a supervised problem, chest mri and ct volumes. In IPMI, pp. 463–474, 2013.
we improve the prediction performance by using our data
compression scheme, as it captures better signal than noise. Johnson, William and Lindenstrauss, Joram. Extensions of
The proposed strategy is thus extremely promising regard- Lipschitz mappings into a Hilbert space. In Conference
ing the statistical analysis of big medical image datasets, as in modern analysis and probability, volume 26 of Con-
it is perfectly compatible with efficient online estimation temporary Mathematics, pp. 189–206. American Math-
methods (Schmidt et al., 2013). ematical Society, 1984.
Li, Ping, Hastie, Trevor J., and Church, Kenneth W. Very
Acknowledgment. Data were provided in part by the Human
sparse random projections. In International Conference
Connectome Project, WU-Minn Consortium (Principal Investi-
on Knowledge Discovery and Data Mining, KDD, pp.
gators: David Van Essen and Kamil Ugurbil; 1U54MH091657)
287, 2006. ISBN 1-59593-339-5.
funded by the 16 NIH Institutes and Centers that support the NIH
BlueprintforNeuroscienceResearch;andbytheMcDonnellCen-
Marcus, Daniel S, Wang, Tracy H, Parker, Jamie, Csernan-
ter for Systems Neuroscience at Washington University.
sky, John G, Morris, John C, and Buckner, Randy L.
Open access series of imaging studies (oasis): cross-
References
sectional mri data in young, middle aged, nondemented,
and demented older adults. J Cogn Neurosci, 19(9):
Baraniuk, Richard G. and Wakin, Michael B. Random pro-
1498–1507, Sep 2007.
jections of smooth manifolds. Found. Comput. Math., 9
(1):51–77, January 2009. ISSN 1615-3375.
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,
Barch, Deanna M, Burgess, Gregory C, Harms, Michael P, Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
Petersen, Steven E, Schlaggar, Bradley L, Corbetta, Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cour-
Maurizio, Glasser, Matthew F, Curtiss, Sandra, Dixit, napeau, D., Brucher, M., Perrot, M., and Duchesnay, E.
Sachin, Feldt, Cindy, Nolan, Dan, Bryant, Edward, Hart- Scikit-learn: Machine learning in Python. Journal of
ley, Tucker, Footer, Owen, Bjork, James M, Poldrack, Machine Learning Research, 12:2825, 2011.
Russ, Smith, Steve, Johansen-Berg, Heidi, Snyder, Abra-
ham Z, Essen, David C Van, and Consortium, W. U- Rahimi, Ali and Recht, Ben. Random features for large-
Minn HCP. Function in the human connectome: task- scale kernel machines. In In Neural Infomration Pro-
fmri and individual differences in behavior. Neuroimage, cessing Systems, 2007.
80:169–189, Oct 2013.
Schmidt, M., Le Roux, N., and Bach, F. Minimizing Finite
Bu¨hlmann, P., Ru¨timann, P., van de Geer, S., and Zhang, Sums with the Stochastic Average Gradient. ArXiv e-
C.-H. Correlated variables in regression: clustering and prints, September 2013.
sparse estimation. ArXiv e-prints, September 2012.
Shehzad, Z., Kelly, AM, Reiss, P.T., Gee, D.G., Gotimer,
Button, Katherine S, Ioannidis, John P A, Mokrysz, Claire,
K., Uddin, L.Q., Lee, S.H., Margulies, D.S., Roy, A.K.,
Nosek, Brian A, Flint, Jonathan, Robinson, Emma S J,
Biswal, B.B., et al. The resting brain: unconstrained yet
and Munaf, Marcus R. Power failure: why small sample
reliable. Cerebral Cortex, 2009.
size undermines the reliability of neuroscience. Nat Rev
Neurosci, 14(5):365–376, May 2013.
Smith, Stephen M, Beckmann, Christian F, Andersson, Jes-
per, Auerbach, Edward J, Bijsterbosch, Janine, Douaud,
Felzenszwalb, Pedro F. and Huttenlocher, Daniel P. Effi-
Gwenalle, Duff, Eugene, Feinberg, David A, Griffanti,
cient graph-based image segmentation. Int. J. Comput.
Ludovica, Harms, Michael P, Kelly, Michael, Laumann,
Vision, 59(2):167–181, September 2004. ISSN 0920-
Timothy, Miller, Karla L, Moeller, Steen, Petersen,
5691.
Steve, Power, Jonathan, Salimi-Khorshidi, Gholamreza,
Snyder, Abraham Z, Vu, An T, Woolrich, Mark W, Xu,
Halko, N., Martinsson, P.-G., and Tropp, J. A. Finding
Junqian, Yacoub, Essa, Uurbil, Kamil, Essen, David
structure with randomness: Probabilistic algorithms for
C Van, Glasser, Matthew F, and Consortium, W. U-
constructing approximate matrix decompositions. ArXiv
Minn HCP. Resting-state fmri in the human connectome
e-prints, September 2009.
project. Neuroimage, 80:144–168, Oct 2013.
Heinrich, Mattias P., Jenkinson, Mark, Papiez, Bart-
lomiej W., Glesson, Fergus V., Brady, Michael, and Stauffer, D. and Aharony, A. Introduction to Percolation
Schnabel, Julia A. Edge- and detail-preserving sparse Theory. Oxford University Press, New York, 1971.

Fast clustering for statistics
Teng, Shang-Hua and Yao, FrancesF. k-nearest-neighbor
clustering and percolation theory. Algorithmica, 49
(3):192–211, 2007. ISSN 0178-4617. doi: 10.1007/
s00453-007-9040-7. URL http://dx.doi.org/
10.1007/s00453-007-9040-7.
Varoquaux, Gae¨l, Gramfort, Alexandre, and Thirion,
Bertrand. Small-sample brain mapping: sparse recov-
ery on spatially correlated designs with randomization
and clustering. In ICML, pp. 1375, 2012.
Ward, Joe H. Hierarchical grouping to optimize an objec-
tive function. Journal of the American Statistical Asso-
ciation, 58:236, 1963.
Zalesky, Andrew, Fornito, Alex, Cocchi, Luca, Gollo,
Leonardo L, and Breakspear, Michael. Time-resolved
resting-state brain networks. Proc Natl Acad Sci U S A,
111:10341–10346, 2014.