∗
Latent Mixture Modeling for Clustered Data
SHONOSUKE SUGASAWA
Risk Analysis Research Center, The Institute of Statistical Mathematics
GENYA KOBAYASHI
Graduate School of Social Sciences, Chiba University
YUKI KAWAKUBO
Graduate School of Social Sciences, Chiba University
7102 rpA 02  ]EM.tats[  1v39950.4071:viXra
Abstract. This article proposes a mixture modeling approach to estimating cluster-
wise conditional distributions in clustered (grouped) data. We adapt the mixture-
of-experts model to the latent distributions, and propose a model in which each
cluster-wise density is represented as a mixture of latent experts with cluster-wise
mixing proportions distributed as Dirichlet distribution. The model parameters are
estimated by maximizing the marginal likelihood function using a newly developed
Monte Carlo Expectation-Maximization algorithm. We also extend the model such
that the distribution of cluster-wise mixing proportions depends on some cluster-level
covariates. The finite sample performance of the proposed model is compared with
some existing mixture modeling approaches as well as linear mixed model through
the simulation studies. The proposed model is also illustrated with the posted land
price data in Japan.
Key words: conditional distribution; Monte Carlo EM algorithm; hierarchical model;
mixture modeling; random effect
1 Introduction
Grouped or clustered data often arise in many scientific fields such as econometrics,
epidemiology, and genetics. Although the mixed-effects model (Demidenko, 2004) has
been widely used for such data, it fundamentally aims at modeling conditional means
in each cluster, which could be inappropriate if the data distribution is skewed or mul-
timodal. As an alternative modeling strategy, the finite mixture model (McLachlan
and Peel, 2000) has been extensively applied for its flexibility to capture the within-
cluster heterogeneity in the data. For modeling independent data, the mixture model
with covariates was originally proposed in Jacob et al. (1991), known as mixture-of-
experts. To date, a large body of literature has been concerned with flexible modeling
of the conditional density for independent data. For example, see Jordan and Jacobs
(1994), Hurn et al. (2003), Geweke and Keane (2007), Villani et al. (2009), Villani
et al. (2012) and Nguyen and McLachlan (2016).
However, the existing models for independent data are not suitable for estimating
cluster-wise conditional distributions. If we globally apply the mixture models to a
∗This version: February 26, 2018
1

whole dataset ignoring the clustering labels (we call global mixture modeling), the
estimated conditional distributions are the same over all clusters, which is clearly
inappropriate in clustered data analysis. On the other hand, applying the mixture
models independently to each cluster in order to capture the cluster heterogeneity (we
call local mixture modeling) leads to unstable results since the within-cluster samples
sizes are usually not large in practice. Hence, another flexible modeling strategy for
clustered data is desired. Up to now, several methods have been proposed for mod-
eling cluster-wise distributions. Rubin and Wu (1997) proposed a mixture of linear
mixed-effects models. Sun et al. (2007) developed a mixture of linear models with
the random effects used in the generalized linear model for the mixing proportions.
Rosen et al. (2000) and Tang and Qu (2016) used the generalized estimating equation
approach to estimate the component distributions by incorporating the correlations
within clusters.
In this article, we propose a compromised model between the global and local
mixture modeling. Note that the local mixture model can be expressed as
K
(cid:88)
f (y|x) = π h (y|x),
i ik ik
k=1
where y is the response variable, x is the vector of covariates, and h is the component
ik
distribution for the kth component of the ith cluster with the mixing proportion π
ik
(cid:80)K
satisfying π = 1. Since the within-cluster sample size is usually small in
k=1 ik
practice, h (y|x) would not be stably estimated. Hence, we restrict h (y|x) =
ik ik
h (y|x), that is, the component distributions are the same over all the clusters like
k
global modeling. Then the model reduces to
K
(cid:88)
f (y|x) = π h (y|x),
i ik k
k=1
which can be interpreted as there exists K latent distributions and each cluster-
wise distribution f (y|x) is expressed by these distributions with cluster-wise mixing
i
proportions π . Hence, as long as K is a moderate number, one can estimate K
ik
component distributions with reasonable accuracy. On the other hand, estimating
unstructured π is not feasible since the number of π ’s grows as the number of clus-
ik ik
ters increases. To overcome this difficulty, we assume that the vector of proportions
π = (π ,...,π )t that characterizes the conditional distribution of the ith cluster,
i i1 iK
is a realization from a multivariate distribution. Therefore, π plays a similar role to
i
the random effect in the context of the mixed-effects model. As a distribution of π ,
i
we use the Dirichlet distribution, which allows us to develop a tractable estimating
method for model parameters.
In this article, the model parameters are estimated based on a likelihood-based
approach. The model can be viewed as a three-stage hierarchical model, where the
first stage consists of the model for the response variable, the second stage consists of
the latent variables which assign the latent distribution, and the third stage consists
of the model for the mixing proportions. We develop a Monte Carlo Expectation-
Maximization (MCEM) algorithm (Dempster et al., 1977; Wei and Tanner, 1990)
for parameter estimation of which the E-step is consist of a simple Gibbs sampling
scheme for imputing the latent variables. Since the number of latent distributions K
2

is generally unknown, we consider selecting K based on the Akaike information crite-
ria (AIC) or Bayesian information criteria (BIC), where the maximum log-marginal
likelihood can be easily computed from a simple Monte Carlo approximation.
The rest of the paper is organized as follows: Section 2 describes the proposed
model in detail and develops the MCEM algorithm for maximizing the marginal
likelihood. In Section 3, the performance of the proposed method is demonstrated
along with some existing methods through simulation studies. An application to the
real data set is also presented. In Section 4, some discussion is provided.
2 Latent Mixture Model
2.1 Model setup
Suppose that we have the clustered (grouped) observations y , i = 1,...,m, j =
ij
1,...,n , with an associated p-dimensional vector of covariates x . Let f (y|x) be a
i ij i
density or probability mass function of y given x , which are the same within clus-
ij ij
ters but different across clusters. Our aim is to estimate the cluster-wise conditional
density f (y|x) from the data set {y ,x }. To this end, we consider the following
i ij ij
latent mixture model:
K
(cid:88)
f (y|π ,x,φ) = π h (y|x,φ ), (1)
i i ik k k
k=1
where π is the weight for the kth component in the ith cluster, h (·|·,φ ), k =
ik k k
1,...,K are the latent conditional densities characterized by the parameter φ , and
k
K is the unknown number of latent densities. Moreover, we assume that the mixing
proportions π ’s are independent realizations from the Dirichlet distribution with the
i
density
(cid:0)(cid:80)K (cid:1) K
Γ α
(cid:89)
k=1 k α −1
p(π |α) = π k (2)
i (cid:81)K ik
Γ(α )
k=1 k k=1
for i = 1,...,m, where Γ(·) denotes the gamma function and α = (α ,...,α )t
1 K
is a vector of unknown parameters. In this article, we let (1) and (2) together de-
note the latent mixture model. The unknown model parameters to be estimated are
φ ,...,φ in latent distributions and α in the Dirichlet distribution. Under the
1 K
setting (1) and (2), taking expectation of π with respect to Dir(α), we have
ik
K
(cid:88) α
k
f (y|x,α,φ) = p h (y|x,φ ), p = , (3)
i k k k k (cid:80)K
α
k=1 (cid:96)=1 (cid:96)
which is referred to the marginal model, and is common over all the clusters. Hence,
we can observe that π characterizes the cluster-wise conditional density and plays
i
a similar role to the random effects in the context of mixed-effects models. The
mixing proportion π can be estimated by the conditional expectation E[π |Y ], where
i i
Y is a set of all the response variables. Under (1) and (2), response variables in
different clusters are mutually independent, so that it holds E[π |Y ] = E[π |Y ] with
i i i
Y = {y ,...,y }. Then, if the model parameters are known, the estimator of the
i i1 in
i
3

cluster-wise conditional density is given by
K
(cid:88)
˜
f (y|x,α,φ) = E[π |Y ]h (y|x,φ ). (4)
i ik i k k
k=1
Generally speaking, the conditional expectation E[π |Y ] tends close to the marginal
ik i
mean p if the cluster-specific sample size n is small, so that the estimated conditional
k i
density would be close to the marginal model (3). On the other hand, in clusters with
relatively large n , the estimated conditional density might vary from the marginal
i
model (3), depending on the information of Y . Therefore, this model allows us to
i
carry out a kind of shrinkage estimation of the cluster-wise conditional densities.
As often done in estimating mixture models, by introducing the latent component
indicator z ∈ {1,...,K}, the proposed model (1) and (2) can be expressed in the
ij
three-stage hierarchical model:
1st stage: y |x ,(z = k) ∼ F (x ,φ ),
ij ij ij k ij k
2nd stage: z |π ∼ Cat(K,π ), (5)
ij i i
3rd stage: π ∼ Dir(α),
i
where F is the distribution having density h , and Cat(K,π ) is the categorical
k k i
distribution on {1,...,K} with the probability vector π . In hierarchy (5), z and
i ij
π are the latent variables. The latent density h is determined by the user and
i k
the generalized linear model is an attractive choice. For example, F (x ,φ ) =
k ij k
N(xt β ,σ2) when y is a continuous variable, and F (x ,φ ) = Po(exp(xtβ ))
ij k k ij k ij k k
when y is a counting variable.
ij
2.2 Monte Carlo EM algorithm for parameter estimation
For completion of the conditional density (4), we need to estimate the unknown
model parameters θ = {φ ,...,φ ,α} based on the data. Under the hierarchical
1 K
formulation (5), the marginal likelihood function L(θ) is expressed as
 
(cid:32) K(cid:80)K k)(cid:33)m (cid:89)m (cid:81)K Γ((cid:80)n (cid:89)n (cid:89)K
L(θ) = Γ (cid:81)( k=1 α (cid:88) k=1 j (cid:80)=i 1 Kw ijk + α k) i h k(y ij|x ij,φ k)w ijk,
Γ(α ) Γ(n + α )
k=1 k i=1 z i k=1 k j=1k=1
i
(cid:80)
where w = I(z = k) and denotes the summation over the all combination
ijk ij z
i
of z ∈ {1,...,K}n i. Hence, a direct maximization of the marginal likelihood is
i
not feasible since evaluation of the likelihood function L(θ) requires the summation
over Kn i elements for each i, which is computationally prohibitive even for small K.
Moreover, since the functional form of L(θ) is complex and not familiar, the brute
force maximization of L(θ) is not realistic.
Instead, we exploit the hierarchical representation (5) and develop the EM algo-
rithm (Dempster et al., 1977) which indirectly and iteratively maximizes L(θ). Let
π = {π ,...,π } and z = {z ,...,z }. Then, the complete log-likelihood function
1 m 1 m
(cid:96)c of (5) is given by
m n K m
i
(cid:88)(cid:88)(cid:88) (cid:8) (cid:9) (cid:88)
c
(cid:96) (θ,z,π) = I(z = k)log π h (y |x ,φ ) + logp(π |α),
ij ik k ij ij k i
i=1 j=1 k=1 i=1
4

where p(π |α) denotes the density function of Dir(α). Then, given the value of θ
i
(t)
in the tth iteration denoted by θ , the E-step entails the imputation of the latent
variables z and π by taking expectation
(t) c (t)
Q(θ|θ ) = E[(cid:96) (θ,z,π)|Y,θ ],
where the expectation is taken with respect to the posterior distribution of (w,π)
(t)
given all the response variables Y . However, since an analytical form of Q(θ|θ ) is
(t)
not available, we consider Monte Carlo approximation of Q(θ|θ ) as
L
1 (cid:88)
(l) c (l) (l)
Q(θ|θ ) ≈ (cid:96) (θ,z ,π ),
L
l=1
where L is a sufficiently large number, and z(l) and π(l) are the lth random sample
(t)
generated from the posterior distribution of (z,π) given Y with θ = θ . Under the
hierarchy (5), the marginal posterior distributions of z and π are not simple forms,
but the full conditional distributions of z|π,Y and π|z,Y are the following familiar
distributions:
z |π ,Y ∼ Cat(K,p ), i = 1,...,m, j = 1,...,n ,
ij i (cid:101)ij i
(6)
π |z,Y ∼ Dir(a ), i = 1,...,m,
i (cid:101)i
where p = (p ,...,p )t and a = (a ,...,a )t with
(cid:101)ij (cid:101)ij1 (cid:101)ijK (cid:101)i (cid:101)i1 (cid:101)iK
(t) n
π h (y |x ,φ ) (cid:88)i
p = ik k ij ij k , and a = α(t) + I(z = k).
(cid:101)ijk (cid:80)K (t) (cid:101)ik k ij
π h (y |x ,φ )
l=1 il l ij ij l j=1
Then we can use a Gibbs sampler for generating random samples of the posterior
distribution of (z,π).
(t)
The M-step maximizes Q(θ|θ ) obtained from the E-step, noting that
m n K m
i
(cid:88)(cid:88)(cid:88) (cid:88)
(t) ∗ (t)
Q(θ|θ ) = C + z logh (y |x ,φ ) + E[logp(π |α)|Y,θ ],
ijk k ij ij k i
i=1 j=1 k=1 i=1
where C is a constant independent of θ and z∗ = E[I(z = k)|Y,θ(t) ] computed
ijk ij
(t)
from the E-step. Therefore, the maximization problem of Q(θ|θ ) can be divided
into the following:
m n
i
(cid:88)(cid:88)
∗
φ(cid:98) = argmax z logh (y |x ,φ ), k = 1,...,K,
k ijk k ij ij k
φ
k i=1 j=1
(7)
(cid:40) (cid:41)
K K K m
(cid:16)(cid:88) (cid:17) (cid:88) (cid:88) (cid:88)
∗
α = argmax mlogΓ α − m logΓ(α ) + α (logπ ) ,
(cid:98) k k k ik
α
k=1 k=1 k=1 i=1
where (logπ )∗ = E[logπ |Y,θ(t) ]. It is noted that the maximization with respect to
ik ik
each φ is identical to maximizing the weighted log-likelihood function of the latent
k
conditional distributions, which can be easily carried out by using, for example, the
Newton-Raphson algorithm. Similarly, the maximization with respect to α is similar
to performing the maximum likelihood method in the Dirichlet distribution and is
not difficult.
The whole procedure of the proposed MCEM algorithm is summarized as follows.
5

Algorithm 1 (MCEM algorithm). Iterative:
(0)
1. Set the initial values θ and t = 0.
2. Draw a large number of samples π and z by Gibbs sampling with the full
conditionals (6), and compute z∗ = E[I(z = k)|Y,θ(t) ] and (logπ )∗ =
ijk ij ik
(t)
E[logπ |Y,θ ].
ik
3. Solve the maximization problem (7) and set φ(t+1) = φ(cid:98) and α(t+1) = α.
k k (cid:98)
4. If the algorithm has converged, the the algorithm is terminated. Otherwise, set
t = t + 1 and go back to Step 2.
In the case of the normal linear regression model as the latent model, namely
F (x ,φ ) = N(xt β ,σ2) in (5), the M-step for φ = (βt ,σ2)t in (7) can be
k ij k ij k k k k k
obtained analytically:
(cid:18) m n (cid:19)−1 m n
i i
(cid:88)(cid:88) (cid:88)(cid:88)
∗ t ∗
β(cid:98) = z x x z x y ,
k ijk ij ij ijk ij ij
i=1 j=1 i=1 j=1
(cid:18) m n (cid:19)−1 m n
i i
(cid:88)(cid:88) (cid:88)(cid:88)
2 ∗ ∗ t 2
σ = z z (y − x β(cid:98) ) .
(cid:98)k ijk ijk ij ij k
i=1 j=1 i=1 j=1
for k = 1,...,K.
Following Shi and Copas (2002), the convergence of the proposed MCEM al-
gorithm is monitored by using the batch mean θ(cid:101)(t) = H−1 (cid:80)H−1 θ(t−h) , after the
h=0
(t)
Hth iteration. The algorithm is terminated when the relative difference (cid:107)θ(cid:101) −
(t−d) (t−d)
θ(cid:101) (cid:107)/((cid:107)θ(cid:101) (cid:107) + δ), is smaller than some predetermined (small) ε. Here, H, d,
ε and δ are specified by the user, and we use H = 30, d = 5, ε = δ = 0.001 as default
choices. For the E-step, L = 500 is used as the default choice and this choice appears
to work well in the numerical examples in Section 3.
For selecting the number of latent distributions, K, we use the Akaike information
criteria (AIC) or the Bayesian information criteria (BIC) based on the log-marginal
likelihood, without any theoretical justifications. When φ is p-dimensional, the
k
number of parameters included in the model (5) is pK + K. Then the formulations
of AIC and BIC are given by
m
(cid:88)
m
AIC = −2 logf (y |x ,θ(cid:98)) + 2(pK + K),
i i i
i=1
m
(cid:88)
m
BIC = −2 logf (y |x ,θ(cid:98)) + (pK + K)logN,
i i i
i=1
(cid:80)m
where N = n is the total number of observations and
i=1 i
 
(cid:90)  n i K 
(cid:89) (cid:88)
m
f (y |x ,θ(cid:98)) = π h (y |x ,φ(cid:98) ) p(π |α)dπ (8)
i i i ik k ij ij k i (cid:98) i
 
j=1k=1
6

is the maximum marginal likelihood. As noted in Section 2.2, since the direct evalua-
tion of the marginal likelihood is computationally prohibitive, the maximum marginal
likelihood is evaluated by the Monte Carlo integration. Let π∗ = (π∗ ,...,π∗ )t be
i i1 iG
the random vector generated from Dir(α). Then, the Monte Carlo approximation of
(cid:98)
(8) is
 
B n K
1 (cid:88) (cid:89)i (cid:88) 
m ∗(b)
f (y |x ,θ(cid:98)) ≈ π h (y |x ,φ(cid:98) ) ,
i i i B ik k ij ij k
 
b=1 j=1k=1
for a large B, where (π∗(b) ,...,π∗(b) )t is the bth draw from Dir(α).
i1 iK (cid:98)
Let K∗ be the selected number of latent distributions based on AIC or BIC. Then
the feasible version of the cluster-wise estimated conditional density (4) is given by
K∗
(cid:88)
f(cid:98)(y|x) = π h (y|x,φ(cid:98) ),
i (cid:98)ik k k
k=1
where π = E[π |Y ] evaluated at θ = θ(cid:98), which can be computed via the Gibbs
(cid:98)ik ik i
sampler (6) with θ = θ(cid:98).
2.3 Flexible modeling of mixing proportions
One possible criticism for the formulation of the proposed latent mixture model (1)
is its simplicity in the relationship between the response variable y and covariate
vector x. In the context of mixture modeling for non-clustered (independent) data,
Geweke and Keane (2007) proposed a flexible modeling of the mixing proportions
by considering covariate dependent structures. Then, we here consider implementing
the idea to the modeling cluster-wise conditional densities, that is, we consider the
following structure in the distribution of the mixing proportions:
t t
π ∼ Dir(α ), α = (α ,...,α ) , α = exp(w γ ), (9)
i i i i1 iK ik i k
where w is the q-dimensional vector of the cluster-specific covariates and γ is
i k
(s) (s)
the corresponding coefficient. One can take, for example, w = x¯ where x¯ =
i i i
−1 (cid:80)n (s) (s)
n i x and x is the subvector of x . Under this setting, it hods that
i j=1 ij ij ij
exp(wtγ )
E[π ] = i k .
ik (cid:80)K
exp(wtγ )
k=1 i k
the MCEM algorithm developed in Section 2.2 can be easily modified to estimate
the model with (9). Specifically, in the E-step a appeared in the full conditional
(cid:101)ik
distribution of π |w,Y in (6) is replaced with
i
n
i
(cid:88)
t (t)
a = exp(w γ ) + I(z = k),
(cid:101)ik i k ij
j=1
and the M-step for α in (7) is replaced with the maximizing
m K m K
(cid:88) (cid:110)(cid:88) (cid:111) (cid:88)(cid:88)
t t
Q(γ) = logΓ exp(w γ ) − logΓ(exp(w γ ))
i k i k
i=1 k=1 i=1 k=1
m K
(cid:88)(cid:88)
t ∗
+ exp(w γ )(logπ ) ,
i k ik
i=1 k=1
7

where γ = {γ ,...,γ }. Finally, it is noted that the number of parameters under
1 K
(9) is K(p+q), so that the penalty terms in AIC and BIC used for selecting K should
be changed accordingly.
3 Numerical Studies
3.1 Simulation studies
The finite sample performance of the proposed latent mixture model is investigated
together with some existing methods. We consider two cases of within-cluster sample
sizes n = 30 and n = 50 for i = 1,...,m and m = 50. For the true conditional
i i
density in the ith cluster, the following two scenarios are considered:
(I) f (y|x) = π φ(y;−1 + x,1) + (1 − π )φ(y;1 − x,1), π ∼ Beta(5,3),
i i i i
2
(II) f (y|x) = I(1 ≤ i ≤ 15)φ(y;−1 + 2x,0.5 ) + I(16 ≤ i ≤ 30)φ(y;1.5 + x,1)
i
2
+ I(31 ≤ i ≤ 50)φ(y;−x,1.5 ),
where i = 1,...,m, and φ(·;a,b) denotes the density function of the normal distribu-
tion N(a,b) and x ∼ N(0,1) in each scenario. The latent mixture regression (LMR)
ij
model with normal linear regression models used as latent models is considered, and
the number of latent components are selected by using BIC. For comparison, we also
consider the local mixture (LM) model where the mixture of normal linear regres-
sions is fitted to each cluster separately and global mixture (GM) model where the
single mixture of normal linear regressions is fitted to the whole data ignoring the
cluster heterogeneity. For both models, the number of components was selected based
on BIC. Moreover, as the competitor from random effect models, we also applied a
random intercept (RI) model. Note that GM ignores the clustering structure and
produces the same conditional densities over all the clusters. On the other hand,
while LM may flexibly express the cluster-wise conditional density, the results are
expected to be unstable due to the relatively small within-cluster sample sizes.
The performance of the models is measured based on the cluster-wise mean inte-
grated squared error (MISE) defined as
R (cid:90)
1 (cid:88) (cid:110) (cid:111)2
(r)
MISE = f(cid:98) (t|x) − f (t|x) dt, i = 1,...,m,
i R i i
r=1
(r)
where f(cid:98) (t|x) is the estimated conditional density obtained from the rth replication.
i
Since the above MISE depends on the covariate x, we considered the three values,
x = −1.5,−0.75,0. We computed the cluster-wise MISE of four models based on
R = 100 replications.
Figure 1 and 2 present the cluster-wise MISE for Scenario (I) and (II), respectively.
The figures show that the proposed LMR model outperforms in all cases. As expected,
LM appears to have produced the unstable results due to the relatively small sample
sizes in spite of its flexibility. On the other hand, GM seems to perform relatively well
in this study as the number of parameters is small compared with LM. However, since
GM produces the same conditional density estimators over the clusters, GM performs
no better than LMR. Concerning RC, it may perform as well as GM for x = 0 in
Scenario (I) and some cases in Scenario (II), but the result is much inferior to that of
8

LMR. Although not shown here, BIC selected the true number of components most
of the time, while the selected number of components by AIC tended to be larger
than the truth. Hence, BIC would be preferable to AIC and only the results based
on BIC are considered in the rest of this article.
We next investigate the efficacy of the modeling the distribution of the mixing
proportion in terms of some covariates as introduced in Section 2.3. To this end, we
consider the following true conditional density:
(III) f (y|x) = π φ(y;−1 + x,1) + (1 − π )φ(y;1 − x,1), π ∼ Beta(α ,α ),
i i i i i1 i1
α = exp(1 + 0.6w ), α = exp(1 − 0.5w ), w ∼ Ber(0.4).
i1 i i2 i i
We set n = i for i = 1,...,m such that the within-cluster sample size varies across
i
clusters and consider two cases of m, m = 50 and 80. As in the previous studies,
the covariates x ’s are generated from N(0,1). The latent mixture regression model
ij
with covariate-dependent structure of mixing proportions (LMR-CD) and the latent
mixture regression model (LMR) are fitted to the simulated data. For both models, we
use the normal linear regression models as the component models, and the number
of components is selected based on BIC. For comparison, we again computed the
MISE with x = −1.5,−0.75,0, and the results are presented in Figure 3. In the
figure, LMR-CD appears to perform better than LMR for the clusters with the small
within-cluster sample sizes for both m.
3.2 Real data example
To demonstrate the proposed method in a practical situation, we apply the latent
mixture model to the posted land price (PLP) data in Tokyo and the surrounding
four prefectures (Chiba, Saitama, Kanagawa and Ibaraki) in 2001. The data units
(locations) are clustered with respect to the nearest station. The number of clusters
is m = 295 and the total number of units is N = 2363. The number of within-cluster
samples n are ranging from 1 to 45, and the histogram of n is provided in the left
i i
panel in Figure 4. We note that there are 221 clusters with n smaller than 10 and
i
25 clusters with n = 1. The response variable y is the PLP which is measured in
i ij
100,000 yen per squared meter. In each jth unit (location) in ith cluster (station),
y is observed with the floor area ratio (%) F and amount of time A (second)
ij ij ij
to station i on foot. Moreover, as cluster level information, the amount of time
T from Tokyo station by train and the prefecture to which the station belongs are
i
available. We use four dummy variables D ,D ,D , and D for Chiba, Saitama,
i1 i2 i3 i4
Kanagawa, and Ibaraki, respectively, which take value one if the station i belongs to
the corresponding prefecture and zero otherwise. The values of y range from 0.158
ij
to 20.3. The right panel of Figure 4 shows that the histogram of y for y < 8. Note
ij ij
that the number of samples with y ≥ 8 is only 20 which is less than 1% of the total
ij
number of observations. Using this dataset, the conditional density of the PLP for
each station is estimated.
Let x = (1,F ,A ,T ,D ,...,D )t. We consider the following latent mixture
ij ij ij i i1 i4
9

regression (LMR) model:
K
(cid:88)
t 2
f (y |π ,...,π ) = π φ(y ;x β ,σ ), j = 1,...,n , i = 1,...,m,
i ij i1 iK ik ij ij k k i
k=1
t ∗
(π ,...,π ) ∼ Dir(α ,...,α ), α = exp(γ + γ T ), k = 1,...,K,
i1 iK i1 iK ik 1k 2k i
(10)
where φ(·;a,b) denotes the density function of N(a,b), and T∗ is the standardized
i
version of T . It is noted that the marginal model (3) is given by
i
K
(cid:88) α
t 2 ik
f (y ) = p φ(y ;x β ,σ ), p = , (11)
i ij ik ij ij k k ik (cid:80)K
α
k=1 (cid:96)=1 i(cid:96)
and the cluster-wise estimated density (4) is
K
(cid:88)
t 2
f (y) = E[π |Y ]φ(y;x β ,σ ),
i ik i k k
k=1
where Y = {y ,...,y } and E[π |Y ] can be computed from the Gibbs sampling
i i1 in ik i
i
(6). Moreover, based on BIC, the number of latent components was selected to be
K = 6 from {1,...,8}. We also doubled the number of Gibbs draws in the E-step,
but the same result was obtained.
For comparison with the proposed method, we also applied the global mixture
(GM) model with K components:
∗
K
∗
(cid:88)
t 2
f(y) = p φ(y;x β ,σ ),
k k k
k=1
(cid:80)K
where ∗ p = 1. It is expected that the estimated GM is similar to the marginal
k=1 k
model in LMR. Based on BIC K = 5 was selected.
∗
To visualize the estimated conditional density in each cluster, we fixed the co-
variate vector x at (1,100,600,T ,D ,...,D )t, in which f (y|x) corresponds to the
i i1 i4 i
density function of the PLP of each cluster when the floor area ratio is 100 and the
location is 10 minutes’ walk from the nearest station. Figure 5 presents the estimated
density under LMR, the marginal model of LMR (mLMR), and GM for the stations
with small n . The figure shows that the cluster-wise estimated densities under LMR
i
are close to those under the marginal model (11) when n is small. This is because the
i
small n values leads to a small difference between the prior mean p and posterior
i ik
mean E[π |Y ] of π , so that the estimated densities in such clusters are automat-
ik i ik
ically close to those under the marginal model which can be stably estimated from
the data. Figure 6 presents the estimated densities for the stations with relatively
large n . Contrary to Figure 5, the estimated densities under LMR are apart from the
i
marginal model in some clusters. The result implies that the marginal model is ad-
justed by the observed data in these clusters. We finally point out that the marginal
model of LMR and GM are similar in most cases since their modeling strategies are
similar in the sense that they aim at estimating the global density by ignoring the
clustering structure.
10

4 Conclusion and Discussion
We have proposed the latent mixture model for estimating the cluster-wise conditional
distributions. The model parameters are estimated by using the simple Monte Carlo
EM algorithm instead of the brute force maximization of the marginal likelihood.
Through the simulation and empirical studies, the proposed method is found to be
useful for flexible modeling of clustered data.
In this article, we selected the number of components by using AIC and BIC.
However, it is well-recognized that the mixture model is a singular model and the
use of AIC or BIC is not justified. The detailed investigation of selecting the number
of latent components with theoretical validity would extend the scope of this article,
which will be left as a valuable future work.
Acknowledgments. This work was supported by JSPS KAKENHI Grant Numbers
[16H07406, 15K17036, 16K17101]. The computational results were obtained using Ox
version 6.21 (Doornik, 2007).
References
[1] Booth, J. G. and Hobert, J. P. (1999). Maximizing generalized linear mixed
model likelihoods with an automated Monte Carlo EM algorithm. Journal of
the Royal Statistical Society: Series B, 61, 265–285.
[2] Demidenko, E. (2004). Mixed Models: Theory and Applications, New York:
Wiley.
[3] Dempster, A., Laird, N. and Rubin, D. (1977). Maximum Likelihood From
Incomplete Data via the EM Algorithm (with discussion). Journal of the Royal
Statistical Society: Series B, 39, 1–38.
[4] Doornik, J. (2007). Ox: Object Oriented Matrix Programming, Timberlake Con-
sultants Press: London.
[5] Geweke, J. and Keane, M. (2007). Smoothly mixing regressions, Journal of
Econometrics, 138, 252–290.
[6] Hurn, M., Justel, A. and Robert, C. P. (2003). Estimating Mixtures of Regres-
sions, Journal of Computational and Graphical Statistics, 12, 55–79.
[7] Jacobs, R. A., Jordan, M. I., Nowlan, S. J. and Hinton, G. E. (1991). Adaptive
mixtures of local experts. Neural Computation, 3, 79–87.
[8] Jordan, M. I. and Jacobs, R. A. (1994). Hierarchical mixtures of experts and
the EM algorithm. Neural Computation, 214, 181–214.
[9] McLachlan, G. J. and Peel, D. (2000). Finite Mixture Models, New York: Wiley.
[10] Nguyen, H. D. and McLachlan, G. J. (2016). Laplace mixture of linear experts.
Computational Statistics & Data Analysis, 93, 177–191.
[11] Rosen, O., Jiang, W. and Tanner, M. A. (2000). Mixtures of marginal models.
Biometrika, 87, 391–404.
[12] Rubin, D. B. and Wu, Y. (1997). Modeling schizophrenic behavior using general
mixture components. Biometrics, 53, 243–261.
11

[13] Shi, J. Q. and Copas, J. (2002). Publication bias and meta-analysis for 2×2
tables: an average Markov chain Monte Carlo EM algorithm. Journal of the
Royal Statistical Society: Series B, 64, 221–236.
[14] Sun, Z., Rosen, O. and Sampson, A. R. (2007). Multivariate Bernoulli mix-
ture models with application to postmortem tissue studies in schizophrenia.
Biometrics, 63, 901–909.
[15] Tang, X. and Qu, A. (2016). Mixture modeling for longitudinal data. Journal
of Computational and Graphical Statistics, 25, 1117–1137.
[16] Villani, M., Kohn, R. and Giordani, P. (2009). Regression density estimation
using smooth adaptive Gaussian mixtures, Journal of Econometrics, 153, 155–
173.
[17] Villani, M., Kohn, R. and Nott, D. J. (2012). Generalized smooth finite mix-
tures, Journal of Econometrics, 171, 121–133.
[18] Wei, G. C. G. and Tanner, M. A. (1990). A Monte Carlo implementation of
the EM algorithm and the poor man’s data augmentation algorithm, Journal
of the American Statistical Association, 85, 699–704.
12

x=−1.5 (n=30) x=−1.5 (n=50)
101−2−3−4−5−6−101−2−3−4−5−6−101−2−3−4−5−6− LMR GM 101−2−3−4−5−6−101−2−3−4−5−6−101−2−3−4−5−6− LMR GM
LM RI LM RI
ESIM golESIM golESIM gol ESIM golESIM golESIM gol
0 10 20 30 40 50 0 10 20 30 40 50
group number group number
x=−0.75 (n=30) x=−0.75 (n=50)
LMR GM LMR GM
LM RI LM RI
0 10 20 30 40 50 0 10 20 30 40 50
group number group number
x=0 (n=30) x=0 (n=50)
LMR GM LMR GM
LM RI LM RI
0 10 20 30 40 50 0 10 20 30 40 50
group number group number
Figure 1: Mean integrated squared error (MISE) of four models evaluated at x =
−1.5,−0.75,0 in scenario (I) with n = 30 (left) and n = 50 (right).
13

| 0      | 1      |
|:-------|:-------|
| LMR GM | LMR GM |
| LM RI  | LM RI  |

| 0      | 1      |
|:-------|:-------|
| LMR GM | LMR GM |
| LM RI  | LM RI  |

| 0      | 1      |
|:-------|:-------|
| LMR GM | LMR GM |
| LM RI  | LM RI  |

| 0      | 1      |
|:-------|:-------|
| LMR GM | LMR GM |
| LM RI  | LM RI  |

| 0      | 1      |
|:-------|:-------|
| LMR GM | LMR GM |
| LM RI  | LM RI  |

| 0      | 1      |
|:-------|:-------|
| LMR GM | LMR GM |
| LM RI  | LM RI  |

x=−1.5 (n=30) x=−1.5 (n=50)
202−4−6−8−202−4−6−8−202−4−6−8− LMR GM 202−4−6−8−202−4−6−8−202−4−6−8− LMR GM
LM RI LM RI
ESIM golESIM golESIM gol ESIM golESIM golESIM gol
0 10 20 30 40 50 0 10 20 30 40 50
group number group number
x=−0.75 (n=30) x=−0.75 (n=50)
LMR GM LMR GM
LM RI LM RI
0 10 20 30 40 50 0 10 20 30 40 50
group number group number
x=0 (n=30) x=0 (n=50)
LMR GM LMR GM
LM RI LM RI
0 10 20 30 40 50 0 10 20 30 40 50
group number group number
Figure 2: Mean integrated squared error (MISE) of four models evaluated at x =
−1.5,−0.75,0 in scenario (II) with n = 30 (left) and n = 50 (right).
14

| 0      | 1      |
|:-------|:-------|
| LMR GM | LMR GM |
| LM RI  | LM RI  |

| 0      | 1      |
|:-------|:-------|
| LMR GM | LMR GM |
| LM RI  | LM RI  |

| 0      | 1      |
|:-------|:-------|
| LMR GM | LMR GM |
| LM RI  | LM RI  |

| 0      | 1      |
|:-------|:-------|
| LMR GM | LMR GM |
| LM RI  | LM RI  |

| 0      | 1      |
|:-------|:-------|
| LMR GM | LMR GM |
| LM RI  | LM RI  |

| 0      | 1      |
|:-------|:-------|
| LMR GM | LMR GM |
| LM RI  | LM RI  |

x=−1.5 (m=50) x=−1.5 (m=80)
5.3−0.4−5.4−0.5−5.5−0.6−5.6−5.3−0.4−5.4−0.5−5.5−0.6−5.6−5.3−0.4−5.4−0.5−5.5−0.6−5.6− 5.3−0.4−5.4−0.5−5.5−0.6−5.6−5.3−0.4−5.4−0.5−5.5−0.6−5.6−5.3−0.4−5.4−0.5−5.5−0.6−5.6−
LMR−CD LMR LMR−CD LMR
ESIM golESIM golESIM gol ESIM golESIM golESIM gol
0 10 20 30 40 50 0 20 40 60 80
group number group number
x=−0.75 (m=50) x=−0.75 (m=80)
LMR−CD LMR LMR−CD LMR
0 10 20 30 40 50 0 20 40 60 80
group number group number
x=0 (m=50) x=0 (m=80)
LMR−CD LMR LMR−CD LMR
0 10 20 30 40 50 0 20 40 60 80
group number group number
Figure 3: Mean integrated squared error (MISE) of three models evaluated at x =
−1.5,−0.75,0 in scenario (III) with m = 50 (left) and m = 80 (right).
15

| 0          | 1          |
|:-----------|:-----------|
| LMR−CD LMR | LMR−CD LMR |

| 0          | 1          |
|:-----------|:-----------|
| LMR−CD LMR | LMR−CD LMR |

| 0          | 1          |
|:-----------|:-----------|
| LMR−CD LMR | LMR−CD LMR |

| 0          | 1          |
|:-----------|:-----------|
| LMR−CD LMR | LMR−CD LMR |

| 0          | 1          |
|:-----------|:-----------|
| LMR−CD LMR | LMR−CD LMR |

| 0          | 1          |
|:-----------|:-----------|
| LMR−CD LMR | LMR−CD LMR |

0050040030020010ytisnedytisned
021080604020ytisnedytisned
ycneuqerFytisnedytisned ycneuqerF
0 10 20 30 40 0 2 4 6 8
sample size PLP
Figure 4: Histograms of within-cluster sample size n (left) and posted land price y
i ij
(right).
station 5 (ni=1) station 19 (ni=1) station 42 (ni=2)
5.20.25.10.15.00.05.20.25.10.15.00.0 5.20.25.10.15.00.05.20.25.10.15.00.0 5.20.25.10.15.00.05.20.25.10.15.00.0
LMR LMR LMR
mLMR mLMR mLMR
GM GM GM
0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5
y y y
station 8 (ni=1) station 40 (ni=2) station 47 (ni=3)
LMR LMR LMR
mLMR mLMR mLMR
GM GM GM
0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5
y y y
Figure 5: Estimated cluster-wise conditional densities of PLP in stations with small
n .
i
16

|    | None   | None   | None   |
|:---|:-------|:-------|:-------|
|    |        |        |        |
|    |        |        |        |
|    |        |        |        |

| None   | None   | None   |    |    | None   | None   | None   | None   | None   |
|:-------|:-------|:-------|:---|:---|:-------|:-------|:-------|:-------|:-------|
|        |        |        |    |    |        |        |        |        |        |
|        |        |        |    |    |        |        |        |        |        |
|        |        |        |    |    |        |        |        |        |        |
|        |        |        |    |    |        |        |        |        |        |
|        |        |        |    |    |        |        |        |        |        |

| 0    | 1    |
|:-----|:-----|
| LMR  | LMR  |
| mLMR | mLMR |
| GM   | GM   |

| 0    | 1    |
|:-----|:-----|
| LMR  | LMR  |
| mLMR | mLMR |
| GM   | GM   |

| 0    | 1    |
|:-----|:-----|
| LMR  | LMR  |
| mLMR | mLMR |
| GM   | GM   |

| 0    | 1    |
|:-----|:-----|
| LMR  | LMR  |
| mLMR | mLMR |
| GM   | GM   |

| 0    | 1    |
|:-----|:-----|
| LMR  | LMR  |
| mLMR | mLMR |
| GM   | GM   |

| 0    | 1    |
|:-----|:-----|
| LMR  | LMR  |
| mLMR | mLMR |
| GM   | GM   |

station 209 (ni=9) station 290 (ni=27) station 293 (ni=33)
5.20.25.10.15.00.05.20.25.10.15.00.05.20.25.10.15.00.0 5.20.25.10.15.00.05.20.25.10.15.00.05.20.25.10.15.00.0 5.20.25.10.15.00.05.20.25.10.15.00.05.20.25.10.15.00.0
LMR LMR LMR
mLMR mLMR mLMR
GM GM GM
ytisnedytisnedytisned ytisnedytisnedytisned ytisnedytisnedytisned
0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5
y y y
station 231 (ni=11) station 291 (ni=31) station 294 (ni=36)
LMR LMR LMR
mLMR mLMR mLMR
GM GM GM
0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5
y y y
station 233 (ni=11) station 292 (ni=31) station 295 (ni=45)
LMR LMR LMR
mLMR mLMR mLMR
GM GM GM
0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5
y y y
Figure 6: Estimated cluster-wise conditional densities of PLP in stations with mod-
erate or large n .
i
17

| 0    | 1    |
|:-----|:-----|
| LMR  | LMR  |
| mLMR | mLMR |
| GM   | GM   |

| 0    | 1    |
|:-----|:-----|
| LMR  | LMR  |
| mLMR | mLMR |
| GM   | GM   |

| 0    | 1    |
|:-----|:-----|
| LMR  | LMR  |
| mLMR | mLMR |
| GM   | GM   |

| 0    | 1    |
|:-----|:-----|
| LMR  | LMR  |
| mLMR | mLMR |
| GM   | GM   |

| 0    | 1    |
|:-----|:-----|
| LMR  | LMR  |
| mLMR | mLMR |
| GM   | GM   |

| 0    | 1    |
|:-----|:-----|
| LMR  | LMR  |
| mLMR | mLMR |
| GM   | GM   |

| 0    | 1    |
|:-----|:-----|
| LMR  | LMR  |
| mLMR | mLMR |
| GM   | GM   |

| 0    | 1    |
|:-----|:-----|
| LMR  | LMR  |
| mLMR | mLMR |
| GM   | GM   |

| 0    | 1    |
|:-----|:-----|
| LMR  | LMR  |
| mLMR | mLMR |
| GM   | GM   |