Fine-grained Sentiment Classification using BERT
Manish Munikar∗, Sushil Shakya† and Aakash Shrestha‡
Department of Electronics and Computer Engineering
Pulchowk Campus, Institute of Engineering, Tribhuvan University
Lalitpur, Nepal
∗070bct520@ioe.edu.np, †070bct547@ioe.edu.np, ‡070bct501@ioe.edu.np
Abstract—Sentiment classification is an important process in Google researchers published BERT (Bidirectional Encoder
understanding people’s perception towards a product, service, Representations from Transformers) [4], a deep bidirectional
or topic. Many natural language processing models have been
language model based on the Transformer architecture [5],
proposed to solve the sentiment classification problem. However,
9102 tcO 4  ]LC.sc[  1v47430.0191:viXra and advanced the state-of-the-art in many popular NLP tasks.
most of them have focused on binary sentiment classification.
In this paper, we use the pretrained BERT model and fine-
In this paper, we use a promising deep learning model called
BERT to solve the fine-grained sentiment classification task. tune it for the fine-grained sentiment classification task on the
Experiments show that our model outperforms other popular Stanford Sentiment Treebank (SST) dataset.
models for this task without sophisticated architecture. We also
The rest of the paper is organized into six sections. In
demonstrate the effectiveness of transfer learning in natural
Section II, we mention our motivation for this work. In
language processing in the process.
Section III, we discuss related works. In Section IV, we
Index Terms—sentiment classification, natural language pro-
cessing, language model, pretraining describe the dataset we performed our experiments on. We
explain our model architecture and methodology in detail
I. INTRODUCTION in Section V. Then we present and analyze our results in
Section VI. Finally, we provide our concluding remarks in
Sentiment classification is a form of text classification in
Section VII.
which a piece of text has to be classified into one of the
predefined sentiment classes. It is a supervised machine learn-
II. MOTIVATION
ing problem. In binary sentiment classification, the possible
classes are positive and negative. In fine-grained sentiment
We have been working on replicating the different research
classification, there are five classes (very negative, negative,
paper results for sentiment analysis, especially on the fine-
neutral, positive, and very positive). Fig 1 shows a black-box
grained Stanford Sentiment Treebank (SST) dataset. After the
view of a fine-grained sentiment classifier model.
popularity of BERT, researchers have tried to use it on different
NLP tasks, including binary sentiment classification on SST-2
(binary) dataset, and they were able to obtain state-of-the-art
results as well. But we haven’t yet found any experimentation
Sentiment
Review Sentiment label
done using BERT on the SST-5 (fine-grained) dataset. Because
Classifier
text (0, 1, 2, 3, 4)
BERT is so powerful, fast, and easy to use for downstream
tasks, it is likely to give promising results in SST-5 dataset
as well. This became the main motivation for pursuing this
Fig. 1. High-level black-box view of a sentiment classifier showing its input work.
and output.
III. RELATED WORK
Sentiment classification model, like any other machine
learning model, requires its input to be a fixed-sized vector Sentiment classification is one of the most popular tasks in
of numbers. Therefore, we need to convert a text—sequence NLP, and so there has been a lot of research and progress
of words represented as ASCII or Unicode—into a fixed- in solving this task accurately. Most of the approaches have
sized vector that encodes the meaningful information of the focused on binary sentiment classification, most probably
text. Many statistical and deep learning NLP models have because there are large public datasets for it such as the IMDb
been proposed just for that. Recently, there has been an movie review dataset [6]. In this section, we only discuss some
explosion of developments in NLP as well as other deep significant deep learning NLP approaches applied to sentiment
learning architectures. classification.
While transfer learning (pretraining and finetuning) has The first step in sentiment classification of a text is the
become the de-facto standard in computer vision, NLP is yet embedding, where a text is converted into a fixed-size vector.
to utilize this concept fully. However, neural language models Since the number of words in the vocabulary after tokenization
such as word vectors [1], paragraph vectors [2], and GloVe [3] and stemming is limited, researchers first tackled the problem
have started the transfer learning revolution in NLP. Recently, of learning word embeddings. The first promising language

model was proposed by Mikolov et al. [1]. They trained con-
–
tinuous semantic representation of words from large unlabeled
0 –
text that could be fine-tuned for downstream tasks. Pennington
0 0 – 0
et al. [3] used a co-occurrence matrix and only trained on non- .
This film
zero elements to efficiently learn semantic word embeddings. – 0
Bojanowski et al. [7] broke words into character n-grams for 0 0 + +
does n’t care
smaller vocabulary size and fast training.
0 +
about
The next step is to combine a variable number of word
+ +
vectors into a single fixed-size document vector. The trivial
+ 0 0 +
or
way is to take the sum or the average, but they don’t lose
+ 0 0 0 0 +
the ordering information of words and thus don’t give good wit any of
+ 0 0 + + ++
,
results. Tai et al. [8] used recursive neural networks to compute cleverness other kind intelligent humor
vector representation of sentences by utilizing the intrinsic
Fig. 2. A sample sentence from the SST dataset. (Source: Adapted from [9].)
tree structure of natural language sentences. Socher et al. [9]
introduced a tensor-based compositionaity function for better
interaction between child nodes in recursive networks. They on all nodes as well as on just the root nodes, and on both
also introduced the Stanford Sentiment Treebank (SST) dataset SST-2 and SST-5.
for fine-grained sentiment classification. Tai et al. [10] applied
various forms of long short-term memory (LSTM) networks V. METHODOLOGY
and Kim [11] applied convolutional neural networks (CNN)
Sentiment classification takes a natural language text as
towards sentiment classification.
input and outputs a sentiment score ∈ {0,1,2,3,4}. Our
All of the approaches mentioned above are context-free, i.e.,
method has three stages from input sentence to output score,
they generate single word embedding for each word in the
which are described below. We use pretrained BERT model
vocabulary. For instance, “bank“ would have the same repre-
to build a sentiment classifier. Therefore, in this section, we
sentation in “bank deposit“ and “river bank“. Recent language
briefly explain BERT and then describe our model architecture.
model research has been trying to train contextual embeddings.
Peters et al. [12] extracted context-sensitive features from left- A. BERT
to-right and right-to-left LSTM-based language model. Devlin
BERT (Bidirectional Encoder Representations from Trans-
et al. [4] proposed BERT (Bidirectional Encoder Represen-
formers is an embedding layer designed to train deep bidi-
tations from Transformers), an attention-based Transformer
rectional representations from unlabeled texts by jointly con-
architecture [5], to train deep bidirectional representations
ditioning on both left and right context in all layers. It is
from unlabeled texts. Their architecture not only obtains state-
pretrained from a large unsupervised text corpus (such as
of-the-art results on many NLP tasks but also allows a high
Wikipedia dump or BookCorpus) using the following objec-
degree of parallelism since it is not based on sequential or
tives:
recurrent connections.
Masked word prediction: In this task, 15% of the words
•
in the input sequence are masked out, the entire sequence
IV. DATASET
is fed to a deep bidirectional Transfomer [5] encoder, and
then the model learns to predict the masked words.
Stanford Sentiment Treebank (SST) [9] is one of the most
Next sentence prediction: To learn the relationship be-
popular publicly available datasets for fine-grained sentiment •
tween sentences, BERT takes two sentences A and B as
classification task. It contains 11,855 one-sentence movie
inputs and learns to classify whether B actually follows
reviews extracted from Rotten Tomatoes. Not only that, each
A or is it just a random sentence.
sentence is also parsed by the Stanford constituency parser
[13] into a tree structure with the whole sentence as the root Unlike traditional sequential or recurrent models, the atten-
node and the individual words as leaf nodes. Moreover, each tion architecture processes the whole input sequence at once,
node is labeled by at least three humans. In total, SST contains enabling all input tokens to be processed in parallel. The layers
215,154 unique manually labeled texts of varying lengths. Fig of BERT architecture are visualized in Fig 3. Pretrained BERT
2 shows a sample review from the SST dataset in a parse- model can be fine-tuned with just one additional layer to obtain
tree structure with all its nodes labeled. Therefore, this dataset state-of-the-art results in a wide range of NLP tasks [4].
can be used to train models to learn the sentiment of words, There are two variants for BERT models: BERT and
BASE
phrases, and sentences together. BERT . The difference between them is listed in Table I.
LARGE
There are five sentiment labels in SST: 0 (very negative), 1) Input format: BERT requires its input token sequence
1 (negative), 2 (neutral), 3 (positive), and 4 (very positive). If to have a certain format. First token of every sequence
we only consider positivity and negativity, we get the binary should be [CLS] (classification token) and there should be
SST-2 dataset. If we consider all five labels, we get SST-5. For a [SEP] token (separation token) after every sentence. The
this research, we evaluate the performance of various models output embedding corresponding to the [CLS] token is the

BERT (Ours) OpenAI GPT
ELMo
Review text
 T T ... T  T T ...  T  T T ...  T
1 2 N 1 2 N 1 2 N
Preprocessing
... ...
Trm Trm Trm Trm Trm Trm
Lstm Lstm ... Lstm Lstm Lstm Lstm
...
BERT Embedding
... ...
Trm Trm Trm Trm Trm Trm
Lstm Lstm ... Lstm Lstm Lstm ... Lstm
Dropout
 E E ...  E  E E ...  E
1 2 N 1 2 N  E E ...  E
1 2 N
Softmax Classifier
Fig. 3. BERT Architecture, where E is the n-th token in the input sequence,
n
Trm is the Transformer block, and T is the corresponding output embedding.
n
(Source: Adapted from [4].) Sentiment label
(0, 1, 2, 3, 4)
TABLE I
BERT VS. BERT LARGE.
BASE Fig. 4. Proposed architecture for fine-grained sentiment classification.
BERT BERT
BASE LARGE
that the sum of the probabilities is 1. The softmax layer is
No. of layers (Transformer blocks) 12 24
just a fully connected neural network layer with the softmax
No. of hidden units 768 1024
No. of self-attention heads 12 16 activation function. The softmax function σ : RK → RK is
Total trainable parameters 110M 340M given in (1).
ez
i
σ(z) = for i = 1,...,K (1)
i (cid:80)K
sequence embedding that can be used for classifying the whole ez j
j=1
sequence.
where z = (z ,...,z ) ∈ RK is the intermediate output of
1 K
B. Preprocessing the softmax layer (also called logits). The output node with
the highest probability is then chosen as the predicted label
We perform the following preprocessing steps on the review
for the input.
text before we feed them into out model.
1) Canonicalization: First, we remove all the digits, punc-
tuation symbols and accent marks, and convert everything to VI. EXPERIMENTS AND RESULTS
lowercase.
In this section, we discuss the results of our model and
2) Tokenization: We then tokenize the text using the Word-
compare with it some of the popular models that solve the
Piece tokenizer [14]. It breaks the words down to their prefix,
same problem, i.e., sentiment classification on the SST dataset.
root, and suffix to handle unseen words better. For example,
playing → play + ##ing.
A. Comparison Models
3) Special token addition: Finally, we add the [CLS] and
[SEP] tokens at the appropriate positions. 1) Word embeddings: In this method, the word vectors
pretrained on large text corpus such as Wikipedia dump are
C. Proposed Architecture averaged to get the document vector, which is then fed to the
sentiment classifier to compute the sentiment score.
We build a simple architecture with just a dropout regular-
ization [15] and a softmax classifier layers on top of pretrained 2) Recursive networks: Various types of recursive neural
BERT layer to demonstrate that BERT can produce great re- networks (RNN) have been applied on SST [9]. We compare
sults even without any sophisticated task-specific architecture. our results with the standard RNN and the more sophisticated
Fig 4 shows the overall architecture of our model. There RNTN. Both of them were trained on SST from scratch,
are four main stages. The first is the proprocessing step as without pretraining.
described earlier. Then we compute the sequence embedding 3) Recurrent networks: Sophisticated recurrent networks
from BERT. We then apply dropout with a probability factor such as left-to-right and bidrectional LSTM networks have
of 0.1 to regularize and prevent overfitting. Dropout is only also been applied on SST [10].
applied in training phase and not in inference phase. Finally, 4) Convolutional networks: In this approach, the input
the softmax classification layer will output the probabilities sequences were passed through a 1-dimensional convolutional
of the input text belonging to each of the class labels such neural network as feature extractors [11].

| Review text        | None   |
| ... T              |        |
| N                  |        |
|:-------------------|:-------|
| Preprocessing      |        |
| ... Trm            |        |
| BERT Embedding     |        |
| ...                | Trm    |
| Dropout            |        |
| ...                | E      |
| N                  |        |
| Softmax Classifier |        |
| Sentiment label    |        |
| (0, 1, 2, 3, 4)    |        |

TABLE II on this paper. We also thank all the helpers and reviewers for
ACCURACY (%) OF OUR MODELS ON SST DATASET COMPARED TO their valuable input to this work.
MODELS.1
OTHER
REFERENCES
SST-2 SST-5
Model [1] T.Mikolov,K.Chen,G.S.Corrado,andJ.Dean,“Efficientestimationof
All Root All Root word representations in vector space,” CoRR, vol. abs/1301.3781, 2013.
[2] Q. Le and T. Mikolov, “Distributed representations of sentences and
Avg word vectors [9] 85.1 80.1 73.3 32.7 documents,” in International Conference on Machine Learning, 2014,
RNN [8] 86.1 82.4 79.0 43.2 pp. 1188–1196.
[3] J. Pennington, R. Socher, and C. Manning, “GloVe: Global vectors
RNTN [9] 87.6 85.4 80.7 45.7
for word representation,” in Proceedings of the 2014 Conference on
Paragraph vectors [2] – 87.8 – 48.7
Empirical Methods in Natural Language Processing (EMNLP), 2014,
LSTM [10] – 84.9 – 46.4 pp. 1532–1543.
BiLSTM [10] – 87.5 – 49.1 [4] J.Devlin,M.-W.Chang,K.Lee,andK.Toutanova,“Bert:Pre-trainingof
CNN [11] – 87.2 – 48.0 deep bidirectional transformers for language understanding,” in NAACL-
HLT, 2018.
BERT 94.0 91.2 83.9 53.2 [5] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
BASE
BERT 94.7 93.1 84.2 55.5 Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances
LARGE
in Neural Information Processing Systems, 2017, pp. 5998–6008.
1 Some values are blank in “All” columns because the original authors [6] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts,
of those paper did not publish their result on all phrases. “Learning word vectors for sentiment analysis,” in Proceedings of the
49th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies. Association for Computational Lin-
guistics, June 2011, pp. 142–150.
B. Evaluation Metric
[7] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, “Enriching word
vectors with subword information,” Transactions of the Association for
Since the dataset has roughly balanced number of samples
Computational Linguistics, vol. 5, pp. 135–146, 2017.
of all classes, we directly use the accuracy measure to evaluate [8] R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and C. D. Manning,
the performance of our model and compare it with other “Semi-supervised recursive autoencoders for predicting sentiment dis-
tributions,” in Proceedings of the Conference on Empirical Methods
models. The accuracy is defined simply as follows:
in Natural Language Processing. Association for Computational
number of correct predictions Linguistics, 2011, pp. 151–161.
accuracy = ∈ [0,1] (2) [9] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng,
total number of predictions
and C. Potts, “Recursive deep models for semantic compositionality
over a sentiment treebank,” in Proceedings of the 2013 Conference on
C. Results
Empirical Methods in Natural Language Processing (EMNLP), 2013,
The result and comparisons are shown in Table II. It shows pp. 1631–1642.
[10] K. S. Tai, R. Socher, and C. D. Manning, “Improved semantic rep-
the accuracy of various models on SST-2 and SST-5. It
resentations from tree-structured long short-term memory networks,”
includes results for all phrases as well as for just the root
in Proceedings of the 53rd Annual Meeting of the Association for
(whole review). We can see that our model, despite being a Computational Linguistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long Papers), 2015, pp.
simple architecture, performs better in terms of accuracy than
1556–1566.
many popular and sophisticated NLP models.
[11] Y. Kim, “Convolutional neural networks for sentence classification,”
arXiv preprint arXiv:1408.5882, 2014.
VII. CONCLUSION
[12] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,
and L. Zettlemoyer, “Deep contextualized word representations,” arXiv
In this paper, we used the pretrained BERT model and fine-
preprint arXiv:1802.05365, 2018.
tuned it for the fine-grained sentiment classification task on the
[13] D. Chen and C. Manning, “A fast and accurate dependency parser using
SST dataset. Even with such a simple downstream architecture, neural networks,” in Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing (EMNLP), 2014, pp. 740–750.
our model was able to outperform complicated architectures
[14] M. Schuster and K. Nakajima, “Japanese and korean voice search,” in
like recursive, recurrent, and convolutional neural networks. 2012 IEEE International Conference on Acoustics, Speech and Signal
Thus, we have demonstrated the transfer learning capability in Processing (ICASSP). IEEE, 2012, pp. 5149–5152.
[15] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-
NLP enabled by deep contextual language models like BERT.
dinov, “Dropout: A simple way to prevent neural networks from over-
fitting,” The Journal of Machine Learning Research, vol. 15, no. 1, pp.
ACKNOWLEDGMENT
1929–1958, 2014.
We would like to express our gratitude towards Prof. Dr.
Shashidhar Ram Joshi for his invaluable advice and guidance