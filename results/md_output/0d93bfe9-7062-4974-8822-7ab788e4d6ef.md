Ridge regularization for Mean Squared Error
Reduction in Regression with Weak
9102 rpA 81  ]ME.noce[  1v08580.4091:viXra
Instruments
Karthik Rajkumar∗
April 19, 2019
Abstract
In this paper, I show that classic two-stage least squares (2SLS) es-
timates are highly unstable with weak instruments. I propose a ridge
estimator (ridge IV) and show that it is asymptotically normal even
with weak instruments, whereas 2SLS is severely distorted and un-
bounded. I motivate the ridge IV estimator as a convex optimization
problem with a GMM objective function and an L penalty. I show
2
that ridge IV leads to sizable mean squared error reductions theoreti-
cally and validate these results in a simulation study inspired by data
designs of papers published in the American Economic Review.
1 Introduction
Instrumental variables are widely used in applied economics and other so-
cial sciences to establish causal relationships in the absence of experimental
variation. Under standard assumptions, instrumental variable (IV) regres-
sion estimators are unbiased and asymptotically consistent. However, when
the first-stage, i.e. the relationship between instrumental variables and the
independent variable at hand, is weak, inference with IV regression is dis-
torted and a vast literature has emerged showing this. In particular, size is
∗Department of economics, Stanford University. krajkumar@stanford.edu. I am very
grateful to my advisor, Guido Imbens, for advising me through this project with patience
and insight.
1

not controlled in finite sample and several finite-sample corrections of the
IV estimator have been proposed to tackle this. These methods attempt to
remove bias in small sample in a way that washes out as sample sizes grow.
Many of these methods, however, do so in a static way, which sometimes
leads to overcorrection or even having no effect at all. We propose a ridge
estimator for IV regression that attempts to alleviate the bias in a way that
is tunable to the data. We motivate the estimator by returning to the clas-
sical perspective of IV regression as a ratio of two estimands (in the case of
a single, just-identified IV), and showing that our approach is equivalent to
stabilizing the denominator away from 0, thus avoiding the division-by-zero
problem.
The paper is organized as follows. Section 2 provides background on the
weak instrument problem and IV outlier sensitivity problem. Section 3 in-
troduces the data model we use throughout the paper. Section 4 motivates
and defines the ridge IV estimator and shows full preservation of efficiency
under “large” parameters. Section 5 takes a local asymptotic approach to
modeling weak instruments and shows using theory how ridge IV leads to
drastic reductions in mean squared error. Section 6 interpets the ridge IV
estimator as the solution to a convex optimization problem with a GMM
objective function with an L penalty on the coefficient. Section 7 uses a
2
simulation study whose parameters are tuned to be consistent with the data
generating processes implied in a sample of papers published in the Ameri-
can Economic Review and shows how ridge IV can lower MSE in practice.
Section 8 concludes.
2 Literature
There is a long literature in econometrics studying weak instruments. Young
(2018) raises concerns on the quality of inference with instrumental variables.
Specifically, the problems cited are weak or irrelevant instruments, non-iid
error processes, and distortion in inference from one or two observations. A
major claim in that paper is that IV estimates have larger MSE than OLS
(OLS here referring to the structural equation of directly regressing the de-
pendent variable on the endogenous variable, bypassing any instruments). In
2

situations arising in practice, one is unable to tell the two estimates apart
in the sense that IV confidence intervals generally include OLS anyway, and
implies a preference for OLS. This calls into question the utility of traditional
econometric tests for endogeneity, such as the Hausman test (e.g. Hausman
(1978)).
Andrews et al. (2018) provides a recent survey on weak instrument diag-
nostics, and inferential methods that are robust to weak instruments. They
focus on the case of heteroskedastic and possibly non-iid errors, and make a
case for the adoption of the robust F-statistic proposed by Olea and Pflueger
(2013) in the case of a single endogenous regressor. Young (2018) counters
this, claiming that such pre-tests for detecting weak instruments do little to
accurately diagnose the problem in practice.
2.1 The weak instrument problem
The primary problem with weak instruments is that they are biased towards
OLS. This gives tests the wrong size, and leads to misleading inference. By
far, the most popular case in the literature appears to be the just-identified
case with a single instrument and heteroskedasticity (Andrews et al., 2018).
To combat this, the most common approach used in the literature is some
form of the following two-stage process:
1. F ≥ 10. Here instruments are not weak and regular 2SLS inference is
used.
2. F < 10. Various “weak instrument robust” methods are used.
What are some of these weak instrument robust methods? Hirano and
Porter (2015) show there does not exist an unbiased or asymptotically unbi-
ased IV estimator. Anderson-Rubin confidence sets (Anderson et al., 1949)
are optimal in just-identified case with single instrument, even with het-
eroskedasticity. In over-identified models, conditional LR (CLR) test is a
good test for homoskedastic settings as it is fully robust to weak instru-
ments. Andrews and Armstrong (2017) provide methods that are unbiased
when sign of first-stage coefficient is known a priori.
3

For the scope of this paper, we address the large MSE critique of 2SLS.
We provide a novel estimator, guiding theory, as well as simulation evidence
for the utility of this estimator.
3 Model
We begin with a a just-identified setting with one instrument. Our data takes
the form
Y = β + β D + (cid:15) (1)
i 0 1 i i
D = π + π Z + η .
i 0 1 i i
Each datapoint is iid and the instruments are exogenous, i.e. Z are inde-
i
pendent of η and (cid:15) . In this notation, Y is the outcome variable of interest,
i i i
and D is the endogenous variable whose effect of the outcome one is inter-
i
ested in.
The main contention we want to address with our paper is that 2SLS is
more sensitive because it is a ratio of two things and its p-value does not
account for this stochasticity of the denominator. To explain this simply, in
the just-identified case, our 2SLS estimator is
(cid:80) ¯ ¯
(Y − Y )(Z − Z)
βˆ = i i i ,
2SLS (cid:80) ¯ ¯
(D − D)(Z − Z)
i i i
which basically is
ˆ
Cov[Y ,Z ] a
i i n
= := ,
Cˆ ov[D ,Z ] b
i i n
where b → 0. Essentially, this is a a division-by-zero problem.
n
4 Ridge estimation
To address the weak instrument problem, we propose adding a bias to the
denominator of the 2SLS, in the case of just-identified single instrument.
This turns the estimator into
a
n
,
b + λ
n
4

where λ is a tuning parameter, appropriately chosen. This serves the dual
objectives of stabilizing the denominator of the IV estimator and also shrink-
ing the coefficient toward zero, thus controlling type-1 error. This naturally
motivates a ridge estimator in the case of just-identified with multiple in-
struments too:
(Z(cid:48)D + λI)−1Z(cid:48)Y. (2)
Similarly for an over-identified setting, the ridge estimator modifies to
(D(cid:48)Z(Z(cid:48)Z)−1Z(cid:48)D + λI)−1(D(cid:48)Z(Z(cid:48)Z)−1Z(cid:48)Y ). (3)
Suppose we allowed that the ridge estimator allowed for the penalty pa-
rameter to vary with sample size. That is, λ is indexed by n to give us a
full sequence of penalty parameters. In our specific example from equation
1, this would mean our estimator is
(cid:80) ¯ ¯
(Y − Y )(Z − Z)
βˆ = i i i
ridge (cid:80) ¯ ¯
(D − D)(Z − Z) + λ
i i i n
ˆ
Cov[Y ,Z ]
i i
= . (4)
Cˆ ov[D ,Z ] + λ n
i i
n
Before we dive into the asymptotic properties of the ridge estimator, we
make a note on the sampling process we assume for the data.
4.1 Sampling assumptions on the data
There are many assumptions that may be used for the data generating pro-
cess. Suppose we condition on the instruments, Z , treating them as “con-
i
stants” and only assuming the existence of probability limits of their mo-
ments. Then the classic OLS asymptotic results hold. What does that mean?
Let us formalize this idea.
Specifically
Assumption 1. The instruments Z are “constants” with respect to the error
i
shocks. This is achieved by conditioning on the instruments in hand. Further,
(cid:80)
under this sampling scheme, we assume 1 Z2 → 1.
n i i
Now consider the first stage equation:
D = π Z + η .
i 1 i i
We ignore constants for the time being.
5

Lemma 1 (CLT for OLS—Version 1). Under Assumption 1, the CLT for
OLS is
√
→d (cid:0) 0,σ2(cid:1)
n(πˆ − π ) N .
1 1 η
Proof. This is easily proven using the Liapounov CLT. Specifically, we have
(cid:0) (cid:1)
D Z = π Z2 + Z η ∼ π Z2,Z2σ2 .
i i 1 i i i 1 i i η
Thus, Liapounov CLT tells us that
(cid:80)
D Z − π Z2
i i i 1 i →d N(0,1).
(cid:113)
(cid:80)
Z2σ2
i i η
Using the second moment condition assumed from the Z sampling, we get
i
the desired result.
However, this is not the result one get when assuming Z is fully stochas-
i
tic!
Assumption 2. Suppose the instruments Z are fully stochastic. Without
i
loss of generality, we assume unit variance and also bounded fourth moment,
m .
4
Lemma 2 (CLT for OLS—Version 2). Under Assumption 2, the CLT for
OLS is
√
n(πˆ − π ) →d N (cid:0) 0,π2(m − 1) + σ2(cid:1) .
1 1 1 4 η
Proof. When Z is also stochastic, the random variable D Z looks like
i i i
(cid:0) (cid:1)
D Z = π Z2 + Z η ∼ π ,π2(m − 1) + σ2 .
i i 1 i i i 1 1 4 η
Here we may apply the classic Lindeberg-Levy CLT to get
√ (cid:16) (cid:17)
(cid:80)
D Z
n i i i − π
1
n d
→ N (0,1).
(cid:113)
π2(m − 1) + σ2
1 4 η
Rearranging terms gives us the desired result.
Observe that the results under Assumptions 1 and 2 are different! Indeed
the variance under full stochasticity is larger than the previous case, because
there is more randomness to account for.
6

ˆ
Theorem 4.1 (Consistency of the ridge estimator). Let β be the ridge
ridge
estimator as defined in Equation (4). Suppose the instrument is not totally
irrelevant, i.e. π (cid:54)= 0. Then under either Assumption 1 or Assumption
1
2, and for a sequence of penalty parameters λ = o(n), the estimator is
n
consistent. That is,
ˆ p
β → β .
ridge 1
Proof. We can rewrite the structural equation in the data generating process
as
Y = β + β (π + π Z + η ) + (cid:15)
i 0 1 0 1 i i i
= (β + β π ) + β π Z + (β η + (cid:15) ).
0 1 0 1 1 i 1 i i
This is just the reduced form equation. Without loss of generality, we
(cid:80)
have assumed Z2/n → 1 (or Var[Z ] = 1, if using Assumption 2). Then
i i i
it is clear that
ˆ p
Cov[Y ,Z ] → β π .
i i 1 1
Similarly, the first stage gives us
ˆ p
Cov[D ,Z ] → π .
i i 1
Since π (cid:54)= 0, and we have λ n → 0, convergence in probability allows us to
1
n
take the ratio of these two estimators, and we get our result.
The natural next question is, what is the asymptotic distribution of the
proposed estimator? We tackle this in the next theorem.
Theorem 4.2 (Asymptotic normality of the ridge estimator—Version 1).
√
Suppose λ = o( n). Then the ridge estimator, as defined in Equation (4),
n
is asymptotically normal. That is,
√ (cid:16) (cid:17)
ˆ
n β − β → N(0,V ).
ridge 1 ridge
Further, under Assumption 1, V = σ2/π2.
ridge (cid:15) 1
Proof. First, we examine the reduced form regression. We understand that
from the Central Limit Theorem of OLS in Lemma 1, we have
√ (cid:16) (cid:17)
n Cˆ ov[Y ,Z ] − β π → N (cid:0) 0,σ2 (cid:1) ,
i i 1 1 red
7

where σ2 is the homoskedastic error variance of the reduced form regression.
red
That is, it is the variance of the (β η + (cid:15) ) residual term. Similarly, analyzing
1 i i
the first-stage regression, we have
√ (cid:16) (cid:17)
Cˆ (cid:0) 0,σ2(cid:1)
n ov[D ,Z ] − π → N ,
i i 1 η
for σ2, the variance of η , the errors in the first stage.
η i
(cid:0) (cid:1)
Next, we have Cov[Y Z ,D Z ] = Z2Cov[Y ,D ] = Z2 β σ2 + σ2 , where
i i i i i i i i 1 η (cid:15)η
σ2 is the covariance between (cid:15) and η . Putting these results together, we
(cid:15)η i i
get the multivariate result
√ (cid:18)(cid:18) Cˆ ov[Y ,Z ] (cid:19) (cid:18) β π (cid:19)(cid:19) (cid:18)(cid:18) 0 (cid:19) (cid:20) σ2 β σ2 + σ2 (cid:21)(cid:19)
n i i − 1 1 → N , red 1 η (cid:15)η .
Cˆ ov[D ,Z ] π 0 β σ2 + σ2 σ2
i i 1 1 η (cid:15)η η
√
Call this covariance matrix Σ. Because λ = o( n), we can use Slutsky’s
n
theorem to get the same asymptotic distribution:
√ (cid:18)(cid:18) Cˆ ov[Y ,Z ] (cid:19) (cid:18) β π (cid:19)(cid:19) (cid:18)(cid:18) 0 (cid:19) (cid:19)
i i 1 1
n − → N ,Σ .
Cˆ ov[D ,Z ] + λ n π 0
i i 1
n
This is the asymptotic distribution of a bivariate estimator. We note that
ˆ
β is simply the ratio of the first and the second elements of this bivariate
ridge
estimator.
Then to obtain the distribution of the ridge estimator, we can use the
multivariate delta method here. It says the ratio estimator converges to
the ratio of individual probability limits. Consider the bivariate function
x/y2)T
h(x,y) = x/y. Its gradient is given by ∇h(x,y) = (1/y, − . So the
asymptotic variance of the ridge estimator is
∇h(β π ,π )T × Σ × ∇h(β π ,π )
1 1 1 1 1 1
(cid:20) (cid:21)(cid:18) (cid:19)
(cid:16) (cid:17) σ2 β σ2 + σ2 1/π
= 1 −β 1 red 1 η (cid:15)η 1 .
π π β σ2 + σ2 σ2 −β /π
1 1
1 η (cid:15)η η 1 1
Here σ2 = β2σ2 + σ2 + 2β σ2 . So the above is
red 1 η (cid:15) 1 (cid:15)η
(cid:20) (cid:21)(cid:18) (cid:19)
(cid:16) (cid:17) β2σ2 + σ2 + 2β σ2 β σ2 + σ2 1/π
= 1 −β 1 1 η (cid:15) 1 (cid:15)η 1 η (cid:15)η 1 ,
π π β σ2 + σ2 σ2 −β /π
1 1
1 η (cid:15)η η 1 1
8

which simplifies to
(cid:18) (cid:19)
1 1
(cid:2) (cid:3)
= σ2 + β σ2 σ2
π2 (cid:15) 1 (cid:15)η (cid:15)η −β
1
1
σ2
= (cid:15) .
π2
1
This is the required V , and we have our asymptotic distribution.
ridge
Theorem 4.3 (Asymptotic normality of the ridge estimator—Version 2).
√
Again suppose λ = o( n). Under Assumption 2, the ridge estimator is
n
asymptotically normal as well, and with the same asymptotic variance, V .
ridge
Proof. From a multivariate central limit theorem as in Lemma 2, we again
have asymptotic normality:
√ (cid:18)(cid:18) Cˆ ov[Y ,Z ] (cid:19) (cid:18) β π (cid:19)(cid:19) (cid:18)(cid:18) 0 (cid:19) (cid:19)
i i 1 1
n − → N ,Σ .
Cˆ ov[D ,Z ] π 0
i i 1
Note that the Σ covariance matrix in this theorem is different from the one
using Assumption 1. Let us obtain it.
We have Y = β π Z +β η +(cid:15) and D = π Z +η . We ignore intercepts
i 1 1 i 1 i i i 1 i i
without loss of generality.
Then,
Var[Y Z ] = E[Y 2Z2] − E[Y Z ]2
i i i i i i
(cid:2) (cid:3)
= E (β π Z2 + β η Z + (cid:15) Z )2 − (β π )2
1 1 i 1 i i i i 1 1
(cid:2) (cid:3)
= E β2π2Z4 + β2η2Z2 + (cid:15)2Z2 + 2β2π Z3η + 2β η (cid:15) Z2 + 2β π Z3(cid:15) −(β π )2
1 1 i 1 i i i i 1 1 i i 1 i i i 1 1 i i 1 1
= β2π2m + β2σ2 + σ2 + 0 + 2β σ2 + 0 − β2π2
1 1 4 1 η (cid:15) 1 (cid:15)η 1 1
= β2π2(m − 1) + β2σ2 + σ2 + 2β σ2 .
1 1 4 1 η (cid:15) 1 (cid:15)η
Next,
Var[D Z ] = π2(m − 1) + σ2
i i 1 4 η
9

from Lemma 2. Finally,
Cov[Y Z ,D Z ] = E[Y D Z2] − E[Y Z ]E[D Z ]
i i i i i i i i i i i
(cid:2) (cid:3)
= E (β π Z + β η + (cid:15) )(π Z + η )Z2 − β π2
1 1 i 1 i i 1 i i i 1 1
(cid:2)(cid:0) (cid:1) (cid:3)
= E β π2Z2 + β π Z η + β π η Z + β η2 + π (cid:15) Z + (cid:15) η Z2 − β π2
1 1 i 1 1 i i 1 1 i i 1 i 1 i i i i i 1 1
= β π2m + 0 + 0 + β σ2 + 0 + σ2 − β π2
1 1 4 1 η (cid:15)η 1 1
= β π2(m − 1) + β σ2 + σ2 .
1 1 4 1 η (cid:15)η
From these it is clear that
(cid:18) (cid:19)
β2π2(m − 1) + β2σ2 + σ2 + 2β σ2 β π2(m − 1) + β σ2 + σ2
Σ = 1 1 4 1 η (cid:15) 1 (cid:15)η 1 1 4 1 η (cid:15)η .
β π2(m − 1) + β σ2 + σ2 π2(m − 1) + σ2
1 1 4 1 η (cid:15)η 1 4 η
Using the multivariate delta method again as in the Theorem 4.2 on this
new ∆, we get the variance of the ridge estimator to be
σ2
(cid:15) ,
π2
1
which is remarkably the same result.
We see that ridge IV recovers full efficiency in the “large” coefficient case.
From another perspective, this result is uninteresting because ridge IV “does
nothing.” The λ rate being too slow, it yields the exact same distribution
n
as the original 2SLS estimator without any penalization. To obtain a novel
asymptotic distribution with ridge IV, we need a faster rate. We show that
now.
√
To see what ridge IV is able to do, we set λ = O( n). To be clear, in
√ n
the previous theorems we use a sub- n rate, whereas now we use exactly
√ √
a n rate for λ . Knight et al. (2000) show that λ = O( n) is necessary
√ n n
for unique n−consistency of classic ridge regression. We show the same for
ridge IV.
√ √
Theorem 4.4. Let λ = O( n), that is λ / n → λ , for some λ ≥ 0.
n n 0 0
Then
(cid:18) (cid:19)
√ (cid:16) (cid:17) β λ
ˆ d 1 0
n β − β → N − ,V .
ridge 1 ridge
π
1
10

Proof. Let us look at the case with Assumption 1 first. The proof of Theorem
4.2 says
√ (cid:18)(cid:18) Cˆ ov[Y ,Z ] (cid:19) (cid:18) β π (cid:19)(cid:19) (cid:18)(cid:18) 0 (cid:19) (cid:19)
i i 1 1 d
n − → N ,Σ .
Cˆ ov[D ,Z ] + λ n π λ
i i 1 0
n
The only thing that changes is the bias term with λ because of the slower
√ 0
O( n) rate of λ . Now take the ratio of the two elements of the vector and
n
perform the delta method like in Theorem 4.2. This gives us
(cid:18) (cid:18) (cid:19) (cid:19)
√ (cid:16) (cid:17) 0
n βˆ − β →d N ∇h(β π ,π )T × ,V ,
ridge 1 1 1 1 ridge
λ
0
using the same notation as in the theorem 4.2. This gives us our result.
Under this new λ regime, we see that we recover the same asymptotic
n
distribution as the 2SLS estimator, but centered at the wrong mean! That
β λ
is, the asymptotic bias is − 1 0, rather than 0. What then do we gain from
π
1
the ridge approach? The next section addresses this point.
5 The Staiger-Stock critique
Recall our main motivation for ridge IV is the weak instrument problem. To
deal with it more explicitly, we take a local asymptotic framework used in
Staiger and Stock (1997). That is, we use
c
π = √ .
1
n
This is to show that the strength of the first-stage is small, even relative to
the sample size and the problem does not go away with bigger samples. Our
next theorem shows the behavior of conventional 2SLS under this sense of
weak instruments.
Theorem 5.1. Suppose our first stage is weak in the Staiger-Stock sense.
√
That is, π = c/ n. Then the 2SLS estimator is unstable and diverges.
1
√ (cid:16) (cid:17)
ˆ ˆ
Specifically, β converges to a Cauchy distribution, and so n β − β
2SLS 2SLS 1
diverges.
11

Proof. We operate under Assumption 1 here. We have
D = π Z + η .
i 1 i i
Consider D Z . Since π varies with sample size, n, we employ a triangular
i i 1
array argument here. D Z has mean µ = π Z2 and variance σ2 = Z2σ2.
i i ni 1 i ni i η
(cid:80)n (cid:80)n
Let T = (D Z −µ ) and s2 = Var[T ] = σ2 . The Lindeberg-
n i=1 i i ni n n i=1 ni
Feller Central Limit Theorem states
T
n d
→ N(0,1).
s
n
Applying this to our setting, we get
(cid:80)n
(D Z − π Z2)
i=1 i i 1 i →d N(0,1).
(cid:112)(cid:80)n
σ Z2
η i=1 i
(cid:80)n
We have Z2/n → 1. So this can be rewritten as
i=1 i
(cid:32) (cid:33)
n
√ (cid:88) D Z − π Z2
n i i 1 i →d N(0,σ2).
η
n
i=1
√
Further, π = c/ n. So this further simplifies to
1
n
√ (cid:88) D Z
n i i →d N(c,σ2).
η
n
i=1
Applying the Lindeberg-Feller CLT similarly to the reduced form equa-
tion, we get
(cid:80)n
(Y Z − β π Z2)
i=1 i i 1 1 i →d N(0,1).
(cid:112)(cid:80)
σ Z2
red i i
And by similar logic, this can be rewritten as
n
√ (cid:88) Y Z
n i i →d N(cβ ,σ2 ).
1 red
n
i=1
Following the derivation of the covariance term in Theorem 4.2, we can
then were a joint normality result as follows:
n (cid:18) (cid:19) (cid:18)(cid:18) (cid:19) (cid:20) (cid:21)(cid:19)
1 (cid:88) Y Z − β π Z2 0 σ2 β σ2 + σ2
√ i i 1 1 i →d N , red 1 η (cid:15)η .
n D Z − π Z2 0 β σ2 + σ2 σ2
i i 1 i 1 η (cid:15)η η
i=1
12

We called this covariance matrix Σ. With this notation, we can rewrite
the joint normality as
√ (cid:18) Cˆ ov[Y ,Z ] (cid:19) (cid:18)(cid:18) cβ (cid:19) (cid:19)
i i d 1
n → N ,Σ .
Cˆ ov[D ,Z ] c
i i
Our 2SLS estimator is
ˆ
Cov[Y ,Z ]
i i
,
ˆ
Cov[D ,Z ]
i i
so we can get its distribution by taking the ratio of the above joint normality
distribution, which would result in a Cauchy distribution. Given that the
√
ratio itself has a distribution, n times its difference from its mean would
be unstable and diverges.
How does ridge IV help with this? We demonstrate that in the next
theorem, which is the most important result of the paper.
Theorem 5.2. Let λ = O(n). That is, λ /n → λ for some λ ≥ 0. Then,
n n 0 0
under Staiger-Stock asymptotics, we have
(cid:18) (cid:19)
√ cβ σ2
nβˆ →d N 1 , red .
ridge
λ λ2
0 0
Proof. The ridge IV estimator is
ˆ
Cov[Y ,Z ]
i i
.
Cˆ ov[D ,Z ] + λ n
i i
n
√
ˆ
From Theorem 5.1, we know that Cov[D ,Z ] = O (1/ n). This implies
i i p
ˆ p
Cov[D ,Z ] → 0,
i i
and from this it follows that
λ
ˆ n p
Cov[D ,Z ] + → λ .
i i 0
n
Also from Theorem 5.1, we know
√
nCˆ ov[Y ,Z ] →d N(cβ ,σ2 ).
i i 1 red
13

Then taking the ratio of the above two results using Slutsky’s theorem, we
get
(cid:32) (cid:33)
√ Cˆ ov[Y ,Z ] 1
n i i →d N(cβ ,σ2 ).
Cˆ ov[D ,Z ] + λ n λ 1 red
i i 0
n
This is the required result.
In the Staiger-Stock regime, ridge IV with an aggressive enough penal-
ization scheme (λ = O(n)) massively lowers mean squared error of the esti-
n
mate. In regimes where instruments are not weak, we can still use ridge IV
√
with a more moderate penalization scheme (λ = o( n)), and lose nothing,
n
although in this case, the choice of ridge IV over 2SLS is superfluous.
6 Understanding ridge IV
6.1 Interpretation of λ as the Lagrange multiplier in a
constrained optimization problem
In the simplest case, as in our model in (1), ignoring intercepts, the standard
2SLS objective function is given by
(cid:32) (cid:33)2
n
(cid:88)
min Z (Y − D β) .
i i i
β
i=1
Then the ridge IV solution is given by the following objective function:
(cid:32) (cid:33)2
n
(cid:88)
min Z (Y − D β) + γ β2. (5)
i i i n
β
i=1
Clearly, setting penalization, λ to zero recovers the original 2SLS esti-
n
mator. What is the relation between λ and the Lagrange multiplier γ we
n n
see above? We address this in the next proposition.
Proposition 1 (Objective function of ridge IV). The objective function of
ridge IV is as given in Equation 5. Further, there is a one-to-one relation
between γ , the Lagrange multiplier in the objective function, and λ , the
n n
level of penalization in the ridge estimator, given by
ˆ
γ = Cov[D ,Z ]λ .
n i i n
14

Proof. Let the ridge IV objective function be
(cid:32) (cid:33)2
n
(cid:88)
L(β) = Z (Y − D β) + γ β2.
i i i n
i=1
This is a convex function, so to minimize it, set its partial derivative with
respect to β to zero. This gives us
(cid:32) (cid:33)(cid:32) (cid:33)
n n
∂ (cid:88) (cid:88)
L(β) = 2 Z (Y − D β) − Z D + 2γ β = 0.
i i i i i n
∂β
i=1 i=1
That is,
(cid:32) (cid:33)(cid:32) (cid:33)
n n n
(cid:88) (cid:88) (cid:88)
Z D β − Z Y Z D + γ β = 0.
i i i i i i n
i=1 i=1 i=1
This gives us
 
(cid:32) (cid:33)2 (cid:32) (cid:33)(cid:32) (cid:33)
(cid:88) (cid:88) (cid:88)
Z D + γ β = Z D Z Y ,
 
i i n i i i i
i i i
or
(cid:80)
Z Y
βˆ = i i i .
(cid:80)
γ
Z D + n
i i i (cid:80) Z D
i i i
Comparing this form with the definition of the ridge IV estimator in
Equation (4), we know
λ γ
n n
= .
(cid:80)
n Z D
i i i
Rearranging the equation gives us the desired result.
7 Results
We look at a simulation design where we are interested in seeing how the
MSE metric varies with varying levels of aggressiveness in the ridge penalty,
λ. The linear IV model for this simulation is
Y = 2.83 + effect × D + (cid:15),
15

D = −0.346 + 0.072 × Z − 0.67 × (cid:15) + η.
This corresponds to our model in equation 1, with coefficients set to β =
0
2.83,π = −0.346,π = 0.072. The coefficients and sample sizes are chosen to
0 1
match a study from the AER (Hornung, 2014). (cid:15),η,Z are some independent
normals.
In the first set of results, we allow the first stage coefficient size, ]pi to
1
vary. Our simulation study is as follows:
1. Pick a first stage coefficient from 0 to 1.
2. Simulate 10,000 datasets of N=150 each.
3. Compute MSE of that estimated coefficient for β (including intercept).
1
16

(a) λ = 0 (Regular 2SLS) (b) λ = 4.0 (Moderate regulariza-
tion)
(c) λ = 10 (Aggressive regulariza-
tion)
Figure 1: MSE of the IV estimator by the level of ridge penalization, λ.
Figure (a) shows the regular 2SLS case, which we compare against. It is
extremely unstable when the instrument is weak, i.e. when we are closer to
0 on the x-axis. When using a moderate level of regularization as in (b), we
see that MSE is reduced by three orders of magnitude. Figure (c) sounds
the alarm against too much regularization because then bias can become a
dominant force to vie with and MSE starts to pick up again.
In the second set of results, we allow effect size, β to vary. This is to
1
show that for a given first stage strength, we may still have some use for
regularization for very small coefficients. This simulation study is as follows:
17

1. Pick an effect size, i.e. β from 0 to 3.475.
1
2. Simulate 10,000 datasets of N=150 each.
3. Compute MSE of that estimated coefficient (including intercept).
(a) λ = 0 (Regular 2SLS) (b) λ = 0.8 (Moderate regulariza-
tion)
(c) λ = 3.0 (Aggressive regular-
ization)
Figure 2: MSE of the IV estimator by the level of ridge penalization, λ.
Figure (a) shows the regular 2SLS case, which we compare against. When
using a moderate level of regularization as in (b), we see that MSE may be
halved. However, figure (c) sounds the alarm against too much regularization
because then bias can become a dominant force to vie with, particularly when
effect sizes are large.
18

We show three cases here. The case of zero regularization is the clas-
sic 2SLS estimator, which has a certain level of mean squared error, which
remains high even when the effect size being studied is small.
8 Conclusion
In this paper, we introduced a novel estimator, called “ridge IV.” We mo-
tivated it as the solution to the GMM objective function for instrumental
variable regression with an additional L penalty. In the theoretical case
2
with “large” coefficients, we showed that ridge IV does not hurt our esti-
mation while in the weak first stage case, we showed that it leads to strong
improvements in the mean squared error of the estimand. We then validated
the theory using simulations inspired by data designs of papers in the Amer-
ican Economic Review.
While this paper is primarily a theoretical contribution to the literature,
we outline several avenues for further research. First, it would be helpful to
provide a method for tuning the λ parameter for a given dataset. Our results
operated at the abstract level of the big-O notation, but for practical use,
more information is needed. We would also like to see results on how exactly
to perform inference with ridge IV. Explicit demonstration of Type 1 error
control, for instance, would be very useful.
We would like to tie back the results of ridge IV and interpret them in
the context of the problem of IV sensitivity to outliers, which is related to
instability. We conjecture that ridge IV under an appropriate penalization
scheme can address this as well. We would also like to expand the results
provided in this paper in the general case of over-identifying instruments. A
natural extension of our estimator is given in Equation (3).
References
Anderson, T. W., H. Rubin, et al.
(1949): “Estimation of the param-
eters of a single equation in a complete system of stochastic equations,”
The Annals of Mathematical Statistics, 20, 46–63.
19

Andrews, I. and T. B. Armstrong
(2017): “Unbiased instrumental vari-
ables estimation under known first-stage sign,” Quantitative Economics, 8,
479–503.
Andrews, I., J. Stock, and L. Sun
(2018): “Weak instruments in iv
regression: Theory and practice,” Tech. rep., Mimeo. Harvard University.
Hausman, J. A.
(1978): “Specification tests in econometrics,” Economet-
rica: Journal of the econometric society, 1251–1271.
Hirano, K. and J. R. Porter
(2015): “Location properties of point esti-
mators in linear instrumental variables and related models,” Econometric
Reviews, 34, 720–733.
Hornung, E.
(2014): “Immigration and the diffusion of technology: The
Huguenot diaspora in Prussia,” American Economic Review, 104, 84–122.
Knight, K., W. Fu, et al.
(2000): “Asymptotics for lasso-type estima-
tors,” The Annals of statistics, 28, 1356–1378.
Olea, J. L. M. and C. Pflueger
(2013): “A robust test for weak instru-
ments,” Journal of Business & Economic Statistics, 31, 358–369.
Staiger, D. and J. H. Stock
(1997): “Instrumental Variables Regres-
sion with Weak Instruments,” Econometrica: Journal of the Econometric
Society, 557–586.
Young, A.
(2018): “Consistency without inference: Instrumental variables
in practical application,” Unpublished manuscript, London: London School
of Economics and Political Science. Retrieved from: http://personal. lse.
ac. uk/YoungA.
20