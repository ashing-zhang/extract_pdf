PT-ResNet: Perspective Transformation-Based Residual
Network for Semantic Road Image Segmentation
Rui Fan1∗, Yuan Wang1∗, Lei Qiao2, Ruiwen Yao2, Peng Han2, Weidong Zhang2, Ioannis Pitas3, Ming Liu1
1Robotics Institute, the Hong Kong University of Science and Technology, Hong Kong.
2Department of Automation, Shanghai Jiao Tong University, Shanghai 200240, China.
3Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece.
eeruifan@ust.hk, ywangeq@connect.ust.hk, qiaolei2008114106@gmail.com, yaoruiwen88@foxmail.com,
han ipac@sjtu.edu.cn, wdzhang@sjtu.edu.cn, eelium@ust.hk, pitas@csd.auth.gr
Abstract—Semantic road region segmentation is a high-level FCN-LC [13] is a classical FCN used for semantic road
9102 tcO 92  ]VC.sc[  1v55031.0191:viXra
task, which paves the way towards road scene understanding. image segmentation. FCN-LC utilizes a network-in-network
This paper presents a residual network trained for semantic
architecture [13] to learn road region segmentation from
road segmentation. Firstly, we represent the projections of road
labeled training image data. This allows fast inference, even
disparities in the v-disparity map as a linear model, which can
for large contextual image window sizes [13]. In addition, in
be estimated by optimizing the v-disparity map using dynamic
programming. This linear model is then utilized to reduce the recent years, a number of conditional random field (CRF)-
redundant information in the left and right road images. The based neural networks, e.g., PGM-ARS [14], Hybrid [15] and
right image is also transformed into the left perspective view,
StixelNet [16] have been proposed for semantic road image
which greatly enhances the road surface similarity between the
segmentation. PGM-ARS [14] and StixelNet [16] were trained
two images. Finally, the processed stereo images and their dis-
using monocular images, while Hybrid [15] also employed the
parity maps are concatenated to create a set of 3D images, which
are then utilized to train our neural network. The experimental 3D road scene information acquired using LiDAR for training.
results illustrate that our network achieves a maximum F1- Furthermore, stereo vision [17], [18] was used to improve
measure of approximately 91.19%, when analyzing the images
road segmentation performance. For example, a so-called BM
from the KITTI road dataset.
neural network [17] selects a region of interest (ROI) in an
I. INTRODUCTION image, by analyzing the v-disparity information. Such ROI
information greatly minimizes the number of incorrectly seg-
Autonomous driving technology has been developing
mented pixels. Furthermore, a so-called HistonBoost network
rapidly, since Google launched its self-driving car project in
[18] post-processes such ROIs using watershed transformation
2009 [1]. In recent years, industry titans, such as Waymo and
and morphological filtering [19]. In this paper, we draw on the
Tesla, race to commercialize autonomous vehicles (AVs) [2],
success of [17], [18] and present a perspective transformation
[3]. However, a number of high-profile experimental accidents
(PT)-based deep convolutional network for road semantic seg-
that occurred in the last year and have called into question
mentation. It is designed using the residual network (ResNet)
whether the autonomous driving technology is mature enough
from DeepLab [20]. The structure of our proposed network is
for employment [4]. Therefore, most researchers believe that
shown in Fig. 1.
in the next few years the research on autonomous driving
The rest of the paper is organized as follows: Section
should focus on developing advanced driver assistance systems
II introduces the proposed PT-ResNet. In Section III, the
(ADASs) [5], such as lane marking detection [6], road surface
experimental results are illustrated and the performance of the
3D reconstruction [7], 2D/3D object detection [8], etc.
proposed approach is evaluated. Section IV contains conclu-
Visual environment perception (VEP) is a key component
sion and some recommendations for future work.
of ADAS [9], [10]. After learning from a large amount of
labeled training data, VEP can extract useful road environment II. METHODOLOGY
information, e.g., free space areas and pedestrians, from road
A. Training Data Pre-Processing and Generation
images [11], semantic image region segmentation can provide
In this paper, the proposed semantic segmentation method
useful information by partitioning an image into semantically
focuses entirely on the road surface, which can be treated as
meaningful regions and classifying them into one of the pre-
a ground plane. According to the perspective transformation
defined categories [12]. State-of-the-art semantic segmentation
algorithm presented in [21], a right image can be transformed
algorithms are generally based on fully convolutional networks
into its left view using the disparity projection model. This
(FCNs) [12], which are an extension of convolutional neural
can greatly enhance the similarity of the road surface between
network (CNN). FCNs utilize classical CNNs to learn image
the stereo images [21]. Therefore, in this paper, we first utilize
feature representations, but the input images can be of any
PSMNet [22] to estimate dense disparity maps (see Fig. 2). A
sizes. FCNs perform image upsampling to produce a proba-
v-disparity map is then created by computing the histograms
bility mask with the same size as the input image [13].
pˆ(d,v) of each horizontal row v of the disparity map [23]. To
*This two authors are joint first authors. find the path corresponding to the road disparity projection in

Fig. 1. PT-ResNet structure.
Fig. 3. The structure of each block unit in Fig. 1.
B. PT-ResNet Structure
In recent years, the encoder-decoder structure has been
prevalently used in deep neural networks for semantic segmen-
tation [26]. The encoder allows fast high-dimensional image
feature map generation, while the decoder enables the network
Fig. 2. Training data pre-processing and generation. to recover sharp object boundaries [26]. In this paper, our
network is designed following ResNet-101 used in DeepLab-
v3+ [26]. The structure of our proposed network is shown in
the v-disparity map, we utilize dynamic programming (DP) to
Fig. 1.
search for every possible solution [24]:
1) Encoder: In the encoder, the spatial dimension of the
feature maps reduces gradually using four blocks, as shown
τ
max
E(d,v) = −pˆ(d,v) + min[E(d + 1,v − τ) − λτ], (1)
in Fig. 1. The structure of each block is shown in Fig. 3,
τ=0
where BN denotes batch normalization, and ReLU denotes
where pˆ(d,v) represents the histogram value at (d,v) in rectified linear unit. ReLU activation function is used to avoid
the v-disparity map, λ is a smoothness term, τ max is the overfitting during training. The parameter of ReLU is set to
maximum search range [25]. E represents the energy of each 0.5 in this paper. BN is a method used to normalize the
possible solution. The path corresponding to the road disparity input of each layer and overcome the internal covariate shift
projection is generally represented using a linear polynomial problem. As the stride in each block is set to 2, the output
[21]: of the fourth block is 256 times smaller than the input of
f(v) = α + α v. (2) the first block. Furthermore, the baseline utilizes an atrous
0 1
convolutional layer instead of a conventional convolutional
The vertical coordinate of the vanishing point, i.e., v , can layer. This allows us to enlarge the field-of-view of filters
py
be estimated using (2). As the vertical coordinates of the road when interpolating the multi-scale context in the framework
pixels are always larger than v , the image region above of spatial pyramid pooling and cascaded modules [27]. The
py
the vanishing point can be removed from the left and right output of each block can be computed by adding the output
images (see Fig. 2). Then, we utilize our previous algorithm of the atrous convolutional layer to the input of the block, as
[21] to transform the perspective view of the right image. This shown in Fig. 3. The output of the fourth block feeds into five
algorithm improves the road surface similarity in the stereo branches, as shown in Fig. 3. The baseline uses global average
images, but also distorts obstacles, such as vehicles and trees. pooling to obtain the global image feature representations.
Finally, the processed stereo images and the left disparity maps In addition, three atrous convolutional layers with different
are concatenated together to generate a set of 3D images with rates are utilized to acquire multi-scale information. The rates
seven channels, which are then utilized to train the neural depend entirely on the feature map produced by block 4, and
network. they are set to 4, 8 and 16, respectively. Finally, the five branch

Fig. 4. Experimental results of road semantic segmentation (threshold is set to 0.9). The green areas in the fourth row are the segmented road surfaces. (a)
Processed left images. (b) Transformed right images. (c) Disparity maps. (d) Segmentation results.
output is concatenated and further compressed using a 1 × 1 reflects the overall performance of UM, UMM and UU. It
convolutional layer. can be observed in Fig. 5(a) that our PT-ResNet method
2) Decoder: In the decoder, the baseline applies skip con- outperforms the others in terms of MaxF, it achieves a MaxF
nection to the feature map, which is produced by the second of approximately 91.91%, which is slightly higher than that
block. This greatly improves the details of local features in the achieved using FCN-LC (90.79%). Fig. 5(b) indicates that PT-
high-level feature map. The low-level and high-level feature ResNet performs better than other networks in terms of AP, as
maps are then concatenated together. A probability map can be it achieves an AP of approximately 91.21%. However, FCN-
obtained after a feature map upsampling process. By finding LC performs slightly better than our network in terms of PRE
the pixels whose probabilities are higher than our pre-set and FPR (see Fig. 5(c) and 5(e)). The overall PRE and FPR we
threshold, the semantic segmentation result can be obtained. achieved is 90.78% and 5.13%, respectively. Additionally, PT-
Some examples of experimental results are shown in Fig. 4. ResNet achieves an intermediate performance in terms of REC
and FNR (see Fig. 5(d) and 5(f)), as the REC and FNR values
III. EXPERIMENTAL RESULTS
obtained using our method is 91.60% and 8.40%, respectively.
In this section, we present our experimental results and
In general, the proposed PT-ResNet achieves the best overall
evaluate the performance of the proposed method using the
performance and its ranking is higher than that of other CNNs.
KITTI road dataset [28]. The dataset contains synchronized
stereo road image pairs, 3D road scenery point clouds acquired IV. CONCLUSION AND FUTURE WORK
using a Velodyne HDL-64E LiDAR, calibration parameters,
In this paper, we presented a deep neural network for
and semantic region segmentation ground truth. The images
semantic road image segmentation. Since the proposed net-
in this dataset are grouped into three categorizes: urban un-
work focuses entirely on the road surface, the left and right
marked (UU), urban marked (UM) and urban multiple marked
stereo images were processed using our previously published
(UMM). To quantify the accuracy of the proposed approach, a
perspective transformation algorithm. This greatly enhanced
set of indicators, including maximum F1-measure (MaxF), av-
the similarity of the road surface between the left and right
erage precision (AP), precision (PRE), recall (REC), false pos-
images. The processed stereo images and their corresponding
itive rate (FPR) and false negative rate (FNR), are computed
subpixel disparity maps were utilized to create 3D train-
and are publicly available on the KITTI road benchmark1.
ing data. Additionally, we developed our network based on
PT-ResNet training was conducted on an NVIDIA GTX 1080
ResNet, a state-of-the-art network with an encoder-decoder
Ti GPU (CUDA 9 and cnDNN v7). In the experiments, the
structure. According to the evaluation results provided by the
learning rate, training step and batch size are set to 0.001,
KITTI road benchmark, our proposed method outperforms
30000 and 8, respectively. The approach was programmed in
FCN-LC, PGM-ARS, Hybrid, StixelNet, BM and HistonBoost
Python language. The runtime of segmenting an image from
in terms of MaxF and AP, achieving an overall MaxF and AP
the KITTI dataset is around 3 seconds. In this section, we
of 91.19% and 91.21%, respectively. However, the ResNet
compare our method with FCN-LC [13], PGM-ARS [14],
from DeepLab-v3+ may not be the best network for learning
Hybrid [15], StixelNet [16], BM [17] and HistonBoost [18].
road semantic segmentation from our created 3D training data.
The comparisons of MaxF, AP, PRE, REC, FPR and FNR
Therefore, we plan to train different state-of-the-art networks,
among these methods are shown in Fig. 5, where urban
such as VGG-16 and VGG-19, and compare the experimental
1http://www.cvlibs.net/datasets/kitti/eval road.php. results with what we achieved in this paper.

(a) (b)
(c) (d)
(e) (f)
Fig. 5. Performance evaluation. (a) Comparison of MaxF. (b) Comparison of AP. (c) Comparison of PRE. (d) Comparison of REC. (e) Comparison of FPR.
(f) Comparison of FNR.
ACKNOWLEDGMENT [2] R. Fan, J. Jiao, H. Ye, Y. Yu, I. Pitas, and M. Liu, “Key ingredients of
self-driving cars,” arXiv:1906.02939, 2019.
This work was supported by the National Natural Science
[3] R. Fan and N. Dahnoun, “Real-time stereo vision-based lane detection
Foundation of China, under grant No. U1713211, the Research system,” Measurement Science and Technology, vol. 29, no. 7, p.
Grant Council of Hong Kong SAR Government, China, under 074005, 2018.
[4] P. Lin, “Why ethics matters for autonomous cars,” in Autonomous
Project No. 11210017, No. 21202816, and the Shenzhen Sci-
driving. Springer, Berlin, Heidelberg, 2016, pp. 69–85.
ence, Technology and Innovation Commission (SZSTI) under [5] D. Cardinal, “The self-driving industry is finally becoming more
grant JCYJ20160428154842603, awarded to Prof. Ming Liu. realistic,” ExtremeTech, Tech. Rep., Jan. 2019. [Online]. Available:
https://www.extremetech.com/extreme/283632-self-driving-industry
REFERENCES [6] U. Ozgunalp, R. Fan, X. Ai, and N. Dahnoun, “Multiple lane detection
algorithm based on novel dense vanishing point estimation,” IEEE
[1] J. A. Brink, R. L. Arenson, T. M. Grist, J. S. Lewin, and D. Enzmann,
Transactions on Intelligent Transportation Systems, vol. 18, no. 3, pp.
“Bits and bytes: the future of radiology lies in informatics and infor-
621–632, 2016.
mation technology,” European radiology, vol. 27, no. 9, pp. 3647–3651,
2017.

[7] R. Fan, J. Jiao, J. Pan, H. Huang, S. Shen, and M. Liu, “Real-time dense [18] G. B. Vitor, A. C. Victorino, and J. V. Ferreira, “Comprehensive
stereo embedded in a uav for road inspection,” in The IEEE Conference performance analysis of road detection algorithms using the common
on Computer Vision and Pattern Recognition (CVPR) Workshops, June urban kitti-road benchmark,” in Proc. IEEE Intelligent Vehicles Symp,
2019. Jun. 2014, pp. 19–24.
[8] X. Du, M. H. Ang, S. Karaman, and D. Rus, “A general pipeline for [19] I. Pitas, Digital image processing algorithms and applications. John
3d detection of vehicles,” in 2018 IEEE International Conference on Wiley & Sons, 2000.
Robotics and Automation (ICRA). IEEE, 2018, pp. 3194–3200. [20] L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille,
[9] C. Yan, H. Xie, D. Yang, J. Yin, Y. Zhang, and Q. Dai, “Supervised “Deeplab: Semantic image segmentation with deep convolutional nets,
hash coding with deep neural network for environment perception of atrous convolution, and fully connected crfs,” IEEE Transactions on
intelligent vehicles,” IEEE transactions on intelligent transportation Pattern Analysis and Machine Intelligence, vol. 40, no. 4, pp. 834–848,
systems, vol. 19, no. 1, pp. 284–295, 2018. Apr. 2018.
[10] R. Fan, “Real-time computer stereo vision for automotive applications,” [21] R. Fan, X. Ai, and N. Dahnoun, “Road surface 3d reconstruction based
Ph.D. dissertation, University of Bristol, 2018. on dense subpixel disparity map estimation,” IEEE Transactions on
[11] J. Y. Baek, I. V. Chelu, L. Iordache, V. Paunescu, H. Ryu, A. Ghiuta, Image Processing, vol. 27, no. 6, pp. 3025–3035, 2018.
A. Petreanu, Y. Soh, A. Leica, and B. Jeon, “Scene understanding [22] J. Chang and Y. Chen, “Pyramid stereo matching network,” in Proc.
networks for autonomous driving based on around view monitoring IEEE/CVF Conf. Computer Vision and Pattern Recognition, Jun. 2018,
system,” in Proc. IEEE/CVF Conf. Computer Vision and Pattern Recog- pp. 5410–5418.
nition Workshops (CVPRW), Jun. 2018, pp. 1074–10747. [23] R. Fan and M. Liu, “Road damage detection based on unsupervised
[12] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks disparity map segmentation,” IEEE Transactions on Intelligent Trans-
for semantic segmentation,” in Proc. IEEE Conf. Computer Vision and portation Systems, 2019.
Pattern Recognition (CVPR), Jun. 2015, pp. 3431–3440. [24] R. Fan, U. Ozgunalp, B. Hosking, M. Liu, and I. Pitas, “Pothole
[13] C. C. T. Mendes, V. Frmont, and D. F. Wolf, “Exploiting fully convolu- detection based on disparity transformation and road surface modeling,”
tional neural networks for fast road detection,” in Proc. IEEE Int. Conf. IEEE Transactions on Image Processing, vol. 29, pp. 897–908, 2020.
Robotics and Automation (ICRA), May 2016, pp. 3174–3179. [25] R. Fan, M. J. Bocus, and N. Dahnoun, “A novel disparity transformation
[14] M. Passani, J. J. Yebes, and L. M. Bergasa, “Fast pixelwise road algorithm for road segmentation,” Information Processing Letters, vol.
inference based on uniformly reweighted belief propagation,” in Proc. 140, pp. 18–24, 2018.
IEEE Intelligent Vehicles Symp. (IV), Jun. 2015, pp. 519–524. [26] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-
[15] L. Xiao, R. Wang, B. Dai, Y. Fang, D. Liu, and T. Wu, “Hybrid decoder with atrous separable convolution for semantic image segmen-
conditional random field based camera-lidar fusion for road detection,” tation,” arXiv preprint arXiv:1802.02611, 2018.
Information Sciences, vol. 432, pp. 543–558, 2018. [27] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, “Rethinking
[16] D. Levi, N. Garnett, E. Fetaya, and I. Herzlyia, “Stixelnet: A deep atrous convolution for semantic image segmentation,” arXiv preprint
convolutional network for obstacle detection and road segmentation.” arXiv:1706.05587, 2017.
in BMVC, 2015, pp. 109–1. [28] J. Fritsch, T. Khnl, and A. Geiger, “A new performance measure and
[17] B. Wang, V. Fre´mont, and S. A. R. Florez, “Color-based road detection evaluation benchmark for road detection algorithms,” in Proc. 16th Int.
and its evaluation on the kitti road benchmark,” in IEEE Intelligent IEEE Conf. Intelligent Transportation Systems (ITSC 2013), Oct. 2013,
Vehicles Symposium (IV 2014), 2014, pp. 31–36. pp. 1693–1700.